[{"categories":["Linux"],"content":"Linux 常识 也许这篇文章对你没有任何作用，因为它更多的是我这4年来将 Linux 作为我日常系统之一的一些杂谈，如果你系统学过 Linux 运维，那么这篇文章对你应该是没有任何帮助 在 Linux 如果遇到参数忘记的命令可以使用 --help 这个 flag 帮助我们，如果 help 这个 flag 没有用，可以使用 man，大多数古老的命令工具可能不支持 help，但是一定支持 man man lsof ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:0:0","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"路径与被小看的命令 在 Linux 命令行中，路径是个奇怪的东西，很多人容易在这里迷路，对于新手它的难度堪比退出 vim 在 Windows 上的图形化界面中路径清晰可见，好像从学电脑开始就是那么自然一样，这好比你会 PHP，然后喊出来那句 “PHP 是世界上最好的语言”，不管是因为只学过 PHP，还是发自内心，别人看你的目光都会变得异样。 在 Linux 系统中我们可以将文件系统想象成一棵树，这棵树的根节点也就是 根 路径用 / 这个表示。 ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:1:0","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"$HOME 当我们创建好用户之后，Linux 通常也为我们准备好了一个 $HOME，我们也通常成为家目录，它位于 根 路径下的 home 路径下的 {username} 这个文件夹中，{username} 是非 root 用户的用户名，如果我的用户名是 jichun_pu 那么这个家目录就是： /home/jichun_pu ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:1:1","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"pwd 当我们登录到 Linux 中时，打开终端（SSH 连接默认在终端中），默认路径就是在 $HOME 路径下，我们可以通过 pwd 命令查看到当前所在路径 pwd 如果经常找不到自己当前的 shell 位于哪个路径下，记得使用 pwd 命令 $HOME 路径下的资源（文件）可以任由你随意操作，就像是在你自己家中一样非常的自由，你就是家里的山大王 ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:1:2","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"ls 通过 ls -a 命令，你可以看到你拥有的文件和文件夹，当然可能运行这个命令后可能只有几个点 ls -a ls: 会列出当前的文件和文件夹 ls -a: 会列出当前的文件和文件夹，包括隐藏的 ls -l: 以列表的形式列出是文件还是目录，它的大小、修改日期和时间、文件或目录的名字以及文件的属主和它的权限 ls -lh: 在 ls -l 的基础上以更容易读的方式显示出文件大小 ls -F: 在 ls 的基础上在文件夹末尾加上 / ls -lt: 在 ls -l 的基础上根据修改时间排序 ls -lS: 在 ls -l 的基础上根据大小排序 ls -l /etc: 以列表的形式列出 /etc 这个文件夹下的文件 ls D*: 列出所有名字以 D 开头的文件夹的（D*是一个正则表达式） 上面的输出中 . 表示当前路径，.. 表示上一级路径 ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:1:3","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"mkdir 通过 mkdir 我们可以创建文件夹 mkdir code 这样我们就创建了一个 code 目录，我们可能还希望在 code 中创建一个 script 文件夹用于存放一些 shell 脚本 mkdir code/script 当然这两个文件夹我们可以一起创建，使用 -p 参数可以创建文件路径表达式中的文件夹 mkdir -p code/script 这还不够，我们可能还想一次性在 code 目录中创建 python、javascript、java文件夹，python文件夹下面有 py1、py2、py3 文件夹，javascript 文件夹 下面有 js1、js2、js3 文件夹，java文件夹下面有 java1、java2、java3 文件夹，结构如下（使用 tree 命令实现） code |-- java | |-- java1 | |-- java2 | `-- java3 |-- javascript | |-- js1 | |-- js2 | `-- js3 |-- python | |-- py1 | |-- py2 | `-- py3 `-- script 对于我们想要创建多个文件夹的需求，只需要用花括号括起来就行 mkdir -p code/{python/{py1,py2,py3},javascript/{js1,js2,js3},java/{java1,java2,java3}} ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:1:4","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"cd cd: Change the current directory to DIR 有时候我们会遇到当我跳转到一个非常深的路径之后想反回的场景，如果是直接 cd xxx/xxx/xxx/xxx/xx 进入的，需要使用很多次 cd ..，当然我们也可以直接使用 cd - 回来（If DIR is “-”, it is converted to $OLDPWD.）。如果我们是使用 cd 进入了一个文件夹之后再使用 cd 进入下一个文件夹，我们可以通过一些其他方法相对快速的反回，我们可以 cd 从跟路径出发，或者从 $HOME 路径出发找到比较近的路径，当然由于在 shell 中打 $HOME 还是比较麻烦的，所以在 shell 中 ~ 会用被翻译为 $HOME，如果你想直接返回 $HOME 甚至可以直接用不带参数的 cd 命令 cd - cd ~ cd / cd $HOME cd 然后通过 cat 重定向该文件描述符进行恢复 ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:1:5","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"du -sh 我们通过 ls 查看当前文件夹和文件列表的时候会能看到文件的大小，但是文件夹的大小只显示元数据的大小，我们可以通过 du 命令查看，但是 du 命令往往不单独使用，它会递归地列出当前目录及其所有子目录占用的磁盘空间。 在使用它之前我建议你看一下它的 flag，我们可以加上 -s 让它显示总大小，-h 可以让结果更易读 du -sh 这样会只显示当前文件夹的总大小，可以再传入一个参数 * 告诉它统计当前文件夹下的所有文件夹 du -sh * ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:1:6","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"Linux 权限 在 Linux 中直接使用 root 身份执行所有操作是危险的，也许 Linux 足够安全，但是当你习惯了这个过程，对权限失去了敬畏之心的时候，危险可能就会在某个使用 Linux 的时刻出现 ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:2:0","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"权限 在 Linux 中使用 id 命令可以查看当前用户的用户id，用户名和所属用户组 id # uid=1006(jichun_pu) gid=1006(jichun_pu) groups=1006(jichun_pu) 线上服务大部分都不涉及没有权限的文件写入操作 ls -l / 执行上面的命令之后可以看到根路径下文件夹和文件的权限，也就是第一列中的内容，代表了文件所属用户、文件所属用户的用户组和其他用户的权限 lrwxrwxrwx. 1 root root 7 Jul 12 2018 bin -\u003e usr/bin dr-xr-xr-x. 5 root root 4096 Jun 8 2021 boot drwxr-xr-x 3 root root 18 Jan 18 09:57 data drwxr-xr-x 18 root root 2940 Mar 1 2023 dev drwxr-xr-x. 89 root root 8192 Jul 15 11:35 etc drwxr-xr-x. 10 root root 146 Jun 20 2023 home lrwxrwxrwx. 1 root root 7 Jul 12 2018 lib -\u003e usr/lib lrwxrwxrwx. 1 root root 9 Jul 12 2018 lib64 -\u003e usr/lib64 drwxr-xr-x. 2 root root 6 Apr 11 2018 media drwxr-xr-x. 2 root root 6 Apr 11 2018 mnt drwxr-xr-x. 5 root root 107 Jan 16 14:46 opt dr-xr-xr-x 229 root root 0 Jun 7 2022 proc dr-xr-x---. 12 root root 4096 May 11 14:43 root drwxr-xr-x 28 root root 960 Jun 13 11:20 run lrwxrwxrwx. 1 root root 8 Jul 12 2018 sbin -\u003e usr/sbin drwxr-xr-x. 2 root root 6 Apr 11 2018 srv dr-xr-xr-x 13 root root 0 Jan 16 14:42 sys drwxr-xr-x 3 root root 19 Nov 16 2022 titan drwxrwxrwt. 9 root root 186 Jul 15 13:58 tmp drwxr-xr-x. 13 root root 155 Jul 12 2018 usr drwxr-xr-x. 20 root root 285 Nov 16 2022 var 可以简单的表示为：{type}{owner_role}{owner_group_role}{other_user_role} 常见的 type: l: link, 表示符号链接 d: 表示文件夹 -: 表示普通文件 后面的字符三个一组，rwx 分别表示可读、可写、可执行，如果没有对应权限就用 - 占位例如： drwxr-xr-x 表示这是一个文件夹，当前用户有读写和执行权限，当前用户所属用户组有读和执行权限，其他用户有读和执行权限。有的时候权限这一列最后一可能还带有一个 . 这和Linux的安全机制有关 第二列表示文件的硬链接数量，第三列表示文件或文件夹所属用户，第四列表示文件或文件夹的元文件大小，第五到七列表示文件或文件夹最后修改日期，第八列是文件或文件夹的名称，如果最后有 -\u003e /xxx/xxx 表示该文件夹或文件链接指向的文件夹或文件 lrwxrwxrwx. 1 root root 7 Jul 12 2018 bin -\u003e usr/bin lrwxrwxrwx.: 这是一个符号链接，所属用户有读写和执行权限，所属用户的所属的用户组有读写和执行权限，其他用户有读写和执行权限 1: 这个文件夹的硬链接数量是1个 root: 该符号链接所属用户是 root root: 该符号链接所属用户的用户组是 root (这里的用户和用户组名称是相同的) 7 Jul 12 2018: 该符号链接最后一次修改是 2018 年7月12日 bin -\u003e usr/bin: 该符号链接指向的是 usr/bin (也就是可以理解为 /bin 文件夹其实就是 /usr/bin 文件夹) ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:2:1","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"修改文件权限 进入到项目文件夹中，服务器上的项目文件都是通过 rsync 同步过来的，非 root 用户是没有写权限的 [jichun_pu@Qxbspideronline-10-2-6-10 gsspiderpy]$ ls -lh total 140K drwxr-xr-x 5 root root 63 Apr 17 19:16 alert drwxr-xr-x 11 root root 327 Jul 10 15:19 core drwxr-xr-x 4 root root 53 Apr 17 16:41 doream -rw-r--r-- 1 root root 544 Jul 2 19:42 gsspiderpy_sync.sh drwxr-xr-x 9 root root 154 Jul 12 14:19 gsxt_search drwxr-xr-x 5 root root 84 Jul 10 15:19 message_send drwxr-xr-x 11 root root 176 Jul 2 14:35 notice -rw-r--r-- 1 root root 117K May 6 14:38 poetry.lock -rw-r--r-- 1 root root 1.2K May 6 14:38 pyproject.toml -rw-r--r-- 1 root root 1.1K Apr 22 11:52 README.md -rw-r--r-- 1 root root 1.8K Apr 22 11:52 requirements.txt -rw-r--r-- 1 root root 572 Apr 22 11:52 run.py 想要让非 root 用户修改 README.md 文件可以通过 chmod (change mode) 修改其他用户权限的方式完成（并不推荐这样做，并不是这样做不行，而是这个文件夹被同步之后文件权限就会恢复） 一说到权限或者 chmod 可能就会有人想到 chmod 777 filename 就完事了，这和提上裤子不认人又有什么区别呢 sudo chmod o+w README.md o: 其他用户 +w: 增加写权限 执行完成之后不使用 sudo 提权就能修改文件成功，如果使用的是带有历史记录来撤销或恢复的这类编辑器可能会提示无法删除备份文件，但是并不影响保存成功，如果仍然觉得有困惑可以给这个文件夹为其他用户增加写权限。 同理如果想要给文件增加可执行权限，可以直接使用 sudo chmod +x README.md 上面对文件夹或文件的操作都使用了 sudo 提权，因为这个操作的文件或文件夹的拥有者都是 root，那是否可以将文件夹或文件的拥有者改为当前用户呢？这是可以的，我们可以使用 chown 办到 [jichun_pu@Qxbspideronline-10-2-6-10 coderoot]$ sudo chown -R jichun_pu gsspiderpy [jichun_pu@Qxbspideronline-10-2-6-10 coderoot]$ ls -l total 88 drwxr-xr-x 8 root root 271 Jun 4 10:35 competitor_spider drwxr-xr-x 13 root root 4096 Jul 11 16:01 gsrunner drwxr-xr-x 9 jichun_pu root 291 Jul 2 19:42 gsspiderpy drwxr-xr-x 24 root root 4096 Jul 11 15:13 jsrunner drwxr-xr-x 6 root root 87 Nov 22 2022 venv chown (change owner) 参数意思如下: -R: 递归更改，也就是指定目录和子目录的拥有者都将修改 jichun_pu: 用户名 gsspiderpy: 需要操作的文件夹 再看看 gsspiderpy 中的权限 [jichun_pu@Qxbspideronline-10-2-6-10 coderoot]$ ls -l gsspiderpy/ total 140 drwxr-xr-x 5 jichun_pu root 63 Apr 17 19:16 alert drwxr-xr-x 11 jichun_pu root 327 Jul 10 15:19 core drwxr-xr-x 4 jichun_pu root 53 Apr 17 16:41 doream -rw-r--r-- 1 jichun_pu root 544 Jul 2 19:42 gsspiderpy_sync.sh drwxr-xr-x 9 jichun_pu root 154 Jul 12 14:19 gsxt_search drwxr-xr-x 5 jichun_pu root 84 Jul 10 15:19 message_send drwxr-xr-x 11 jichun_pu root 176 Jul 2 14:35 notice -rw-r--r-- 1 jichun_pu root 119155 May 6 14:38 poetry.lock -rw-r--r-- 1 jichun_pu root 1196 May 6 14:38 pyproject.toml -rw-r--r-- 1 jichun_pu root 1091 Apr 22 11:52 README.md -rw-r--r-- 1 jichun_pu root 1836 Apr 22 11:52 requirements.txt -rw-r--r-- 1 jichun_pu root 572 Apr 22 11:52 run.py 可以看到所有文件和文件夹的拥有者都变成了当前用户 ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:2:2","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"可执行权限 读写和可执行三个权限其实都是非常好理解的，至于为什么要单独写一节可执行权限原因如下 在 $HOME 目录下创建一个 listdir.sh 文件 # listdir.sh ls -lah 这很明显是一个 shell 脚本文件，那么我们应该怎么执行呢，如果直接执行，肯定会提示没有权限，我们可能会经历下面的一系列过程 [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ nvim listdir.sh [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ ./listdir.sh -bash: ./listdir.sh: Permission denied [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ sudo ./listdir.sh sudo: ./listdir.sh: command not found [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ ls -l | grep \"listdir\" -rw-rw-r-- 1 jichun_pu jichun_pu 8 Jul 15 18:13 listdir.sh [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ 这时候想起来没有给这个 shell 脚本赋予可执行权限，于是发现能够执行了 [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ chmod +x listdir.sh [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ ./listdir.sh total 168M drwx------ 12 jichun_pu jichun_pu 4.0K Jul 15 18:13 . drwxr-xr-x. 10 root root 146 Jun 20 2023 .. -rw------- 1 jichun_pu jichun_pu 183K Jul 14 15:04 .bash_history -rw-r--r-- 1 jichun_pu jichun_pu 18 Apr 1 2020 .bash_logout -rw-r--r-- 1 jichun_pu jichun_pu 193 Apr 1 2020 .bash_profile -rw-r--r-- 1 jichun_pu jichun_pu 351 Dec 9 2022 .bashrc drwxrwxr-x 5 jichun_pu jichun_pu 41 Jun 28 2022 .cache -rw-rw-r-- 1 jichun_pu jichun_pu 79 Jul 6 2023 .clicmd drwxrwxr-x 4 jichun_pu jichun_pu 33 Jul 5 2023 .config drwxrwxr-x 2 jichun_pu jichun_pu 4.0K Jul 9 11:00 enterprise_small_update -rw-rw-r-- 1 jichun_pu jichun_pu 55 Jun 8 2022 .gitconfig -rw-rw-r-- 1 jichun_pu jichun_pu 555 Jul 4 18:07 imp.py drwxrwxr-x 3 jichun_pu jichun_pu 29 Jun 28 2022 .ipython -rw-rw-r-- 1 jichun_pu jichun_pu 12K Jun 4 15:11 .iredis_history -rw-rw-r-- 1 jichun_pu jichun_pu 2.9K Jul 6 2023 .iredisrc -rwxrwxr-x 1 jichun_pu jichun_pu 8 Jul 15 18:13 listdir.sh drwx------ 5 jichun_pu jichun_pu 41 Jun 9 2022 .local drwxrwxr-x 4 jichun_pu jichun_pu 72 Jun 10 2022 .npm drwxrw---- 3 jichun_pu jichun_pu 19 Jun 8 2022 .pki drwxrwxr-x 5 jichun_pu jichun_pu 144 Jun 22 2022 .pm2 -rw------- 1 jichun_pu jichun_pu 34K Jul 14 12:44 .python_history -rw-rw-r-- 1 jichun_pu jichun_pu 468K Sep 13 2023 records.csv -rw------- 1 jichun_pu jichun_pu 797 Oct 21 2022 .rediscli_history drwx------ 2 jichun_pu jichun_pu 29 Jun 8 2022 .ssh 可能有人会疑惑，脚本里面没有 Shebang 行，为什么能够执行成功，其实在 Linux 中不写 Shebang 行，那么在执行脚本的时候会使用当前用户的 shell 去执行这个脚本，如果是个 python 脚本或者 nodejs 脚本，问题就会暴露出来 为什么二进制文件在赋予可执行权限的时候，不用考虑这个 Shebang 行，不用指定文件入口？而脚本需要？ 上面的问题是一个陷阱，bash、python、node 这些不都是可执行文件吗，他们都是解释器，都可以交互，都可以输入文件执行，输入的文件我们通常称之为脚本，最终都是靠 bash、pytho、node 这些解释器在运行和操作系统交互。二进制可执行文件不需要解释器，它直接和操作系统交互，通过 syscall 调用完成很多事情，操作系统只要找到文件中的入口就可以了，在 Linux 中二进制文件执行步骤如下： ┌──────────────────┐ ┌───────────────────┐ ┌─────────────────────┐ │ │ │ │ │ │ │ parse elf header ├───────► read file context ├───────►exec from entry point│ │ │ │ to memory │ │ │ └──────────────────┘ └───────────────────┘ └─────────────────────┘ 我们可以通过 readelf -h main 查看 entry point [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ readelf -h /usr/bin/python ELF Header: Magic: 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 Class: ELF64 Data: 2's complement, little endian Version: 1 (current) OS/ABI: UNIX - System V ABI Version: 0 Type: EXEC (Executable file) Machine: Advanced Micro Devices X86-64 Version: 0x1 Entry point address: 0x400665 Start of program headers: 64 (bytes into file) Start of section headers: 5288 (bytes into file) Flags: 0x0 Size of this header: 64 (bytes) Size of program headers: 56 (bytes) Number of program headers: 9 Size of section headers: 64 (bytes) Number of section headers: 29 Section header string table index: 28 操作系统通过 Entry point address 就能找到二进制可执行文件的执行入口地址 ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:2:3","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"环境变量 谈到环境变量，可能有人会认为环境变量就是 $PATH，这种情况非常常见，环境、环境变量和 $PATH 傻傻分不清 他们的关系如下 ┌─────────────────────────────────────────────┐ │ │ │ environment │ │ │ │ ┌─────────────────────────────────────┐ │ │ │ env value │ │ │ │ │ │ │ │ ┌────────────────┐ │ │ │ │ │ │ │ │ │ │ │ PATH │ │ │ │ │ │ │ │ │ │ │ └────────────────┘ │ │ │ └─────────────────────────────────────┘ │ └─────────────────────────────────────────────┘ 我们可以把环境想象成正在执行的程序，环境变量是我们正在执行的程序中的一些全局变量，PATH 变量是一个比较特殊的变量 在操作系统这个环境中，我们可以随意的读取环境变量的值 [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ echo $SHELL /bin/bash [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ echo $PWD /home/jichun_pu [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ echo $HOME /home/jichun_pu [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ echo $USER jichun_pu 这是一些比较常见的环境变量，这些环境变量可能每个用户都不一样，他们会在用户登录的时候进行加载。 ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:3:0","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"环境变量加载 系统启动的时候首先会加载系统级环境变量：通过执行 /etc/profile 和 /etc/environment 完成系统级环境变量的加载 然后回加载用户级别的环境变量，用户级别的环境变量只会对当前用户生效，首先是 $HOME/.profile，它只会在用户第一次登录的时候执行一次，设置全局的环境变量，定义登录时需要执行的初始化命令等行为。然后应该就是我们说的 $SHELL 的配置文件，如果是 bash，那么加载的就是 $HOME/.bashrc，如果是 zsh，就是 $HOME/.bashrc，如果是 fish，就是 $HOME/.config/fish/ 文件夹中的所有文件，$SHELL 的配置文件会在每次打开新的 $SHELL 的时候执行 这些文件大多数都是用 bash 写的，除了在指定条件加载外，我们还可以通过手动加载他们 source $HOME/.bashrc 有的安装软件的教程中会在告诉你将二进制文件除了放到 /usr/bin 中之外，可能也会告诉你修改 /etc/profile 中 $PATH 变量的值，从而达到在环境中找到这个可执行文件 但是我希望告诉你的是这两种方法在大多数场景下都不是那么好，因为他们太强势了 ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:3:1","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"$PATH 变量 前面说了半天 $PATH ，那么 $PATH 到底是什么，$PATH 环境变量在 Linux 和类 Unix 系统指定了 shell 在搜索可执行文件时应该查找的路径列表。具体来说，$PATH 定义了系统在运行命令时查找可执行文件的顺序和位置。 [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ echo $PATH /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin 也就是说，如果我想要执行 ping 命令，shell 会根据 $PATH 中的路径列表挨个去寻找有没有 ping 这个拥有可执行权限的文件 事实上我们可以很轻松的在 $PATH 的路径列表中找到 ping [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ which ping /usr/bin/ping ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:3:2","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"定义变量 我们说 $PATH 路径列表，并不是说 $PATH 的值是一个列表，它是一个字符串，是一个包含多个路径的单个变量，每个路径由冒号分隔开来。 所以在 /etc/profile 或 $HOME/.bashrc 中修改 $PATH 的值的时候我们做的是字符串拼接 export PATH=/usr/local/node/bin:$PATH 在 bash 或 zsh 中，想要读取变量的值，需要在变量之前加上一个 $，但是我们在定义的时候并不需要使用 $ 这个符号。 我们可以在 $SHELL 中定义一个变量，然后读取这个变量，然后尝试在脚本中读取这个变量看看会发生什么 [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ TEST_VAR=\"Hello\" [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ echo $TEST_VAR Hello [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ nvim envtest.sh [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ cat envtest.sh #!/usr/bin/bash echo $TEST_VAR [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ chmod +x envtest.sh [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ ./envtest.sh [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ 最终我们可以看到执行 envtest.sh 脚本之后什么都没有输出，这说明在脚本中没有读取到这个变量。 我们在 $SHELL 或 脚本文件中定义的变量不一定能让别当前进程的子进程或其他进程读取到，因为他们可能没有使用 export 提升为环境变量，他们还只是变量，变量只能在当前进程中访问到，export 的作用就是让这个变量导出，而环境变量可以让当前线程的子线程也访问到 换句话说，如果我现在打开了 SSH 连接会话，我执行了 export TEST_VAR=Hello，那么在我没有关闭这个 SSH 会话前开启的所有程序都能读取到这个环境变量，而当我关闭这个 SSH 会话重新连接，或者直接重新连接，那么 TEST_VAR 这个环境变量就会失效 那么为什么写在 /etc/profile 或 $HOME/.bashrc 中的环境变量每个会话都能生效，因为每次用户登录的时候都重新加载了它们 至此说回前文，为什么不推荐直接修改 /etc/profile 中 $PATH 的值，或直接将二进制文件放入 /usr/bin/ 目录中。 /etc/profile 中的环境变量所有用户都要加载，如果直接操作它，可能会给所有用户带来影响 /usr/bin 每个用户的 $PATH 变量中都有 /usr/bin 这个路径，直接将二进制文件放进去也可能给别的用户带来影响。并且如果二进制文件有其他依赖也跟着放进去，后果想必也能想到是什么情况 笔者更建议修改 $HOME/.bashrc 文件，只会对当前用户产生影响，如果是安装的类似于 python、node 这样有他们自己依赖的一些 lib 文件的二进制文件，建议使用软链接，链接到 /usr/bin 中 那么 $HOME/.profile 中应该写什么些什么呢？对于没有将 Linux 当作日常使用的读者可能会有此困惑，其实也很正常，大家一般使用的都是 SSH 连接服务器，服务器可以不用关机的，因此也就没有感知过第一次登录的情况，但是这其实和 Windows 下差不多，Windows下电脑重启后用户输入密码进入之后会启动一些 登录项 也就是自动启动的任务，$HOME/.profile 就是干这件事的，我们在 Linux 中可能会在第一次登录的之后让 fcitx、compton 或 picom、bluetooch 在后台启动，以及显示器缩放倍数等操作基本上都是只会在第一次登录的时候执行一次 ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:3:3","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"工程师运维 ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:4:0","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"程序运行权限 在前面讲的权限中我们可以知道在正常情况下，我们想要在非用户目录下以非 root 身份运行程序似乎是比较麻烦的，大多数时候我们没有非 $HOME 下的写入权限。那是否可以将代码放到 $HOME 下运行，答案是最好不要，那什么时候可以： 你确保这台服务器现在将来都只有你使用 你确保你会在公司工作到最后 如果以上两点如果无法保证，那么你能保证交接时你能处理好（PS：这种情况我自己都觉得头大） 那么我们可以以 root 身份运行程序吗？答案也是最好不要，那么什么时候可以： 像 docker 这种难缠的东西不得不使用的时候（其实通过用户组可以避免，但是用了用户组并不能以逸待劳） 似乎我们只得使用当前用户运行，看来最好是这样的，但是有时候也不必太死板，我只是认为如果你需要在看这篇文章认为我写的有用，并且对权限这个东西无所畏惧那么最好还是注意，以免有一天你在裸机中运行的程序爆发漏洞或者用了开源库被留了暗门导致无可了一些无法挽回的损失发生。 在现代运维手段下，我们的程序如果还跑在裸机上，肯定还有一些不得已的问题，不管是历史还是个人问题，即然已经造成所以更应注意 你要知道的是，以非 root 用户权限运行，几乎不会对你的程序造成任何阉割。要在服务器上存文件，找一个磁盘空间足够的路径创建一个文件夹然后赋予读写权限就可以了，除此之外我想不到其他任何需要提权的地方。 你可能会说我的程序需要 kill 别的进程，如果是你自己的创建的进程，肯定有权限去 kill。如果是别人的进程，我们就不应该在程序中 kill，这是一件慎重的事情，应在在检查确定问题后 kill ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:4:1","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"grep 在使用 grep 的时候我们应该避免使用 cat filename | grep xxx 这种方式筛选文件，如果文件过大会给服务器造成很大压力，grep 命令可以直接指定文件然按行读取 grep xxx filename grep 时写正则表达式如果觉得写很多转义符号比较麻烦，可以考虑增加 -E flag ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:4:2","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"ps命令 当我们使用 ps 命令的时候只会将在前台的任务显示出来 [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ ps PID TTY TIME CMD 10052 pts/18 00:00:00 bash 10402 pts/18 00:00:00 ps 我比较常用的是 ps -aux 这样会显示出所有的进程包括后台的进程，并且会显示足够多的信息 USER: 启动这个进程的用户（进程所属用户） PID: 进程ID（Process ID） %CPU: 进程CPU占用率 %MEM: 进程内存占用率 VSZ: 进程占用的虚拟内存大小（Virtual Memory Size） RSS: 进程占用物理内存大小 TTY: 启动进程的终端类型 STAT: 进程当前状态 R: Running S: Sleeping T: Stopped Z: Zombie （僵尸进程） s: 表示进程是父进程，并且有子进程 其他状态请自行查阅 START: 进程创建的时间 TIME: 进程在CPU上执行的时间 COMMAND: 进程启动命令 ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:4:3","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"top top 是一个非常好用的命令，它非常强大，奈何我能力有限，我通常只用它来快速预览一下系统信息和查找 CPU 或 内存占用高的进程 top - 11:44:59 up 769 days, 20:19, 2 users, load average: 0.40, 0.43, 0.39 第一行表示：当前系统时间是 11:47:17，系统已经运行了 769 天 20 小时 19 分钟，当前系统登录的用户数量为 2，系统 1 分钟以内负载为 0.40，5 分钟以内负载为 0.43，15 分钟以内负载为 0.39 Tasks: 203 total, 1 running, 201 sleeping, 1 stopped, 0 zombie 第二行表示：系统进程总数为 203，1 个正在运行中，201 个正在休眠中，1 个已经停止，0 个僵尸进程 %Cpu(s): 9.4 us, 1.6 sy, 0.0 ni, 89.1 id, 0.0 wa, 0.0 hi, 0.4 si, 0.0 st 第三行表示：用户态 CPU 占用百分比为 9.4（9.4 us），内核态 CPU 占用百分比为 1.6（1.6 sy），用户态改变过优先级的进程占用的 CPU 百分比为 0.0（0.0 ni）， 空闲的 CPU 百分比为 89.1（89.1 id），等待中的 CPU 百分比为 0.0（0.0 wa） 至于下面的进程信息和使用 ps 命令看到的基本一致。 如果没有对 top 界面进行设置，那么看到的就是这样，还是挺难看的。在 top 界面下按下 ? 就可以进入帮助界面 Help for Interactive Commands - procps-ng version 3.3.10 Window 1:Def: Cumulative mode Off. System: Delay 3.0 secs; Secure mode Off. Z,B,E,e Global: 'Z' colors; 'B' bold; 'E'/'e' summary/task memory scale l,t,m Toggle Summary: 'l' load avg; 't' task/cpu stats; 'm' memory info 0,1,2,3,I Toggle: '0' zeros; '1/2/3' cpus or numa node views; 'I' Irix mode f,F,X Fields: 'f'/'F' add/remove/order/sort; 'X' increase fixed-width L,\u0026,\u003c,\u003e . Locate: 'L'/'\u0026' find/again; Move sort column: '\u003c'/'\u003e' left/right R,H,V,J . Toggle: 'R' Sort; 'H' Threads; 'V' Forest view; 'J' Num justify c,i,S,j . Toggle: 'c' Cmd name/line; 'i' Idle; 'S' Time; 'j' Str justify x,y . Toggle highlights: 'x' sort field; 'y' running tasks z,b . Toggle: 'z' color/mono; 'b' bold/reverse (only if 'x' or 'y') u,U,o,O . Filter by: 'u'/'U' effective/any user; 'o'/'O' other criteria n,#,^O . Set: 'n'/'#' max tasks displayed; Show: Ctrl+'O' other filter(s) C,... . Toggle scroll coordinates msg for: up,down,left,right,home,end k,r Manipulate tasks: 'k' kill; 'r' renice d or s Set update interval W,Y Write configuration file 'W'; Inspect other output 'Y' q Quit ( commands shown with '.' require a visible task display window ) Press 'h' or '?' for help with Windows, Type 'q' or \u003cEsc\u003e to continue 我一般会按 e 将界面信息美化一下，然后按 xb 增强一下显示效果 被选中的列是排序行，被选中的行就是被选中的进程，如果觉得每次进入都要设置麻烦，也可以按 W 将配置写入配置文件 我们想要选择排序的列，可以通过键盘上的 \u003c 和 \u003e 来进行移动 ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:4:4","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"定时任务 如果有需要定时执行的脚本，在 Linux 中可以使用 crontab，但是在设置定时任务的时候应该避免使用 crontab -e 这种方式会按照用户进行区分，如果服务器上有定时任务异常，那么这样会给其他排查的人员造成不必要的麻烦 我比较建议直接修改 /etc/crontab，在这个文件中我们同样可以指定定时任务执行的用户，并且这样也能对定时任务有比较好的区分，任何人打开这个文件就知道在这台服务器上有哪些人在上面运行什么定时任务 ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:4:5","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"归档日志筛选 当线上程序或数据出现问题的时候我们通常会到日志服务器查看日志，如果日志已经归档，并且需要筛选的日志时间跨度大可以修改下面的脚本后使用 #!/bin/bash # 解压当前文件夹下的所有tar.gz文件 for file in *.tar.gz; do echo \"Extracting $file...\" tar -xzf \"$file\" || { echo \"Failed to extract $file\"; continue; } # 获取解压后的文件名 extracted_file=$(basename \"$file\" .tar.gz) # 使用grep进行筛选 grep -E \"hello|world\" \"$extracted_file\" # 删除解压出来的文件 echo \"Cleaning up...\" rm \"$extracted_file\" done echo \"All done!\" ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:4:6","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"进程查杀 # 批量查杀进程 for i in `ps -aux | grep \"nvim\" | awk '{print $2}'`; do kill -9 $i; done; 对于进程查杀这是很常见的一个操作列几点需要注意的 避免执行类似于 echo \"Kill Process xxx\" 之后就就没有下文的脚本 避免脚本中查询的进程匹配条件覆盖脚本名称，如果脚本名称也被匹配了，执行脚本的进程也会被杀掉 如果出现进程 kill 不了的情况，可以考虑 kill 这个进程的父进程 如果出现僵尸进程，考虑检查一下代码，通常情况下僵尸进程出现在 Python 多进程脚本中，如果僵尸进程过多，建议重启父进程 ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:4:7","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"lsof lsof (list open files) 可以列出进程打开的文件、打开文件的进程、进程打开的端口（TCP/UDP） 因为在 Linxu 中任何资源都以文件的形式存在，文件不仅可以访问常规数据，还可以访问网络连接和硬件。系统在后台都为网络资源和硬件资源都分配了一个文件描述符 如果在 Linux 中我们在后台运行了一个脚本，但是脚本被我们不小心删除了，这时候我们可以通过lsof 定位到占用该文件的进程（注意：如果进行停止了，那么这个文件也就无法被恢复了），然后通过进程文件还原改文件 lsof | grep -i delete | grep {filename} 查询到进程 ID 之后就可以在该进程文件夹中找到被删除的文件 ls /proc/{PID}/fd 查询占用端口的进程 lsof -i:8080 ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:4:8","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"default editor 在 Linux 我使用 nvim 编辑器 我一般不会考虑在服务器上修改它，但是你可以作为一个了解 默认编辑器对于我而言是非常重要的一项设置，如果你知道 sudoedit 命令，那么应该会考虑使用 sudoedit 替代 sudo nvim，因为这是一个更安全的行为。它会创建一个零时文件，文件的内容和目标文件相同，当我们修改完文件之后会将文件覆盖过去 设置默认编辑器我们可以修改 /etc/sudoers 文件完成，但是系统给我们提供了一个更安全的命令 visudo visudo 修改或增加一行就可以完成默认编辑器的设置 Defaults editor=/usr/bin/nvim 如果感觉这样太暴力，或者没有权限修改这个文件，Linux 还支持环境变量 $EDITOR，只需要在 ~/.bashrc 中设置即可 export EDITOR=/usr/bin/nvim 如果你能执行 visudo 那么可能会发现里面有一个用户组叫 wheel，这是 Linux 为我们准备的一个用户权限非常高的用户组，如果有一天你也使用 Linux 作为自己的主要系统应该会使用到它 ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:4:9","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"keybind 和 alias ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:5:0","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"keybind shell 中通常都会有一个 vi mode，这个模式能帮助我们让光标在命令上快速跳转，这对于熟悉 vim 的人来说是好用的，我当然建议你进行尝试，但是这无疑会增加非常多的学习成本，所以我更推荐使用快捷键，这些快捷键都是通过 bind 命令绑定的，其中也包括 vi mode 的快捷键 bind -p 通过这个命令我们大概可以看到有哪些快捷键，因为我习惯使用 vi mode 所以我只知道几个快捷键，你可以通过看 bind -p 的输出找到你需要的快捷键，或者直接问 AI，我推荐几个我常用的快捷键 ctrl+r: 搜索历史命令 ctrl+l: 清空屏幕 ctrl+u: 删除光标前的内容 ctrl+w: 删除光标前的单词 ctrl+x ctrl+u: 撤销 如果你觉得某组连串操作需要经常使用，那么你也可以通过 bind key 将快捷键和封装好的命令绑定 ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:5:1","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["Linux"],"content":"alias 对于前面说的绑定快捷键这样的事情应该很少有人会这样做，我也仅仅因为使用 joshuto 绑定过快捷键，我们更多的需求往往是想将一串我经常输入的命令变得简短，比如 # 激活虚拟环境 source ./.venv/bin/activate # 进入代码文件夹 cd /opt/coderoot 我们可以在 shell 中执行下面的命令然后进行尝试 alias sdv=\"./.venv/bin/activate\" alias cdc=\"cd /opt/coderoot\" [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ alias cdc=\"cd /opt/coderoot\" [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ pwd /home/jichun_pu [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ cdc [jichun_pu@Qxbspideronline-10-2-6-10 coderoot]$ cd [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ pwd /home/jichun_pu [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ unalias cdc [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ cdc -bash: cdc: command not found [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ echo 'alias cdc=\"cd /opt/coderoot\"' \u003e\u003e ~/.bashrc [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ source ~/.bashrc [jichun_pu@Qxbspideronline-10-2-6-10 ~]$ cdc [jichun_pu@Qxbspideronline-10-2-6-10 coderoot]$ pwd /opt/coderoot [jichun_pu@Qxbspideronline-10-2-6-10 coderoot]$ 这是非常好用的，甚至还可以定义一个函数，用 alias 给函数起一个别名方便调用，写入完成之后执行 source ~/.bashrc 就会生效 ","date":"2024-07-15","objectID":"/posts/linux%E6%8C%87%E5%8C%97/:5:2","tags":["Linux"],"title":"Linux 指北","uri":"/posts/linux%E6%8C%87%E5%8C%97/"},{"categories":["rust"],"content":"最近有一个简单的面试 demo，程序涉及前后端和数据库，使用 docker-compose 编排容器。由于我第一次使用 Axum，并且太久没有使用 docker 了于是记录一下过程 ","date":"2024-03-09","objectID":"/posts/rust_axum/:0:0","tags":["rust"],"title":"Rust Axum框架","uri":"/posts/rust_axum/"},{"categories":["rust"],"content":"需求 写一个发票录入系统，两个页面，一个是发票页面，另一个是发票列表页面，需要实现对发票的增删改查，前后端程序和数据库需要使用 docker-compose 编排 ","date":"2024-03-09","objectID":"/posts/rust_axum/:1:0","tags":["rust"],"title":"Rust Axum框架","uri":"/posts/rust_axum/"},{"categories":["web"],"content":"节流是控制函数在一定时间间隔内只执行一次，防抖是确保函数在停止触发后延迟一定时间再执行。 在前端中节流和防抖是常见的优化手段 ","date":"2024-01-14","objectID":"/posts/%E9%98%B2%E6%8A%96%E4%B8%8E%E8%8A%82%E6%B5%81/:0:0","tags":["web"],"title":"防抖与节流","uri":"/posts/%E9%98%B2%E6%8A%96%E4%B8%8E%E8%8A%82%E6%B5%81/"},{"categories":["web"],"content":"节流和防抖的定义 节流：控制函数在一定时间间隔内只执行一次 防抖：确保函数在停止触发后延迟一定时间再执行 节流：类似于游戏中的技能冷却，假设技能只能在10s内触发一次，当我们使用了这个技能之后，在之后的10s内技能会进入冷却期，无法再次使用，只有等cd结束，才能再次使用。 防抖：类似于LOL中的回城技能，假设当我们开始读条回城时，此时会有一个回城计时器。如果不做操作，回城计时器计时结束后，便可以回城；但如果在读条期间，打断了回城，此时想要再回城需要等待重新读条完成才能回城。 ","date":"2024-01-14","objectID":"/posts/%E9%98%B2%E6%8A%96%E4%B8%8E%E8%8A%82%E6%B5%81/:1:0","tags":["web"],"title":"防抖与节流","uri":"/posts/%E9%98%B2%E6%8A%96%E4%B8%8E%E8%8A%82%E6%B5%81/"},{"categories":["web"],"content":"函数定义 节流和防抖函数本质上是通过闭包设置定时器完成，指定时间间隔内函数只能执行一次，和指定时间间隔内触发的函数只会执行最后一次调用，并且期间只要有调用就会重新计时。 ","date":"2024-01-14","objectID":"/posts/%E9%98%B2%E6%8A%96%E4%B8%8E%E8%8A%82%E6%B5%81/:2:0","tags":["web"],"title":"防抖与节流","uri":"/posts/%E9%98%B2%E6%8A%96%E4%B8%8E%E8%8A%82%E6%B5%81/"},{"categories":["web"],"content":"节流 节流是指定时间间隔内函数只能执行一次，如果换一句话说，也就是指定时间间隔内如果多次调用函数，只会执行第一次调用 const throttle(fn, delay) =\u003e { let timer = null return (...args) =\u003e { // 使用定时器 是否为 null 作为判断条件 // 如果为 null 则说明函数在 delay 时间内已经执行过 if (timer) return // fn 写在 setTimeout 外是执行后等待 delay 才能再次触发，即先触发再等待 fn.call(undefined, ...args); timer = setTimeout(() =\u003e { // 清空定时器 timer = null }, delay) } } ","date":"2024-01-14","objectID":"/posts/%E9%98%B2%E6%8A%96%E4%B8%8E%E8%8A%82%E6%B5%81/:2:1","tags":["web"],"title":"防抖与节流","uri":"/posts/%E9%98%B2%E6%8A%96%E4%B8%8E%E8%8A%82%E6%B5%81/"},{"categories":["web"],"content":"防抖 指定时间间隔内触发的函数只会执行最后一次调用，并且期间只要有调用就会重新计时 const debounce = (fn, delay) =\u003e { let timer = null return (...args) =\u003e { // 如果定时器不为 null 则说明有函数在等待执行，则需要清楚定时器打断等待 if (timer !== null) { clearTimeout(timer) } timer = setTimeout(() =\u003e { // 等待 delay 之后才能执行，期间有调用就会被清除 fn.call(undefined, ...args) timer = null }, delay) } } ","date":"2024-01-14","objectID":"/posts/%E9%98%B2%E6%8A%96%E4%B8%8E%E8%8A%82%E6%B5%81/:2:2","tags":["web"],"title":"防抖与节流","uri":"/posts/%E9%98%B2%E6%8A%96%E4%B8%8E%E8%8A%82%E6%B5%81/"},{"categories":["web"],"content":"应用场景 ","date":"2024-01-14","objectID":"/posts/%E9%98%B2%E6%8A%96%E4%B8%8E%E8%8A%82%E6%B5%81/:3:0","tags":["web"],"title":"防抖与节流","uri":"/posts/%E9%98%B2%E6%8A%96%E4%B8%8E%E8%8A%82%E6%B5%81/"},{"categories":["web"],"content":"节流 节流函数主要用于那些需要控制函数执行频率，但又不想完全阻止函数执行的场景： 滚动事件处理：在Web页面中，当用户滚动页面时，会频繁触发滚动（scroll）事件。如果为滚动事件绑定了复杂的处理函数（如动态加载内容、计算位置等），那么这些函数可能会因为过于频繁的执行而导致性能问题。使用节流函数可以限制这些处理函数的执行频率，从而优化页面性能。 窗口大小调整（resize）：当用户调整浏览器窗口大小时，会触发resize事件。类似于滚动事件，resize事件也可能因为用户频繁的调整窗口大小而频繁触发。使用节流函数可以减少resize事件处理函数的执行次数，避免不必要的计算和渲染。 键盘输入：在搜索框中输入文本时，如果为键盘的keyup或keydown事件绑定了搜索请求函数，那么随着用户输入速度的加快，搜索请求也会越来越频繁。这不仅会给服务器带来压力，还可能因为请求过多而导致搜索结果的延迟。使用节流函数可以限制搜索请求的发送频率，提高用户体验。 游戏开发：在游戏开发中，一些操作（如连续点击、快速移动等）可能需要被限制在一定频率内执行，以避免游戏逻辑的错误或过度消耗系统资源。节流函数可以帮助开发者控制这些操作的执行频率。 动画和过渡效果：在创建动画或过渡效果时，可能需要限制某些操作（如更新动画状态、计算动画帧等）的执行频率，以确保动画的流畅性和性能。节流函数可以在这方面发挥重要作用。 自动完成功能：在自动完成输入框（如搜索框、代码编辑器中的自动补全等）中，用户输入文本时，需要动态显示建议列表。如果为每次输入都发送请求或执行搜索算法，可能会导致性能问题。使用节流函数可以减少请求或搜索算法的执行次数，提高响应速度和性能。 ","date":"2024-01-14","objectID":"/posts/%E9%98%B2%E6%8A%96%E4%B8%8E%E8%8A%82%E6%B5%81/:3:1","tags":["web"],"title":"防抖与节流","uri":"/posts/%E9%98%B2%E6%8A%96%E4%B8%8E%E8%8A%82%E6%B5%81/"},{"categories":["web"],"content":"防抖 防抖函数主要用于那些需要等待连续事件停止触发后再执行操作的场景： 搜索框搜索输入：当用户在搜索框中输入内容时，每次输入都会触发input事件。如果每次输入都立即发送请求到服务器获取搜索结果，这不仅可能导致用户体验不佳（因为请求过多可能造成延迟），而且也没有必要。通过防抖函数，可以确保只有当用户停止输入一段时间后才发起搜索请求。这样既能减少服务器压力，又能提高搜索效率。 窗口大小调整（resize）：用户可能快速地调整浏览器窗口大小，导致resize事件在短时间内被大量触发。如果每次调整都重新计算布局或刷新数据，可能会引起性能问题。防抖函数能确保在窗口大小稳定后，才执行相关操作，如重新计算布局、调整样式等。 表单验证与提交：在多步表单填写场景中，用户可能连续点击“下一步”或“提交”按钮，而实际上只需要最后一个点击动作有效。应用防抖函数可以确保最后一次点击后的等待时间过后才进行表单验证和提交操作，从而避免重复提交或不必要的验证。 无限滚动加载：在用户滚动页面到底部时自动加载更多内容的功能中，如果不做防抖处理，可能会在接近底部时由于页面滚动造成的连续触发多次加载请求。通过防抖技术，可以确保在滚动行为稳定后（即用户停止滚动后）才执行一次加载新数据的操作，从而避免不必要的请求和性能问题。 高频点击事件：如登录、发短信等按钮的点击事件，如果用户点击过快，可能会发送多次请求到服务器。使用防抖函数可以限制这类高频点击事件的执行频率，确保在指定的时间间隔内只执行一次操作，从而避免重复请求和不必要的资源消耗。 文本编辑器实时保存：在文本编辑器中，当用户进行编辑时，可能需要实时保存内容以防止数据丢失。然而，如果每次字符输入都触发保存操作，可能会导致性能问题。通过防抖函数，可以设定一个时间间隔（如1秒），只有在用户停止输入一段时间后，才执行保存操作。这样既能保证数据的安全性，又能提高编辑器的性能。 ","date":"2024-01-14","objectID":"/posts/%E9%98%B2%E6%8A%96%E4%B8%8E%E8%8A%82%E6%B5%81/:3:2","tags":["web"],"title":"防抖与节流","uri":"/posts/%E9%98%B2%E6%8A%96%E4%B8%8E%E8%8A%82%E6%B5%81/"},{"categories":["web"],"content":"总结 防抖和节流这两个函数特点还是还是非常明显的，我们在业务中使用的时候只需要在进行优化的地方根据防抖和节流的特性进行评估就可以了 ","date":"2024-01-14","objectID":"/posts/%E9%98%B2%E6%8A%96%E4%B8%8E%E8%8A%82%E6%B5%81/:4:0","tags":["web"],"title":"防抖与节流","uri":"/posts/%E9%98%B2%E6%8A%96%E4%B8%8E%E8%8A%82%E6%B5%81/"},{"categories":["self"],"content":"2023 我的主题是思考。我也无人诉说，那便留在心里，随着时间流逝也就消散了，我是一个庸俗的人，那便在此留下让我觉得有意思的事情，希望以后每年如此。 ","date":"2023-12-31","objectID":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/:0:0","tags":["self"],"title":"2023年总结","uri":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/"},{"categories":["self"],"content":"最佳动漫台词 旅行的话，你想去哪里？—— 《海贼王》- 懦弱胆小的暴君熊 当你严阵以待的时候，死神是不会找上你的，哈萨维。———《高达机动战士——闪光的哈萨维》- 阿姆罗 ","date":"2023-12-31","objectID":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/:0:1","tags":["self"],"title":"2023年总结","uri":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/"},{"categories":["self"],"content":"最佳编程语言 Rust，C# ","date":"2023-12-31","objectID":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/:0:2","tags":["self"],"title":"2023年总结","uri":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/"},{"categories":["self"],"content":"最佳软件 windows terminal，能满足我所有需求而又简洁的终端 arc，改变了我的浏览器使用习惯 ","date":"2023-12-31","objectID":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/:0:3","tags":["self"],"title":"2023年总结","uri":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/"},{"categories":["self"],"content":"最佳硬件 switch，带来了非常多的快乐与感动 罗技M750M，小巧好用 ","date":"2023-12-31","objectID":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/:0:4","tags":["self"],"title":"2023年总结","uri":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/"},{"categories":["self"],"content":"最佳饮料 外星人电解质水西柚味，喝了好几箱了 ","date":"2023-12-31","objectID":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/:0:5","tags":["self"],"title":"2023年总结","uri":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/"},{"categories":["self"],"content":"最佳游戏 塞尔达传说——王国之泪 ","date":"2023-12-31","objectID":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/:0:6","tags":["self"],"title":"2023年总结","uri":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/"},{"categories":["self"],"content":"最佳电影 高达机动战士——闪光的哈萨维 死这种事情，大家都做过了，我也应该能做好。 时间给予我忘却，这话是谁写下的呢？ 说出这种话的人，是乐天派，抑或是深知对于真实、绝望的恐惧的人吧。不论对哪一方而言，可以说，所谓的言语，都有着多重性和暧昧性，并不能表达真相。 然而，用言语来讲述的这个故事，尽管是一个在不论什么时代都会被讲述的故事，但不论多少次也都有必要继续讲述以流传于后世。 人世间的悲伤、正因为人世的存在而产生的悲哀… 这种悲哀，直到“产生这的原因是人的存在”这样一种恐惧，带着单纯的构造向我们逼来。 一边祈求希望能够幸福、一边却又使得幸福溜走的人们，真是可悲啊。 之后，仅仅是梦想着何日会解脱、会被解放，被想要高呼“如果去做在这个故事中登场的人们所能做到的事，这是人类的悲剧”这样一种冲动所驱使。 ","date":"2023-12-31","objectID":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/:0:7","tags":["self"],"title":"2023年总结","uri":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/"},{"categories":["self"],"content":"最佳好物 LOL S13 提伯斯玩偶 Trudi河狸公仔挂件 PS: 我可太喜欢娃娃了 ","date":"2023-12-31","objectID":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/:0:8","tags":["self"],"title":"2023年总结","uri":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/"},{"categories":["self"],"content":"最佳书籍 硅谷革命，成就苹果公司的疯狂往事 ","date":"2023-12-31","objectID":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/:0:9","tags":["self"],"title":"2023年总结","uri":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/"},{"categories":["self"],"content":"最佳音乐 愿与愁 ","date":"2023-12-31","objectID":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/:0:10","tags":["self"],"title":"2023年总结","uri":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/"},{"categories":["self"],"content":"比较大的影响 主要开发编辑器使用neovim，使用vim三年终于下定决心，适应了一个月已经有了非常好的体验 学习使用rust开发操作系统内核，这将是影响我职业生涯的又一个开端 买了switch玩塞尔达，从2017年知道塞尔达这个游戏开始，等了6年，音乐响起的那一刻一切都值得 见识到了超强的路由器unifi，能决解我对网络的几乎所有的需求 chatGPT的出现让我短时间打破了很多行业或技术墙壁 和方健舟浅聊操作系统，让我从原理层面认知到了上层设计其中的原因 深入了解了wasm，今年wasm有很多应用但是声音却更小了，但我认为这一定是未来的一大火热技术，各个平台一定如此 xxxx 今年吃了超多麦当劳的麦旋风，长这么大第一次这么嚣张 ","date":"2023-12-31","objectID":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/:0:11","tags":["self"],"title":"2023年总结","uri":"/posts/2023%E5%B9%B4%E6%80%BB%E7%BB%93/"},{"categories":["rust"],"content":"python urllib3 内存泄漏问题偶尔会蹦出来烦我一下，于是萌生了能不能用 Rust 写一个请求库 ","date":"2023-09-23","objectID":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/:0:0","tags":["rust","python"],"title":"用 Rust 开发一个 python 模块","uri":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/"},{"categories":["rust"],"content":"创建项目 首先使用 cargo 创建一个 lib 项目，项目名称就叫 reqrs 是 request 和 rust 的缩写 cargo new reqrs --lib 然后需要在 Cargo.toml 中指定 crate 的类型为动态链接库 [lib] name = \"reqrs\" crate-type = [\"cdylib\"] 指定为动态链接库之后通过 cargo build 就会在 target/debug 目录下生成一个 .so 文件，如果是 windows 就是 dll 文件 当然，因为当前项目还没有开始编写，我们需要考虑一些其他的东西，比如在 python 中如何调用、python 如何给这个模块传递参数 ","date":"2023-09-23","objectID":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/:1:0","tags":["rust","python"],"title":"用 Rust 开发一个 python 模块","uri":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/"},{"categories":["rust"],"content":"依赖 rust 代码编译为一个动态库，我们可以通过 python 中的 ctypes 或 cffi 导入模块。当然我们最希望能直接安装，然后直接在 python 代码中 import，那么我们就需要使用 maturin 进行打包了。 maturin 能够让使用 pyo3、cffi 和 uniffi 绑定构建和发布的 crate，将它们打包成 python 能够使用的二进制包，pyo3 和 maturin 都是 PyO3 这个组织中的项目 pyo3 能够将 rust 类型和 python 类型进行绑定，用于创建原生的 python 扩展模块 那么到此就能够确定，我们可以通过 maturin 将 reqrs 打包为一个 python 可以安装的使用的模块，通过 pyo3 我们能将 rust 中的数据类型和 python 中的数据类型进行交互绑定 maturin 是工具，可以直接使用 pip 进行安装，为了保证环境的干净先创建一个虚拟环境 python3 -m venv .venv 然后激活虚拟环境 source ./.venv/bin/activate 然后安装 maturin pip install maturin 然后是 reqrs 项目依赖，我们需要使用一个请求库，可以是 reqwest 也可以是更底层的 hyper 为了方便我们使用 reqwest，并且不考虑异步 然后我们还需要 serde ，因为我们可能需要将请求的响应结果中的 json 数据反序列化为结构体 至此 Cargo.toml 中的内容如下 [package] name = \"reqrs\" version = \"0.1.0\" edition = \"2021\" [lib] name = \"reqrs\" crate-type = [\"cdylib\"] [dependencies] pyo3 = { version = \"0.20.0\", features = [\"extension-module\", \"serde\", \"auto-initialize\"] } reqwest = { version = \"0.11.23\", features = [\"json\", \"blocking\", \"cookies\", \"multipart\"] } serde_urlencoded = \"0.7.1\" serde = { version = \"1.0.193\", features = [\"derive\"] } serde_json = {version = \"1.0.109\", features = [\"arbitrary_precision\", \"preserve_order\"]} 可能还需要一些其他依赖，我们后续开发过程中再考虑 ","date":"2023-09-23","objectID":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/:2:0","tags":["rust","python"],"title":"用 Rust 开发一个 python 模块","uri":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/"},{"categories":["rust"],"content":"初试 在正式编写模块之前，我们编写一个 sum 函数试试 // lib.rs use pyo3::prelude::*; #[pyfunction] fn sum(a: i64, b: i64) -\u003e PyResult\u003ci64\u003e { Ok(a + b) } #[pymodule] fn reqrs(py: Python, m: \u0026PyModule) -\u003e PyResult\u003c()\u003e { m.add_function(wrap_pyfunction!(sum, m)?)?; Ok(()) } 上面的代码中使用了两个过程宏 pyfunction 和 pymodule，过程宏会在编译的时候展开，pyfunction 会将函数进行一些处理，包括 python 函数定义以及python 类型的参数和 rust 类型互相转换。pymodule 也是类似的作用，只是针对的是模块，pymodule 的参数比较固定 一个被 pyfunction 宏注解的函数想要导出需要在 pymodule 注解的函数中通过 \u0026PyModule.add_function(wrap_pyfunction!(xxx)) 才能被添加到模块中 短短几行代码使用了两个过程宏和一个声明宏帮我们完成了很多工作，虽然这会导致完全理解它们变得十分困难，但这也足以见得 rust 宏的强大 ","date":"2023-09-23","objectID":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/:3:0","tags":["rust","python"],"title":"用 Rust 开发一个 python 模块","uri":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/"},{"categories":["rust"],"content":"使用 maturin 编译并安装 maturin develop 命令执行成功之后在底部可以看到如下输出则说明模块已经编译安装完成，我们并不需要额外的工作就能直接在 python 中使用它了 Finished dev [unoptimized + debuginfo] target(s) in 0.07s 📦 Built wheel for CPython 3.11 to /var/folders/pj/fv48jvhj2j76t23xdtf2lrd80000gn/T/.tmpVKKo83/reqrs-0.1.0-cp311-cp311-macosx_11_0_arm64.whl ✏️ Setting installed package as editable 🛠 Installed reqrs-0.1.0 我们可以通过命令来查看库是否安装成功，并在 python shell 中使用它 通过上图足以见得这是好用的 ","date":"2023-09-23","objectID":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/:3:1","tags":["rust","python"],"title":"用 Rust 开发一个 python 模块","uri":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/"},{"categories":["rust"],"content":"函数设计 reqrs 中主要包含三个函数，分别是：request、get、post，get 和 post 调用最终调用的函数还是 request，request 通过传入的请求方法再去执行具体逻辑 函数参数上和 python 中的 requests 保持一致 ","date":"2023-09-23","objectID":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/:4:0","tags":["rust","python"],"title":"用 Rust 开发一个 python 模块","uri":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/"},{"categories":["rust"],"content":"HTTPMethod //method.rs #[pyclass(module = \"reqrs.request\", frozen)] #[derive(Debug, Clone, Copy, Default)] pub enum HTTPMethod { #[default] GET, POST, } impl ToPyObject for HTTPMethod { fn to_object(\u0026self, py: Python) -\u003e PyObject { match self { Self::GET =\u003e PyString::new(py, \"GET\").to_object(py), Self::POST =\u003e PyString::new(py, \"POST\").to_object(py), } } } impl From\u003cString\u003e for HTTPMethod { fn from(value: String) -\u003e Self { if let Ok(http_method) = HTTPMethod::from_str(value.as_str()) { return http_method; } HTTPMethod::default() } } impl FromStr for HTTPMethod { type Err = \u0026'static str; fn from_str(s: \u0026str) -\u003e Result\u003cSelf, Self::Err\u003e { match s.to_uppercase().as_str() { \"GET\" =\u003e Ok(Self::GET), \"POST\" =\u003e Ok(Self::POST), _ =\u003e Err(\"Invalid HTTPMethod\"), } } } rust 中的类型想要在 python 中使用需要使用 pyclass 宏，并且要为这个类型实现 ToPyObject 这个 trait，我们需要实现 to_object 方法将 rust 中的类型转为 python 中的类型 为了方便使用我们还为 HTTPMethod 实现了 From 和 FromStr 这两个 trait ","date":"2023-09-23","objectID":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/:5:0","tags":["rust","python"],"title":"用 Rust 开发一个 python 模块","uri":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/"},{"categories":["rust"],"content":"get 和 post get 和 post 函数签名几乎相同，其实对于这两个函数包括将来可能会实现的 delete、head、put、option 等函数我们可以使用声明宏帮我们实现，比如定义一个 requests! 宏 macro_rules! requests { ($name:ident) =\u003e { fn $name(...) -\u003e PyResult\u003cString\u003e { let method: HTTPMethod; match $name { get =\u003e { method = HTTPMethod::GET; }, post =\u003e { method = HTTPMethod::POST; } } request(..., method) } } } requests!(get); requests!(post); 上面的声明宏只是笔者在写到这里的时候突然想到的，内容可能会有错误 // request.rs #[pyfunction] pub fn get( py: Python, url: String, headers: Option\u003c\u0026PyDict\u003e, params: Option\u003c\u0026PyDict\u003e, proxies: Option\u003c\u0026PyDict\u003e, timeout: Option\u003cu64\u003e, cookies: Option\u003c\u0026PyDict\u003e, allow_redirects: Option\u003cbool\u003e, ) -\u003e PyResult\u003cString\u003e { request( py, HTTPMethod::GET, url, headers, params, proxies, timeout, cookies, allow_redirects, None, None, ) } #[pyfunction] pub fn post( py: Python, url: String, headers: Option\u003c\u0026PyDict\u003e, params: Option\u003c\u0026PyDict\u003e, proxies: Option\u003c\u0026PyDict\u003e, timeout: Option\u003cu64\u003e, cookies: Option\u003c\u0026PyDict\u003e, allow_redirects: Option\u003cbool\u003e, data: Option\u003c\u0026PyAny\u003e, json: Option\u003c\u0026PyDict\u003e, ) -\u003e PyResult\u003cString\u003e { request( py, HTTPMethod::POST, url, headers, params, proxies, timeout, cookies, allow_redirects, data, json, ) } ","date":"2023-09-23","objectID":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/:6:0","tags":["rust","python"],"title":"用 Rust 开发一个 python 模块","uri":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/"},{"categories":["rust"],"content":"serde 前面写的函数接口为了兼容 requests.request 的函数签名，导致 data 这个参数只能是 Option\u003c\u0026PyAny\u003e 类型，因为 data 可能是 dict 或 str，当然我们可以定义一个 enum 来表示，但是有点多此一举了，还有一个 json 参数，当然这两个参数只会使用一个，因为它们在 requests.request 中都表示请求的 body（在这里我们只考虑纯文本的body） 在 request 中我们不得不为了 data 这个参数做更多的工作，因为它可能是 form body，也可能是 json body。我们想要给 reqwest 传递 body 参数，可以是 form 或 json 但是他们都需要是 Serialize + ?Sized 因此我们需要实现一个函数能够将 PyObject 转为 Serialize + ?Sized 类型，当然如果 body 的类型就是 text，那么我们当然不在乎，但是由于 body 中的类型不确定，如果是 form 或 json 我们还不太好确定里面的字段，所以优先考虑将 PyObject 类型转为 serde::Value，因为这是最容易的 笔者在 GitHub 上找到一个实现了将 PyObject 转为 serde::Value 的代码文件，但是由于非常长时间没有更新了，所以并不能直接使用，于是对这个文件进行了修改 需要安装 canonical_json 这个依赖 # Cargo.toml canonical_json = \"0.5.0\" /// serdejson.rs use canonical_json::ser::CanonicalJSONError; use pyo3::exceptions::PyTypeError; use pyo3::prelude::*; use pyo3::types::{PyDict, PyFloat, PyList, PyTuple}; #[derive(Debug)] pub enum PyCanonicalJSONError { InvalidConversion { error: String }, PyErr { error: String }, DictKeyNotSerializable { typename: String }, InvalidFloat { value: PyObject }, InvalidCast { typename: String }, } impl From\u003cCanonicalJSONError\u003e for PyCanonicalJSONError { fn from(error: CanonicalJSONError) -\u003e PyCanonicalJSONError { PyCanonicalJSONError::InvalidConversion { error: format!(\"{:?}\", error), } } } impl From\u003cpyo3::PyErr\u003e for PyCanonicalJSONError { fn from(error: pyo3::PyErr) -\u003e PyCanonicalJSONError { PyCanonicalJSONError::PyErr { error: format!(\"{:?}\", error), } } } impl From\u003cPyCanonicalJSONError\u003e for pyo3::PyErr { fn from(e: PyCanonicalJSONError) -\u003e pyo3::PyErr { match e { PyCanonicalJSONError::InvalidConversion { error } =\u003e { PyErr::new::\u003cPyTypeError, _\u003e(format!(\"Conversion error: {:?}\", error)) } PyCanonicalJSONError::PyErr { error } =\u003e { PyErr::new::\u003cPyTypeError, _\u003e(format!(\"Python Runtime exception: {}\", error)) } PyCanonicalJSONError::DictKeyNotSerializable { typename } =\u003e { PyErr::new::\u003cPyTypeError, _\u003e(format!( \"Dictionary key is not serializable: {}\", typename )) } PyCanonicalJSONError::InvalidFloat { value } =\u003e { PyErr::new::\u003cPyTypeError, _\u003e(format!(\"Invalid float: {:?}\", value)) } PyCanonicalJSONError::InvalidCast { typename } =\u003e { PyErr::new::\u003cPyTypeError, _\u003e(format!(\"Invalid type: {}\", typename)) } } } } pub fn to_json(py: Python, obj: \u0026PyObject) -\u003e Result\u003cserde_json::Value, PyCanonicalJSONError\u003e { macro_rules! return_cast { ($t:ty, $f:expr) =\u003e { if let Ok(val) = obj.downcast::\u003c$t\u003e(py) { return $f(\u0026val); } }; } macro_rules! return_to_value { ($t:ty) =\u003e { if let Ok(val) = obj.extract::\u003c$t\u003e(py) { return serde_json::value::to_value(val).map_err(|error| { PyCanonicalJSONError::InvalidConversion { error: format!(\"{}\", error), } }); } }; } if obj.is_none(py) { return Ok(serde_json::Value::Null); } return_to_value!(String); return_to_value!(bool); return_to_value!(u64); return_to_value!(i64); return_cast!(PyDict, |x: \u0026PyDict| { let mut map = serde_json::Map::new(); for (key_obj, value) in x.iter() { let key = if key_obj.is_none() { Ok(\"null\".to_string()) } else if let Ok(val) = key_obj.extract::\u003cbool\u003e() { Ok(if val { \"true\".to_string() } else { \"false\".to_string() }) } else if let Ok(val) = key_obj.str() { Ok(val.to_string()) } else { Err(PyCanonicalJSONError::DictKeyNotSerializable { typename: key_obj .to_object(py) .as_ref(py) .get_type() .name()? .to_string(), }) }; map.insert(key?, to_json(py, \u0026value.to_object(py))?); } Ok(serde_json::Value::Object(map)) }); return_cast!(PyList, |x: \u0026PyList| Ok(serde_json::Value::Array( x.iter() .map(|item| to_json(py, \u0026item.to_object(py)).unwrap()) .collect::\u003cVec\u003cserde_json::Value\u003e\u003e() ))); return_cast!(PyTuple, |x: \u0026PyTuple| Ok(serde_json::Value::Array( x.iter() .map(|item| to_json(py, \u0026item.to_object(py)).unwrap()) .collect::\u003cVec\u003cserde_json::Value\u003e\u003e() ))); return_cast!(PyFloat, |x: \u0026PyFloat| { match serde_json::Number::from_f64(x.value()) { Some(n) =\u003e Ok(serde_json::Value::Number(n)), None =\u003e Err(PyCanonicalJSONError::InvalidFloat { value: x.to_object(py), }), } }); // At this point we can't cast it, set up the error object Err(PyCano","date":"2023-09-23","objectID":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/:7:0","tags":["rust","python"],"title":"用 Rust 开发一个 python 模块","uri":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/"},{"categories":["rust"],"content":"request // request.rs #[pyfunction] pub fn request( py: Python, method: HTTPMethod, url: String, headers: Option\u003c\u0026PyDict\u003e, params: Option\u003c\u0026PyDict\u003e, proxies: Option\u003c\u0026PyDict\u003e, timeout: Option\u003cu64\u003e, cookies: Option\u003c\u0026PyDict\u003e, allow_redirects: Option\u003cbool\u003e, data: Option\u003c\u0026PyAny\u003e, json: Option\u003c\u0026PyDict\u003e, ) -\u003e PyResult\u003cString\u003e { let mut base_url = Url::parse(\u0026url).expect(\"invalid url\"); let mut builder = reqwest::blocking::Client::builder(); if let Some(param_args) = params { let mut args = HashMap::new(); for item in param_args.iter() { let (key, value) = item; let key_str = key.extract::\u003cString\u003e()?; let value_str = value.extract::\u003cString\u003e()?; args.insert(key_str, value_str); } let query_string = serde_urlencoded::to_string(\u0026args).expect(\"parse params args error\"); base_url.set_query(Some(\u0026query_string)); } if let Some(redirect_value) = allow_redirects { if !redirect_value { builder = builder.redirect(reqwest::redirect::Policy::none()); } } if let Some(cookie_pair) = cookies { let jar = reqwest::cookie::Jar::default(); for item in cookie_pair.iter() { let (key, value) = item; let key_str = key.extract::\u003cString\u003e()?; let value_str = value.extract::\u003cString\u003e()?; jar.add_cookie_str(\u0026format!(\"{}={}\", key_str, value_str), \u0026base_url); } builder = builder.cookie_provider(Arc::new(jar)); } if let Some(proxy) = proxies { if let Some(http_proxy_url_obj) = proxy.get_item(\"http\")? { let http_proxy_url = http_proxy_url_obj.extract::\u003cString\u003e()?; let proxy_url = Url::parse(\u0026http_proxy_url).unwrap(); let proxy = Proxy::http(format!( \"{}://{}:{}/{}\", proxy_url.scheme(), proxy_url.host_str().unwrap_or_default(), proxy_url.port().unwrap_or_default(), proxy_url.path() )) .unwrap() .basic_auth( proxy_url.username(), proxy_url.password().unwrap_or_default(), ); builder = builder.proxy(proxy); } if let Some(http_proxy_url_obj) = proxy.get_item(\"https\")? { let http_proxy_url = http_proxy_url_obj.extract::\u003cString\u003e()?; let proxy_url = Url::parse(\u0026http_proxy_url).unwrap(); let proxy = Proxy::https(format!( \"{}://{}:{}/{}\", proxy_url.scheme(), proxy_url.host_str().unwrap_or_default(), proxy_url.port().unwrap_or_default(), proxy_url.path() )) .unwrap() .basic_auth( proxy_url.username(), proxy_url.password().unwrap_or_default(), ); builder = builder.proxy(proxy); } } if let Some(timeout_value) = timeout { builder = builder.timeout(time::Duration::new(timeout_value, 0)) } let mut request_builder: reqwest::blocking::RequestBuilder; match method { HTTPMethod::GET =\u003e { request_builder = builder.build().unwrap().get(base_url); } HTTPMethod::POST =\u003e { request_builder = builder.build().unwrap().post(base_url); if let Some(data_value) = data { // 判断是dict还是str if let Ok(dict) = data_value.downcast::\u003cPyDict\u003e() { //判断header中Content-Type if let Some(hv) = headers { if let Some(content_type) = hv.get_item(\"Content-Type\").unwrap() { let content_type_str = content_type.extract::\u003cString\u003e()?; if content_type_str.contains(\"json\") { let json_value = to_json(py, \u0026dict.to_object(py))?; request_builder = request_builder.json(\u0026json_value); } else { request_builder = request_builder.form(\u0026to_json(py, \u0026dict.to_object(py))?); } } else { request_builder = request_builder.form(\u0026to_json(py, \u0026dict.to_object(py))?); } } else { let json_value = to_json(py, \u0026dict.to_object(py))?; let s = serde_json::to_string(\u0026json_value).unwrap(); println!(\"{}\", s); request_builder = request_builder.form(\u0026json_value); } } else if let Ok(py_str) = data_value.downcast::\u003cPyString\u003e() { let body = py_str.extract::\u003cString\u003e()?; request_builder = request_builder.body(body); } } else if let Some(json_data) = json { request_builder = request_builder.json(\u0026to_json(py, \u0026json_data.to_object(py))?); } } } if let Some(header_args) = headers { let mut header_map = header::HeaderMap::new(); for item in header_args.iter() { let (key, value) = item; let key_str = key.extract::\u003cString\u003e()?; let value_str = value.extract::\u003cString\u003e()?; header_map.insert( header::HeaderName::from_str(\u0026key_str).unwrap(), header::HeaderValue::from_str(\u0026val","date":"2023-09-23","objectID":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/:8:0","tags":["rust","python"],"title":"用 Rust 开发一个 python 模块","uri":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/"},{"categories":["rust"],"content":"安装测试 最后只需要将函数和类导出就可以了 // lib.rs #[pymodule] fn reqrs(py: Python, m: \u0026PyModule) -\u003e PyResult\u003c()\u003e { m.add_function(wrap_pyfunction!(request::request, m)?)?; m.add_function(wrap_pyfunction!(request::post, m)?)?; m.add_function(wrap_pyfunction!(request::get, m)?)?; m.add_class::\u003crequest::HTTPMethod\u003e()?; Ok(()) } 笔者已经测试成功 ","date":"2023-09-23","objectID":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/:9:0","tags":["rust","python"],"title":"用 Rust 开发一个 python 模块","uri":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/"},{"categories":["rust"],"content":"最后 上面的 request 代码有很多重复的地方，嵌套了很多 if-else 笔者认为还有很多优化空间，但是至此只已经足够 ","date":"2023-09-23","objectID":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/:10:0","tags":["rust","python"],"title":"用 Rust 开发一个 python 模块","uri":"/posts/rust_%E5%BC%80%E5%8F%91python%E6%A8%A1%E5%9D%97/"},{"categories":["rust"],"content":" Send 和 Sync 这两个 Trait 在跨线程的时候需要关注 Send: A type is Send if it is safe to send it to another thread. Sync: A type is Sync if it is safe to share between threads (T is Sync if and only if \u0026T is Send) Send 和 Sync 是 Rust 并发机制的基础。因此，Rust 赋予它们许多的特性，以保证它们能正确工作。首当其冲的，它们都是非安全 trait。这表明它们的实现也是非安全的，而其他的非安全代码则可以假设这些实现是正确的。由于它们是标志 trait（它们没有任何关联的方法），“正确地实现” 仅仅意味着实现满足它所需要的内部特征。不正确地实现 Send 和 Sync 会导致未定义行为。 Send 和 Sync 还是自动推导的 trait。和其他的 trait 不同，如果一个类型完全由 Send 或 Sync 组成，那么这个类型本身也是 Send 或 Sync。几乎所有的基本类型都是 Send 和 Sync，因此你能见到的很多类型也就都是 Send 和 Sync。 如果是在单线程的异步执行引擎中，类型是否 Sync 其实是无所谓的，因为无法达成竟态条件，但是在 Tokio 中，异步执行引擎是可以多线程执行的，所以必须如果可变的数据在多个线程之间共享一定是需要是 Sync Rc\u003cT\u003e 一定是 !Send \u0026\u0026 !Sync 因为在 Rc 中是将对象引用 Copy 了多份，并且 Rc 中的计数器本身就不是 Send，也不是 Sync 如果使用 Arc\u003cT\u003e 那么这个 Arc\u003cT\u003e 是不是 Send 和 Sync 要取决于 T ","date":"2023-08-24","objectID":"/posts/rust_send_sync/:0:0","tags":["rust"],"title":"Rust Send 和 Sync","uri":"/posts/rust_send_sync/"},{"categories":["rust"],"content":"Rc 只能在单线程中使用 use std::thread; use std::rc::Rc; fn main() { let v = Rc::new(5); let t = thread::spawn(move || { println!(\"{}\",v); }); t.join().unwrap(); } 上面的代码将 v 的所有权通过 move 转移到子线程中是不正确的 运行 cargo build 之后会报错 error[E0277]: `Rc\u003ci32\u003e` cannot be sent between threads safely --\u003e src/main.rs:5:27 | 5 | let t = thread::spawn(move || { | ------------- ^------ | | | | _____________|_____________within this `{closure@src/main.rs:5:27: 5:34}` | | | | | required by a bound introduced by this call 6 | | println!(\"{}\", v); 7 | | }); | |_____^ `Rc\u003ci32\u003e` cannot be sent between threads safely | = help: within `{closure@src/main.rs:5:27: 5:34}`, the trait `Send` is not implemented for `Rc\u003ci32\u003e` 意思是说 Rc\u003ci32\u003e 没有实现 Send 这个 trait ","date":"2023-08-24","objectID":"/posts/rust_send_sync/:1:0","tags":["rust"],"title":"Rust Send 和 Sync","uri":"/posts/rust_send_sync/"},{"categories":["rust"],"content":"Rc 和 Arc 在 Rust 并发编程中，如果要在多个线程之间共享变量一般会使用 Arc，Arc 就是 Atomic Reference Count 也就是 Atomic 的 Rc 通过源码可以看到 Rc 与 Arc 的区别 #[stable(feature = \"rust1\", since = \"1.0.0\")] impl\u003cT: ?Sized, A: Allocator\u003e !Sync for Rc\u003cT, A\u003e {} #[stable(feature = \"rust1\", since = \"1.0.0\")] impl\u003cT: ?Sized, A: Allocator\u003e !Sync for Rc\u003cT, A\u003e {} #[stable(feature = \"rust1\", since = \"1.0.0\")] unsafe impl\u003cT: ?Sized + Sync + Send, A: Allocator + Send\u003e Send for Arc\u003cT, A\u003e {} #[stable(feature = \"rust1\", since = \"1.0.0\")] unsafe impl\u003cT: ?Sized + Sync + Send, A: Allocator + Sync\u003e Sync for Arc\u003cT, A\u003e {} Rc\u003cT, A\u003e 移除了 Send 和 Sync 这两个特征，而 Arc 则相反，实现了 Sync + Send ","date":"2023-08-24","objectID":"/posts/rust_send_sync/:2:0","tags":["rust"],"title":"Rust Send 和 Sync","uri":"/posts/rust_send_sync/"},{"categories":["rust"],"content":"RwLock 和 Mutex Rust 中 Mutex 只关注独占访问，即使程序想要查看数据，MutexGuard 也会提供对受保护对象的独占引用。 RwLock 是 Mutex 的复杂版本，它能理解独占访问和共享访问的区别，即读和写的区别。它有三种状态：未锁定、单个写锁定、和任意数量的读锁定，它更常用于读多写少的数据。 RwLock 的接口分为两个部分，它没有独占的 lock 方法，而是使用了 read 和 write 方法分别以读写的身份进行锁定。它带有两种保护类型，一种是用于读的 RwLockReadGuard，RwLockReadGuard 实现了 Deref 特性，使行为类似于受保护数据的共享引用；还有一种是用于写的 RwLockWriteGuard，RwLockWriteGuard 实现了 DerefMut 协议，使行为类似于独占引用 #[stable(feature = \"rust1\", since = \"1.0.0\")] unsafe impl\u003cT: ?Sized + Send\u003e Send for RwLock\u003cT\u003e {} #[stable(feature = \"rust1\", since = \"1.0.0\")] unsafe impl\u003cT: ?Sized + Send + Sync\u003e Sync for RwLock\u003cT\u003e {} #[stable(feature = \"rust1\", since = \"1.0.0\")] unsafe impl\u003cT: ?Sized + Send\u003e Send for Mutex\u003cT\u003e {} #[stable(feature = \"rust1\", since = \"1.0.0\")] unsafe impl\u003cT: ?Sized + Send\u003e Sync for Mutex\u003cT\u003e {} RwLock 和 Mutex 都实现了 Send + Sync 这也说明 rust 允许通过它们将 T 发送到另一个线程，同时也允许它们在多个线程持有受保护的剧的共享引用 ","date":"2023-08-24","objectID":"/posts/rust_send_sync/:3:0","tags":["rust"],"title":"Rust Send 和 Sync","uri":"/posts/rust_send_sync/"},{"categories":["rust"],"content":"cargo 1.68 版本开始支持稀疏索引：不再需要完整克隆 crates.io-index 仓库，可以加快获取包的速度。如果 cargo 版本大于等于 1.68，可以直接使用而不需要开启 nightly。 ","date":"2023-08-22","objectID":"/posts/cargo_mirrors/:0:0","tags":["rust"],"title":"Cargo镜像仓库设置","uri":"/posts/cargo_mirrors/"},{"categories":["rust"],"content":"cargo 设置国内源 config file path: $HOME/.cargo/config [source.crates-io] replace-with = 'tuna' [source.tuna] registry = \"sparse+https://mirrors.tuna.tsinghua.edu.cn/crates.io-index/\" 如果感觉清华大学开源软件镜像站的镜像比较慢的话可以使用字节的镜像 http://rsproxy.cn/ ","date":"2023-08-22","objectID":"/posts/cargo_mirrors/:1:0","tags":["rust"],"title":"Cargo镜像仓库设置","uri":"/posts/cargo_mirrors/"},{"categories":["tool"],"content":"安装 安装部分参考清华大学开源镜像站 如果有代理可以直接使用官方文档中的方式直接安装 ","date":"2023-08-21","objectID":"/posts/homebrew_install/:1:0","tags":["Mac","Linux"],"title":"Homebrew 安装和换源","uri":"/posts/homebrew_install/"},{"categories":["tool"],"content":"替换源 同样参考清华大学开源镜像站的文档，但是需要注意，homebrew程序是脚本程序，可以通过更新仓库的方式更新 查看 brew.git 当前源 cd \"$(brew --repo)\" \u0026\u0026 git remote -v 查看 homebrew-core.git 当前源 cd \"$(brew --repo homebrew/core)\" \u0026\u0026 git remote -v 当前版本的文档中提示了 brew 4.0 起，大部分 Homebrew 用户无需设置 homebrew/core 和 homebrew/cask 镜像，只需设置 HOMEBREW_API_DOMAIN 即可。如果需要使用 Homebrew 的开发命令 (如 brew cat \u003cformula\u003e)，则仍然需要设置 homebrew/core 和 homebrew/cask 镜像。 添加以下环境变量，可以放入 $HOME/.bashrc 或者 $HOME/.zshrc export HOMEBREW_BOTTLE_DOMAIN=\"https://mirrors.tuna.tsinghua.edu.cn/homebrew-bottles\" export HOMEBREW_API_DOMAIN=\"https://mirrors.tuna.tsinghua.edu.cn/homebrew-bottles/api\" export HOMEBREW_BREW_GIT_REMOTE=\"https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git\" export HOMEBREW_CORE_GIT_REMOTE=\"https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.git\" 设置好并加载环境变量后，进行更新 brew update 如果需要设置 tap 仓库还需要进行更新 brew tap --custom-remote --force-auto-update homebrew/cask-fonts https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-cask-fonts.git brew tap --custom-remote --force-auto-update homebrew/cask-versions https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-cask-versions.git brew tap --custom-remote --force-auto-update homebrew/command-not-found https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-command-not-found.git brew tap --custom-remote --force-auto-update homebrew/services https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-services.git brew update ","date":"2023-08-21","objectID":"/posts/homebrew_install/:2:0","tags":["Mac","Linux"],"title":"Homebrew 安装和换源","uri":"/posts/homebrew_install/"},{"categories":["tool"],"content":"安装 brew cask brew install brew-cask ","date":"2023-08-21","objectID":"/posts/homebrew_install/:3:0","tags":["Mac","Linux"],"title":"Homebrew 安装和换源","uri":"/posts/homebrew_install/"},{"categories":["python"],"content":" 以下功能中不管是静态检查插件、格式化工具配置、launch profile 等，基本都可以使用 pycharm 和 vscode 做到，但是都同时做到是不可能的，像 black 在 pycharm 中配置起来还是比较麻烦，开发环境依赖都需要借助第三方包管理工具，并且依赖关系混乱，launch profile 每一个编辑器都要配置，而 poetry 同时做到了，并且让所有端的体验都相同 前情提要：阅读本文之前你至少需要知道什么是虚拟环境，笔者使用操作系统为 Debian，编辑器为 Neovim ","date":"2023-05-20","objectID":"/posts/poetry2/:0:0","tags":["python"],"title":"poetry - python 的包管理工具","uri":"/posts/poetry2/"},{"categories":["python"],"content":"安装 pip3 install poetry ","date":"2023-05-20","objectID":"/posts/poetry2/:1:0","tags":["python"],"title":"poetry - python 的包管理工具","uri":"/posts/poetry2/"},{"categories":["python"],"content":"设置源 方式一 使用 poetry-plugin-pypi-mirror 插件实现全局替换 安装插件 poetry self add poetry-plugin-pypi-mirror 使用 poetry 安装插件速度可能会很慢，可以使用 pip 进行安装 pip3 install poetry-plugin-pypi-mirror 配置 修改poetry配置文件 $HOME/.config/pypoetry/config.toml [plugins] [plugins.pypi_mirror] url = \"https://pypi.tuna.tsinghua.edu.cn/simple/\" 方式二 项目创建后在项目配置文件中配置 [[tool.poetry.source]] name = \"tsinghua\" default = true url = \"https://pypi.tuna.tsinghua.edu.cn/simple/\" ","date":"2023-05-20","objectID":"/posts/poetry2/:2:0","tags":["python"],"title":"poetry - python 的包管理工具","uri":"/posts/poetry2/"},{"categories":["python"],"content":"初始化 新建项目 poetry new project-name 已有项目初始化 poetry init ","date":"2023-05-20","objectID":"/posts/poetry2/:3:0","tags":["python"],"title":"poetry - python 的包管理工具","uri":"/posts/poetry2/"},{"categories":["python"],"content":"配置 mypy 插件(如果需要) mypy 插件的作用是帮助 python 做静态类型检查 sqlalchemy-stubs sqlalchemy-stubs -\u003e sqlalchemy 安装 poerty add --group=dev sqlalchemy-stubs [tool.mypy] plugins = [\"sqlmypy\"] pydantic.mypy pydantic.mypy -\u003e pydantic [tool.mypy] plugins = [\"pydantic.mypy\"] ","date":"2023-05-20","objectID":"/posts/poetry2/:4:0","tags":["python"],"title":"poetry - python 的包管理工具","uri":"/posts/poetry2/"},{"categories":["python"],"content":"配置格式化工具 编辑 pyproject.toml [tool.black] line-length = 100 target-version = ['py38', 'py39'] [tool.isort] profile = \"black\" include_trailing_comma = true multi_line_output = 3 skip = [\".venv\", \".git\"] 安装isort和black为项目开发依赖 poetry add --group=dev isort black ","date":"2023-05-20","objectID":"/posts/poetry2/:5:0","tags":["python"],"title":"poetry - python 的包管理工具","uri":"/posts/poetry2/"},{"categories":["python"],"content":"安装 taskipy 辅助工具 taskipy 能够帮助使用poetry的开发者简化命令 poetry add --group=dev taskipy 当我们需要使用测试框架或者运行代码有复杂的启动参数的时候，taskipy可以帮助我们简化这个过程，以 isort 和 black 举例 [tool.taskipy.tasks] isort = \"isort .\" black = \"black .\" 这里参数少是因为在上面已经配置过了，所以这里需要写的参数很少 查看已有 task poetry run task --list 运行 task poetry run task black ","date":"2023-05-20","objectID":"/posts/poetry2/:6:0","tags":["python"],"title":"poetry - python 的包管理工具","uri":"/posts/poetry2/"},{"categories":["python"],"content":"安装第三方包 poetry add package-name poetry 中如果不同环境有不同的依赖可以使用 group 进行区分，比如开发环境、文件构建环境、测试环境、生成环境等 通过指定 group 参数对依赖进行分组 poetry add --group=dev isort 当对一个已有的 poetry 项目安装依赖的时候可以指定只安装的分组 poetry install --only dev 因为场景过于丰富，超过了笔者使用范畴，更多使用方法查看官方文档 https://python-poetry.org/docs/managing-dependencies/#optional-groups ","date":"2023-05-20","objectID":"/posts/poetry2/:7:0","tags":["python"],"title":"poetry - python 的包管理工具","uri":"/posts/poetry2/"},{"categories":["python"],"content":"一些可能有用的配置 $HOME/.config/pypoetry/config.toml [virtualenvs] in-project = true # 将虚拟环境创建在项目中 ","date":"2023-05-20","objectID":"/posts/poetry2/:8:0","tags":["python"],"title":"poetry - python 的包管理工具","uri":"/posts/poetry2/"},{"categories":["python"],"content":"结语 使用集成环境固然方便，但是当面临昂贵的许可证的时候我选择了开源的 Neovim ，因为vscode在 python 上的体验并不好，或者说是在动态类型语言上的体验都不好。还有一个原因，当项目上线的时候如果不知道项目怎么做环境隔离那么这个项目是非常危险的，除了写代码还应该掌握更多周边的技能 ","date":"2023-05-20","objectID":"/posts/poetry2/:9:0","tags":["python"],"title":"poetry - python 的包管理工具","uri":"/posts/poetry2/"},{"categories":["Go"],"content":"Context是go中专门用来处理多个goroutine之间上下文管理的问题，在go1.7的时候被加入go的标准库中 go官方为什么要将Context加入标准库呢？ 在goroutine中我们想让一个goroutine退出，可以有多种方法，除了使用channel还可以使用全局变量的方式 wg = sync.WithGroup() var exit bool func worker() { for !exit { fmt.Println(\"worker\") time.Sleep(time.Second) } wg.Done() } func main() { wg.Add(1) go worker() time.Sleep(time.Second * 3) // sleep3秒以免程序过快退出 exit = true // 修改全局变量实现子goroutine的退出 wg.Wait() fmt.Println(\"over\") } 程序的开头声明了一个全局的bool类型的exit，因为开始的时候没有进行赋值，所以exit为bool默认值false 但是全局变量的方式并不容易维护，不管是在跨包时不容易维护，当我们的goroutine中再启动一个子goroutine时将会难以控制 我们想让一个goroutine退出的方式多种多样，可能一个程序员就有一种自己创造的方法，那么当大家在协同工作的时候程序就会十分的混乱，为了约束这种行为，go官方就推出了Context标准库，那么标准就得到了统一 context.Context 接口定义了四个需要实现的方法，其中包括： Deadline — 返回 context.Context 被取消的时间，也就是完成工作的截止日期 Done — 返回一个 Channel，这个 Channel 会在当前工作完成或者上下文被取消后关闭，多次调用 Done 方法会返回同一个 Channel Err — 返回 context.Context 结束的原因，它只会在 Done 方法对应的 Channel 关闭时返回非空的值 如果 context.Context 被取消，会返回 Canceled 错误 如果 context.Context 超时，会返回 DeadlineExceeded 错误 Value — 从 context.Context 中获取键对应的值，对于同一个上下文来说，多次调用 Value 并传入相同的 Key 会返回相同的结果，该方法可以用来传递请求特定的数据 type Context interface { Deadline() (deadline time.Time, ok bool) Done() \u003c-chan struct{} Err() error Value(key interface{}) interface{} } context 包中提供的 context.Background、context.TODO、context.WithDeadline 和 context.WithValue 函数会返回实现该接口的私有结构体 ","date":"2023-04-11","objectID":"/posts/go-context/:0:0","tags":["Go"],"title":"go 上下文context","uri":"/posts/go-context/"},{"categories":["Go"],"content":"Context 的作用 在 Goroutine 构成的树形结构中对信号进行同步以减少计算资源的浪费是 context.Context 的最大作用。Go 服务的每一个请求都是通过单独的 Goroutine 处理的，HTTP/RPC 请求的处理器会启动新的 Goroutine 访问数据库和其他服务。 在 Go 的 http 源码中可看到，每当一个新的链接进来的时候会启动一个新的 Goroutine 去处理 func (srv *Server) Serve(l net.Listener) error { ... baseCtx := context.Background() ... ctx := context.WithValue(baseCtx, ServerContextKey, srv) for { rw, err := l.Accept() connCtx := ctx if cc := srv.ConnContext; cc != nil { connCtx = cc(connCtx, rw) if connCtx == nil { panic(\"ConnContext returned nil\") } } c := srv.newConn(rw) c.setState(c.rwc, StateNew, runHooks) // before Serve can return go c.serve(connCtx) } } 如下图所示，我们可能会创建多个 Goroutine 来处理一次请求，而 context.Context 的作用是在不同 Goroutine 之间同步请求特定数据、取消信号以及处理请求的截止日期。 下图中 Goroutine1 可能因为一些原因停止了，Goroutine1 中创建的 Goroutine1.1 可能因为 Goroutine1 停止而不需要继续执行，但是如果 Goroutine1 没有收到信号，那么它将继续执行，但是如果正确的使用 context.Context 就可以在下层及时停掉无用的工作以减少额外资源的消耗： 每一个 context.Context 都会从最顶层的 Goroutine 一层一层传递到最下层。context.Context 可以在上层 Goroutine 执行出现错误时，将信号及时同步给下层。 下面的代码中，创建了一个过期时间为 1s 的 context.Context，并向上下文传入 handle 函数，该方法会使用 500ms 的时间处理传入的请求： func main() { ctx, cancel := context.WithTimeout(context.Background(), 1*time.Second) defer cancel() go handle(ctx, 500*time.Millisecond) select { case \u003c-ctx.Done(): fmt.Println(\"main\", ctx.Err()) } } func handle(ctx context.Context, duration time.Duration) { select { case \u003c-ctx.Done(): fmt.Println(\"handle\", ctx.Err()) case \u003c-time.After(duration): fmt.Println(\"process request with\", duration) } } 因为 context.WithTimeout 传入的过期时间比较长，所以在 handler 中有足够的时间执行对应的逻辑，运行上面的代码之后会输出下面的内容 process request with 500ms main context deadline exceeded 如果将超时时间改为 100*time.Millisecond 会输出下面的内容 main context deadline exceeded handle context deadline exceeded ","date":"2023-04-11","objectID":"/posts/go-context/:1:0","tags":["Go"],"title":"go 上下文context","uri":"/posts/go-context/"},{"categories":["Go"],"content":"默认 Context context 包中最常用的方法还是 context.Background、context.TODO，这两个方法都会返回预先初始化好的私有变量 background 和 todo，它们会在同一个 Go 程序中被复用 var ( background = new(emptyCtx) todo = new(emptyCtx) ) func Background() Context { return background } func TODO() Context { return todo } 这两个私有变量都是通过 new(emptyCtx) 语句初始化的，它们是指向私有结构体 context.emptyCtx 的指针，这是最简单、最常用的上下文类型 type emptyCtx int func (*emptyCtx) Deadline() (deadline time.Time, ok bool) { return } func (*emptyCtx) Done() \u003c-chan struct{} { return nil } func (*emptyCtx) Err() error { return nil } func (*emptyCtx) Value(key interface{}) interface{} { return nil } 从上述代码中，我们不难发现 context.emptyCtx 通过空方法实现了 context.Context 接口中的所有方法，它没有任何功能。 从源代码来看，context.Background 和 context.TODO 也只是互为别名，没有太大的差别，只是在使用和语义上稍有不同： context.Background 是上下文的默认值，所有其他的上下文都应该从它衍生出来； context.TODO 应该仅在不确定应该使用哪种上下文时使用； 在多数情况下，如果当前函数没有上下文作为入参，我们都会使用 context.Background 作为起始的上下文向下传递。 ","date":"2023-04-11","objectID":"/posts/go-context/:2:0","tags":["Go"],"title":"go 上下文context","uri":"/posts/go-context/"},{"categories":["Go"],"content":"context.WithCancel 如果我们创建了多个 Goroutine，我们可以通过调用 cancel 函数让所有 Goroutine 退出 func worker(ctx context.Context, id int) { for { select { case \u003c-ctx.Done(): fmt.Printf(\"worker %d stopped\\n\", id) return default: fmt.Printf(\"worker %d is working\\n\", id) time.Sleep(1 * time.Second) } } } func main() { ctx, cancel := context.WithCancel(context.Background()) for i := 1; i \u003c= 3; i++ { go worker(ctx, i) } time.Sleep(3 * time.Second) cancel() time.Sleep(1 * time.Second) } 运行上面的程序之后会输出如下内容 worker 3 is working worker 1 is working worker 2 is working worker 3 is working worker 2 is working worker 1 is working worker 1 is working worker 3 is working worker 2 is working worker 2 stopped worker 1 stopped worker 3 stopped 从 context.WithCancel 函数的实现来看它到底做了什么： func WithCancel(parent Context) (ctx Context, cancel CancelFunc) { c := newCancelCtx(parent) propagateCancel(parent, \u0026c) return \u0026c, func() { c.cancel(true, Canceled) } } context.newCancelCtx 将传入的上下文包装成私有结构体 context.cancelCtx context.propagateCancel 会构建父子上下文之间的关联，当父上下文被取消时，子上下文也会被取消： func propagateCancel(parent Context, child canceler) { done := parent.Done() if done == nil { return // 父上下文不会触发取消信号 } select { case \u003c-done: child.cancel(false, parent.Err()) // 父上下文已经被取消 return default: } if p, ok := parentCancelCtx(parent); ok { p.mu.Lock() if p.err != nil { child.cancel(false, p.err) } else { p.children[child] = struct{}{} } p.mu.Unlock() } else { go func() { select { case \u003c-parent.Done(): child.cancel(false, parent.Err()) case \u003c-child.Done(): } }() } } 上述函数总共与父上下文相关的三种不同的情况： 当 parent.Done() == nil，也就是 parent 不会触发取消事件时，当前函数会直接返回 当 child 的继承链包含可以取消的上下文时，会判断 parent 是否已经触发了取消信号 如果已经被取消，child 会立刻被取消 如果没有被取消，child 会被加入 parent 的 children 列表中，等待 parent 释放取消信号 context.propagateCancel 的作用是在 parent 和 child 之间同步取消和结束的信号，保证在 parent 被取消时，child 也会收到对应的信号，不会出现状态不一致的情况。 context.cancelCtx 实现的几个接口方法也没有太多值得分析的地方，该结构体最重要的方法是 context.cancelCtx.cancel，该方法会关闭上下文中的 Channel 并向所有的子上下文同步取消信号： func (c *cancelCtx) cancel(removeFromParent bool, err error) { c.mu.Lock() if c.err != nil { c.mu.Unlock() return } c.err = err if c.done == nil { c.done = closedchan } else { close(c.done) } for child := range c.children { child.cancel(false, err) } c.children = nil c.mu.Unlock() if removeFromParent { removeChild(c.Context, c) } } 除了 context.WithCancel 之外，context 包中的另外两个函数 context.WithDeadline 和 context.WithTimeout 也都能创建可以被取消的计时器上下文 context.timerCtx： func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) { return WithDeadline(parent, time.Now().Add(timeout)) } func WithDeadline(parent Context, d time.Time) (Context, CancelFunc) { if cur, ok := parent.Deadline(); ok \u0026\u0026 cur.Before(d) { return WithCancel(parent) } c := \u0026timerCtx{ cancelCtx: newCancelCtx(parent), deadline: d, } propagateCancel(parent, c) dur := time.Until(d) if dur \u003c= 0 { c.cancel(true, DeadlineExceeded) // 已经过了截止日期 return c, func() { c.cancel(false, Canceled) } } c.mu.Lock() defer c.mu.Unlock() if c.err == nil { c.timer = time.AfterFunc(dur, func() { c.cancel(true, DeadlineExceeded) }) } return c, func() { c.cancel(true, Canceled) } } context.WithDeadline 在创建 context.timerCtx 的过程中判断了父上下文的截止日期与当前日期，并通过 time.AfterFunc 创建定时器，当时间超过了截止日期后会调用 context.timerCtx.cancel 同步取消信号。 context.timerCtx 内部不仅通过嵌入 context.cancelCtx 结构体继承了相关的变量和方法，还通过持有的定时器 timer 和截止时间 deadline 实现了定时取消的功能： type timerCtx struct { cancelCtx timer *time.Timer // Under cancelCtx.mu. deadline time.Time } func (c *timerCtx) Deadline() (deadline time.Time, ok bool) { return c.deadline, true } func (c *timerCtx) cancel(removeFromParent bool, err error) { c.cancelCtx.cancel(false, err) if removeFromParent { removeChild(c.cancelCtx.Context, c) } c.mu.Lock() if c.timer != nil { c.timer.Stop() c.timer = nil } c.mu.Unlock() } context.timerCtx.cancel 方法不仅调用了 context.cancelCtx.cancel，还会停止持有的定时器减少不必要的资源浪费。 ","date":"2023-04-11","objectID":"/posts/go-context/:3:0","tags":["Go"],"title":"go 上下文context","uri":"/posts/go-context/"},{"categories":["Go"],"content":"context.WithValue 在最后我们需要了解如何使用上下文传值，context 包中的 context.WithValue 能从父上下文中创建一个子上下文，传值的子上下文使用 context.valueCtx 类型： func WithValue(parent Context, key, val interface{}) Context { if key == nil { panic(\"nil key\") } if !reflectlite.TypeOf(key).Comparable() { panic(\"key is not comparable\") } return \u0026valueCtx{parent, key, val} } context.valueCtx 结构体会将除了 Value 之外的 Err、Deadline 等方法代理到父上下文中，它只会响应 context.valueCtx.Value 方法，该方法的实现也很简单： type valueCtx struct { Context key, val interface{} } func (c *valueCtx) Value(key interface{}) interface{} { if c.key == key { return c.val } return c.Context.Value(key) } 如果 context.valueCtx 中存储的键值对与 context.valueCtx.Value 方法中传入的参数不匹配，就会从父上下文中查找该键对应的值直到某个父上下文中返回 nil 或者查找到对应的值。 ","date":"2023-04-11","objectID":"/posts/go-context/:4:0","tags":["Go"],"title":"go 上下文context","uri":"/posts/go-context/"},{"categories":["CSharp"],"content":"C# 代码是怎么运行的 dotnet的编译工具会将C#源代码转换成IL（Intermediate Language）代码，并将IL代码存储在DLL或EXE文件中（程序集an Assembly） IL代码就像汇编语言指令一样，由dotnet的虚拟机CoreCLR执行 在运行时CLR就从Assembly中加载IL，再由即时编译器（JIT）将IL编程机器码运行 ","date":"2023-01-01","objectID":"/posts/csharp%E9%9A%8F%E7%AC%94/:1:0","tags":["CSharp","DotNetCore"],"title":"C#随笔","uri":"/posts/csharp%E9%9A%8F%E7%AC%94/"},{"categories":["CSharp"],"content":"dotnet常用命令 dotnet -h dotnet --info dotnet --list-sdks dotnet --list-runtimes dotnet --version dotnet help new dotnet new console -n project_name dotnet build dotnet run ","date":"2023-01-01","objectID":"/posts/csharp%E9%9A%8F%E7%AC%94/:1:1","tags":["CSharp","DotNetCore"],"title":"C#随笔","uri":"/posts/csharp%E9%9A%8F%E7%AC%94/"},{"categories":["CSharp"],"content":"类型 ","date":"2023-01-01","objectID":"/posts/csharp%E9%9A%8F%E7%AC%94/:2:0","tags":["CSharp","DotNetCore"],"title":"C#随笔","uri":"/posts/csharp%E9%9A%8F%E7%AC%94/"},{"categories":["CSharp"],"content":"类型推断 // 指定具体类型 string str = \"Hello, World\"; Console.WriteLine(str); int num = 10; Console.WriteLine(num); // 使用var关键词进行类型推断 var bar = \"推断类型\"; Console.WriteLine(bar); // VeryLongClassName _test1 = new VeryLongClassName(); // 类型推断 VeryLongClassName _test2 = new(); class VeryLongClassName { public int Id { get; set; } public string Name { get; set; } } ","date":"2023-01-01","objectID":"/posts/csharp%E9%9A%8F%E7%AC%94/:2:1","tags":["CSharp","DotNetCore"],"title":"C#随笔","uri":"/posts/csharp%E9%9A%8F%E7%AC%94/"},{"categories":["CSharp"],"content":"字符串 格式化字符串 string name = \"张三\"; int age = 18; string formatStr = string.Format(\"姓名: {0}, 年龄: {1}\", name, age); Console.WriteLine(formatStr); 插值字符串 string name = \"张三\"; int age = 18; string formatStr = $\"姓名: {name}, 年龄: {age}\"; Console.WriteLine(formatStr); ","date":"2023-01-01","objectID":"/posts/csharp%E9%9A%8F%E7%AC%94/:2:2","tags":["CSharp","DotNetCore"],"title":"C#随笔","uri":"/posts/csharp%E9%9A%8F%E7%AC%94/"},{"categories":["CSharp"],"content":"多线程 ","date":"2023-01-01","objectID":"/posts/csharp%E9%9A%8F%E7%AC%94/:3:0","tags":["CSharp","DotNetCore"],"title":"C#随笔","uri":"/posts/csharp%E9%9A%8F%E7%AC%94/"},{"categories":["CSharp"],"content":"API namespace System.Threading { public sealed class Thread : CriticalFinalizerObject { public Thread(ParameterizedThreadStart start); public Thread(ThreadStart start); public Thread(ParameterizedThreadStart start, int maxStackSize); public Thread(ThreadStart start, int maxStackSize); ~Thread(); public static IPrincipal? CurrentPrincipal { get; set; } public static Thread CurrentThread { get; } public CultureInfo CurrentCulture { get; set; } public CultureInfo CurrentUICulture { get; set; } public bool IsAlive { get; } [Obsolete(\"The ApartmentState property has been deprecated. Use GetApartmentState, SetApartmentState or TrySetApartmentState instead.\")] public ApartmentState ApartmentState { get; set; } public bool IsBackground { get; set; } public bool IsThreadPoolThread { get; } public int ManagedThreadId { get; } public string? Name { get; set; } public ExecutionContext? ExecutionContext { get; } public ThreadPriority Priority { get; set; } public ThreadState ThreadState { get; } public static LocalDataStoreSlot AllocateDataSlot(); public static LocalDataStoreSlot AllocateNamedDataSlot(string name); public static void BeginCriticalRegion(); public static void BeginThreadAffinity(); public static void EndCriticalRegion(); public static void EndThreadAffinity(); public static void FreeNamedDataSlot(string name); public static int GetCurrentProcessorId(); public static object? GetData(LocalDataStoreSlot slot); public static AppDomain GetDomain(); public static int GetDomainID(); public static LocalDataStoreSlot GetNamedDataSlot(string name); public static void MemoryBarrier(); [Obsolete(\"Thread.ResetAbort is not supported and throws PlatformNotSupportedException.\", DiagnosticId = \"SYSLIB0006\", UrlFormat = \"https://aka.ms/dotnet-warnings/{0}\")] public static void ResetAbort(); public static void SetData(LocalDataStoreSlot slot, object? data); public static void Sleep(int millisecondsTimeout); public static void Sleep(TimeSpan timeout); public static void SpinWait(int iterations); [return: NotNullIfNotNull(\"address\")] public static object? VolatileRead([NotNullIfNotNull(\"address\")] ref object? address); public static IntPtr VolatileRead(ref IntPtr address); public static float VolatileRead(ref float address); [CLSCompliant(false)] public static UIntPtr VolatileRead(ref UIntPtr address); [CLSCompliant(false)] public static uint VolatileRead(ref uint address); [CLSCompliant(false)] public static ulong VolatileRead(ref ulong address); public static long VolatileRead(ref long address); [CLSCompliant(false)] public static ushort VolatileRead(ref ushort address); public static int VolatileRead(ref int address); [CLSCompliant(false)] public static sbyte VolatileRead(ref sbyte address); public static double VolatileRead(ref double address); public static byte VolatileRead(ref byte address); public static short VolatileRead(ref short address); [CLSCompliant(false)] public static void VolatileWrite(ref UIntPtr address, UIntPtr value); [CLSCompliant(false)] public static void VolatileWrite(ref ulong address, ulong value); [CLSCompliant(false)] public static void VolatileWrite(ref uint address, uint value); [CLSCompliant(false)] public static void VolatileWrite(ref ushort address, ushort value); [CLSCompliant(false)] public static void VolatileWrite(ref sbyte address, sbyte value); public static void VolatileWrite([NotNullIfNotNull(\"value\")] ref object? address, object? value); public static void VolatileWrite(ref IntPtr address, IntPtr value); public static void VolatileWrite(ref long address, long value); public static void VolatileWrite(ref int address, int value); public static void VolatileWrite(ref float address, float value); public static void VolatileWrite(ref double address, double value); public static void VolatileWrite(ref byte address, byte value); public static void VolatileWrite(ref short address, short value); public static bool Yield(); [Obsolete(\"Thread.Abort is not supported and throws PlatformNotSupportedException.\", Di","date":"2023-01-01","objectID":"/posts/csharp%E9%9A%8F%E7%AC%94/:3:1","tags":["CSharp","DotNetCore"],"title":"C#随笔","uri":"/posts/csharp%E9%9A%8F%E7%AC%94/"},{"categories":["CSharp"],"content":"异步编程模型 异步编程模型（Asynchronous Programming Model），提出于.Net Framework 1.x时代，基于IAsyncResult接口实现类似BeginXXX和EndXXX的方法。 APM是建立在委托之上的，.Net Core中委托不支持异步调用，也就是BeginInvoke和EndInvoke方法。 EndInvoke会做三件事: 如果异步委托还没有结束，它会等待异步委托执行完成。 它会接收返回值，也包括ref和out方式的参数。 它会向调用线程抛出未处理的异常。 ","date":"2023-01-01","objectID":"/posts/csharp%E9%9A%8F%E7%AC%94/:4:0","tags":["CSharp","DotNetCore"],"title":"C#随笔","uri":"/posts/csharp%E9%9A%8F%E7%AC%94/"},{"categories":["CSharp"],"content":"基于事件的异步模式 基于事件的异步模式（Event-Based Asynchronous Pattern），EAP是在 .Net Framework2.0中提出的，让类可以提供多线程的能力，而不需要使用者显示启动和管理线程，这种模式具有以下能力： 协作取消模型（cooperative cancellation model） 线程亲和性（thread affinity） 将异常转发到完成事件（forwarding exception） ","date":"2023-01-01","objectID":"/posts/csharp%E9%9A%8F%E7%AC%94/:4:1","tags":["CSharp","DotNetCore"],"title":"C#随笔","uri":"/posts/csharp%E9%9A%8F%E7%AC%94/"},{"categories":["CSharp"],"content":"基于任务的异步模式（TAP） 从 .Net Framework4开始引入，现在普遍使用的异步编程模式是TAP模式，也就是C# 提供的 async 和 await 关键词 ","date":"2023-01-01","objectID":"/posts/csharp%E9%9A%8F%E7%AC%94/:4:2","tags":["CSharp","DotNetCore"],"title":"C#随笔","uri":"/posts/csharp%E9%9A%8F%E7%AC%94/"},{"categories":["CSharp"],"content":"异步编程 使用 async await 关键字可以很轻松的实现异步编程，只需要将方法加上 async 关键字，方法内的异步操作使用 await 等待异步操作完成后再执行后续操作。 class Program { static void Main(string[] args) { Method1(); Method2(); Console.ReadKey(); } public static void Method1() { Task.Run( async () =\u003e { for (int i = 0; i \u003c 50; i++) { Console.WriteLine(\" Method 1\"); // Do something await Task.Delay(100); } }); } public static void Method2() { for (int i = 0; i \u003c 50; i++) { Console.WriteLine(\" Method 2\"); // Do something Task.Delay(100).Wait(); } } } 上面程序运行后会交替输出 Method 1 和 Method 2，Method1 中使用了 Task.Run 启动了一个异步后台任务，然后就返回了。创建的这个异步任务会在后台运行，至于会不会被放到子线程中主要看调度程序如何执行。 至于任务如何切换任务和 rust 中类似，都是通过不停的轮训判断任务状态。遇到可以切换的任务的位置一般都是在 I/O 等待中，或者说是阻塞。 C# 中的异步任务同样可以在不同的线程上运行，也就是 await 后的代码可能会在不同的上下文中执行。 ","date":"2023-01-01","objectID":"/posts/csharp%E9%9A%8F%E7%AC%94/:5:0","tags":["CSharp","DotNetCore"],"title":"C#随笔","uri":"/posts/csharp%E9%9A%8F%E7%AC%94/"},{"categories":["rust"],"content":"Rust 中有许多包装类型用于访问数据，每种都有独特性。 ","date":"2022-11-12","objectID":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/:0:0","tags":["rust"],"title":"Rust 包装类型","uri":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/"},{"categories":["rust"],"content":"基础指针类型 ","date":"2022-11-12","objectID":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/:1:0","tags":["rust"],"title":"Rust 包装类型","uri":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/"},{"categories":["rust"],"content":"Box Box 是一种owned或box。虽然它可以提供对数据的借用引用，但它是数据的唯一所有者。特别是，当发生如下情况时： let x = Box::new(1); let y = x; // x 在这里不再可访问 在这里，box 被移动到 y 中。由于 x 不再拥有它，编译器将不再允许开发者在此之后使用 x。一个 box 也可以通过返回值从函数中移动出来，当一个box（一个尚未移动的box）超出作用域时，deref 函数会运行，释放内部数据。 这种抽象是一种低成本的动态分配抽象。如果想在堆上分配一些内存并安全地传递一个指向该内存的指针，这是理想的选择。但是要注意，按照常规借用规则，只允许在编译时的检查的情况下共享对数据借用引用。 ","date":"2022-11-12","objectID":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/:1:1","tags":["rust"],"title":"Rust 包装类型","uri":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/"},{"categories":["rust"],"content":"Copy Move/ownership 语义并不是 Box 独有的；它是所有非 Copy 类型的特性。 一个 Copy 类型是其逻辑上包含的所有数据（通常是拥有的）的一个副本。这些数据通常是它的栈表示的一部分。大多数包含指向其他数据的指针的类型不是 Copy 类型，因为有额外的数据在其他地方，只复制栈表示可能会不安全地共享这些数据的所有权。 像 Vec 和 String 这样也有堆上数据的类型也不是 Copy类型。像 integer/bool 这样的类型是 Copy 类型。 \u0026T 和裸指针是 Copy。虽然它们确实指向更多的数据，但它们并不“拥有”那些数据。而 Box 可以被看作是“一些恰好是动态分配的数据”，\u0026T 被认为是“对某些数据的借用引用”。虽然两者都是指针，但只有第一个被视为“数据”。因此，第一个的复制应涉及数据的复制（这不是其栈表示的一部分），但第二个的复制只需要复制引用。\u0026mut T 不是 Copy，因为可变引用不能共享，并且 \u0026mut T 在某种程度上“拥有”它指向的数据，因为它可以修改这些数据。 如果一个类型的栈表示的复制不会出现内存安全，那么它可以是 Copy 类型。 ","date":"2022-11-12","objectID":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/:1:2","tags":["rust"],"title":"Rust 包装类型","uri":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/"},{"categories":["rust"],"content":"\u0026T and \u0026mut T \u0026T 和 \u0026mut T 分别是不可变引用和可变引用。它们遵循的“读写锁”模式，即对于某些数据，只能有一个可变引用，或者任意数量的不可变引用，但不能两者都有。这种保证是在编译时强制实施的，运行时没有明显的成本。在大多数情况下，这样的指针足以在代码的不同部分之间共享廉价的引用。 这些指针不能以超出其关联生命周期的方式被复制。 ","date":"2022-11-12","objectID":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/:1:3","tags":["rust"],"title":"Rust 包装类型","uri":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/"},{"categories":["rust"],"content":"*const T and *mut T *const T 和 *mut T是指针（类似于C语言中的原始指针），它们没有与之关联的生命周期或所有权。它们只是指向内存中的某个位置，没有其他限制。这些指针提供的唯一保证是，除非在标记为不安全的代码块中，否则不能对它们进行解引用（即不能直接访问它们指向的值）。 这些指针在构建安全且成本低的抽象（如Vec\u003cT\u003e）时非常有用，但在编写安全代码时应避免直接使用它们。因为直接在安全代码中使用这些原始指针很容易引发内存安全问题，如空指针解引用、悬挂指针、内存泄漏等。 在Rust等语言中，虽然提供了这种原始指针的能力，但鼓励开发者使用更安全的数据结构和操作，如智能指针（如Box\u003cT\u003e、Rc\u003cT\u003e、Arc\u003cT\u003e等）和所有权系统，来自动管理内存和生命周期，从而减少内存错误的风险。 ","date":"2022-11-12","objectID":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/:1:4","tags":["rust"],"title":"Rust 包装类型","uri":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/"},{"categories":["rust"],"content":"Rc Rc\u003cT\u003e 是 Rust 中一个具有运行时成本的智能指针包装器，它实现了引用计数机制。这意味着我们可以有多个“拥有”指向同一数据的指针，并且当所有这些指针都超出作用域时，该数据才会被释放（即deref函数会被调用）。 在内部，Rc\u003cT\u003e 包含一个共享的“引用计数”，每次 Rc 被克隆时，这个计数就会增加；每当一个 Rc 超出作用域时，计数就会减少。Rc\u003cT\u003e 的主要职责是确保对于共享的数据，在其不再被任何 Rc 指针引用时调用其析构函数。 值得注意的是，Rc\u003cT\u003e 内部的数据是不可变的。此外，如果形成了一个引用循环（即多个 Rc 指针相互引用，形成一个闭环），那么这些数据将无法被自动释放，从而导致内存泄漏。对于需要处理引用循环以避免内存泄漏的情况，我们需要使用垃圾收集器（Garbage Collector, GC）。 然而Rust 标准库中并没有直接提供垃圾收集器。但是，Rust 社区中有一些人在努力开发这样的工具。但是 Rust 的设计哲学通常倾向于通过所有权和生命周期规则来避免需要垃圾收集器的场景。 当希望动态分配并在程序的各个部分之间共享一些数据（只读）时，而又不确定哪个部分会是最后使用这些指针的，这时 Rc\u003cT\u003e 就非常有用。它是 \u0026T（即裸指针或不可变引用）的一个可行替代方案，尤其是在 \u0026T 的正确性无法静态检查，或者会导致编写起来极为不便的代码时。开发者可能不愿意花费太多开发成本来处理这些情况。 需要注意的是，Rc\u003cT\u003e 不是线程安全的，Rust 不允许它跨线程发送或共享。这有助于在不需要原子操作时避免原子操作的开销。 与 Rc\u003cT\u003e 相对应的还有一个姐妹类型 Weak\u003cT\u003e。Weak\u003cT\u003e 是一个非拥有（non-owning）且非借用（non-borrowed）的智能指针。它也类似于 \u0026T，但在生命周期上没有限制——Weak\u003cT\u003e 可以永远持有。然而，由于它可能比拥有的 Rc\u003cT\u003e 存活时间更长，因此尝试访问内部数据时可能会失败并返回 None。这在需要循环数据结构或其他类似情况时非常有用。 总的来说，Rc\u003cT\u003e 和 Weak\u003cT\u003e 提供了一种在 Rust 中处理共享数据所有权和生命周期的灵活方式，尽管它们不是线程安全的，并且需要小心处理以避免内存泄漏或无效的内存访问。 开销 在内存使用方面，Rc\u003cT\u003e 的内存分配是单一的，但它与普通的 Box\u003cT\u003e 相比会额外分配两个字段的空间，用于存储“强”引用计数（strong refcount）和“弱”引用计数（weak refcount）。这两个引用计数分别用于跟踪 Rc\u003cT\u003e 和 Weak\u003cT\u003e 类型的智能指针的数量。 Rc\u003cT\u003e 在计算成本上，每次克隆（clone）时都会递增内部引用计数，每次超出作用域时都会递减引用计数。这里需要注意的是，克隆操作并不会对 T 类型的值进行深拷贝（deep copy），而只是简单地递增了引用计数，并返回了一个新的 Rc\u003cT\u003e 副本，这个副本和原始的 Rc\u003cT\u003e 指向同一个 T 类型的值。 这种设计使得 Rc\u003cT\u003e 在处理需要多个所有者（owner）共享同一数据的场景时非常有效，因为它避免了不必要的深拷贝，从而节省了内存和计算资源。然而，正如之前提到的，Rc\u003cT\u003e 不是线程安全的，并且在存在引用循环时可能会导致内存泄漏。因此，在使用时需要特别注意这些问题。 另外，Weak\u003cT\u003e 智能指针提供了一种解决引用循环问题的方法，因为它不拥有数据，只是提供了一种访问数据的方式，并且不会递增强引用计数。但是，由于 Weak\u003cT\u003e 可能在强引用计数变为零之后仍然存活，因此尝试通过 Weak\u003cT\u003e 访问数据时可能会失败。这种机制允许在保持数据可达性的同时，避免内存泄漏。 ","date":"2022-11-12","objectID":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/:1:5","tags":["rust"],"title":"Rust 包装类型","uri":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/"},{"categories":["rust"],"content":"Cell 类型 “Cells” 提供了内部可变性。换句话说，它们包含的数据即使在其类型无法以可变形式获取的情况下（例如，当数据位于 \u0026-ptr 或 Rc 后面时）也可以被操作。 关于这些的说明，cell 模块的文档中有很好的解释。 这些类型通常出现在结构体字段中，但也可能在其他地方找到。 ","date":"2022-11-12","objectID":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/:2:0","tags":["rust"],"title":"Rust 包装类型","uri":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/"},{"categories":["rust"],"content":"Cell Cell\u003cT\u003e 是一种提供零成本内部可变性的类型，但它仅适用于 Copy 类型的值。由于编译器知道被包含的值所拥有的所有数据都在栈上，因此简单地替换数据不会导致任何数据通过引用泄露（或更糟的情况）。 然而，使用此包装器时仍有可能违反您自己的不变性规则。如果一个字段被 Cell 包装，那么表明该数据块是可变的，并且可能在首次读取它和打算使用它之间发生变化。 let x = Cell::new(1); let y = \u0026x; let z = \u0026x; x.set(2); y.set(3); z.set(4); println!(\"{}\", x.get()); 在这里我们能够通过各种不可变引用来修改相同的值。 这与以下的运行时成本相同： let mut x = 1; let y = \u0026mut x; let z = \u0026mut x; x = 2; *y = 3; *z = 4; println!(\"{}\", x; 但是，它有一个额外的好处，那就是实际上能够编译成功。 开销 使用 Cell\u003cT\u003e 在运行时是没有成本的，但是如果用它来包装较大的（Copy）结构体，那么可能值得考虑将结构体的各个字段分别包装在 Cell\u003cT\u003e 中，因为每次写入都会对整个结构体进行完整的复制。 ","date":"2022-11-12","objectID":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/:2:1","tags":["rust"],"title":"Rust 包装类型","uri":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/"},{"categories":["rust"],"content":"RefCell RefCell\u003cT\u003e 同样提供了内部可变性，但它并不局限于 Copy 类型的值。 相反，RefCell\u003cT\u003e 有一个运行时成本。RefCell\u003cT\u003e 在运行时强制实现了类似读写锁（RWLock）的模式（尽管它更像是一个单线程的互斥锁），这与 \u0026T 和 \u0026mut T 在编译时进行的借用检查不同。这是通过 borrow() 和 borrow_mut() 函数来实现的，这两个函数会修改内部的一个引用计数，并返回智能指针，这些智能指针可以分别被不可变和可变地解引用。当这些智能指针超出作用域时，引用计数会被恢复。通过这个系统，我们可以动态地确保在有一个可变借用处于活动状态时，不会有其他任何借用（无论是可变还是不可变）处于活动状态。如果程序员试图进行这样的借用，程序将会发生 panic。 let x = RefCell::new(vec![1,2,3,4]); { println!(\"{:?}\", *x.borrow()) } { let my_ref = x.borrow_mut(); my_ref.push(1); } 类似于 Cell，RefCell 主要用于那些难以满足借用检查器（borrow checker）要求的情况。通常，我们知道这样的突变不会以嵌套的形式发生，但进行检查总是好的。 对于大型、复杂的程序，将某些内容放入 RefCell 以简化事情变得很有用。例如，Rust 编译器内部 ctxt 结构体中的许多映射都位于这个包装器内部。这些映射通常只被修改一次（在创建期间，而不是在初始化之后立即）或在几个分离的地方被修改几次。然而，由于这个结构体被广泛使用，因此处理可变和不可变指针将会非常困难（甚至可能不可能），并且可能会形成一团难以扩展的 \u0026 指针的混乱。另一方面，RefCell 提供了一种廉价（尽管不是零成本）的方式来安全地访问这些映射。在未来，如果有人添加了尝试在已借用时修改单元的代码，它将导致一个（通常是确定的）panic，这个panic可以追踪到导致问题的借用。 类似地，在 Servo 的 DOM 中，我们有很多突变，其中大多数突变是局部的，属于某个 DOM 类型，但也有一些突变会跨越 DOM 并修改各种事物。使用 RefCell 和 Cell 来保护所有突变可以让我们不必到处担心可变性，并且同时突出显示实际发生突变的位置。 请注意，如果可以使用 \u0026 指针以更简单的方式解决问题，则应避免使用 RefCell。 开销 RefCell 本身并不进行内存分配，但它除了存储数据之外，还包含一个额外的“借用状态”指示器（通常是一个字的大小）。在运行时，每次进行借用操作时，都会对这个引用计数进行修改或检查。 ","date":"2022-11-12","objectID":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/:2:2","tags":["rust"],"title":"Rust 包装类型","uri":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/"},{"categories":["rust"],"content":"同步类型 上面提到的许多类型都不能以线程安全的方式使用。特别是 Rc\u003cT\u003e 和 RefCell\u003cT\u003e，它们都使用非原子的引用计数，因此不能这样使用。这使得它们使用起来更便宜，但人们也需要这些类型的线程安全版本。这些版本是存在的，它们分别是 Arc\u003cT\u003e 和 Mutex\u003cT\u003e/RWLock\u003cT\u003e。 请注意，非线程安全的类型不能在线程之间发送，这一点在编译时会被检查。 ","date":"2022-11-12","objectID":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/:3:0","tags":["rust"],"title":"Rust 包装类型","uri":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/"},{"categories":["rust"],"content":"Arc Arc\u003cT\u003e 是 Rc\u003cT\u003e 的一个版本，它使用原子引用计数。Arc\u003cT\u003e 可以在线程之间自由发送。 C++ 的 shared_ptr 与 Arc 类似，但在 C++ 的情况下，内部数据总是可变的。为了获得与 C++ 类似的语义，我们应该使用 Arc\u003cMutex\u003cT\u003e\u003e、Arc\u003cRwLock\u003cT\u003e\u003e 或 Arc\u003cUnsafeCell\u003cT\u003e\u003e（UnsafeCell\u003cT\u003e 是一种可以存储任何数据的单元类型，没有运行时成本，但访问它需要 unsafe 代码块）。最后一个选项（Arc\u003cUnsafeCell\u003cT\u003e\u003e）仅当确定使用它不会导致任何内存不安全时才应使用。请记住，写入结构体不是一个原子操作，而且许多函数（如 vec.push()）可能会在内部重新分配并导致不安全行为。 开销 这增加了使用原子操作来改变引用计数的成本（每当它被克隆或超出作用域时都会发生）。当在单个线程中共享来自 Arc 的数据时，尽可能共享 \u0026 指针是更好的选择。 ","date":"2022-11-12","objectID":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/:3:1","tags":["rust"],"title":"Rust 包装类型","uri":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/"},{"categories":["rust"],"content":"Mutex and RwLock Mutex\u003cT\u003e 和 RwLock\u003cT\u003e 通过 RAII（Resource Acquisition Is Initialization）保护提供互斥。 互斥锁在调用其 lock() 方法之前是不透明的，此时线程将阻塞，直到可以获得锁，然后返回一个保护对象。这个保护对象可以用来访问内部数据（可变的），并且当保护对象超出作用域时，锁将被释放。 { let guard = mutex.lock(); *guard += 1; } // 释放锁 RwLock 的额外好处在于它对多个读取操作是高效的。只要没有写入者，就可以安全地允许多个读取者访问共享数据；而 RwLock 允许读取者获得“读锁”。这样的锁可以并发地获得，并通过引用计数来跟踪。写入者必须获得“写锁”，而写锁只有在所有读取者都超出作用域后才能获得。 开销 这些锁使用类似于内部原子类型的机制来维护锁，而这些锁是相当昂贵的（它们可以阻塞跨处理器的所有内存读取，直到它们完成）。当有很多并发访问发生时，等待这些锁也可能很慢。 ","date":"2022-11-12","objectID":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/:3:2","tags":["rust"],"title":"Rust 包装类型","uri":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/"},{"categories":["rust"],"content":"最后 阅读 Rust 代码时，经常会看到 Rc\u003cRefCell\u003cVec\u003cT\u003e\u003e\u003e 这样的类型以及这些类型的更复杂组合。 通常，这是在将所需的 guard 组合在一起，同时避免为不必要的东西付出代价。 例如，Rc\u003cRefCell\u003cT\u003e\u003e 就是这样一个组合。Rc 本身不能可变地解引用；因为 Rc 提供共享，而共享的可变性是不好的，所以我们把 RefCell 放在里面以获得动态验证的共享可变性。现在我们有了共享的可变数据，但它是以一种方式共享的，即一次只能有一个修改者（且没有读者）或多个读者。 现在，我们可以更进一步，使用 Rc\u003cRefCell\u003cVec\u003cT\u003e\u003e\u003e 或 Rc\u003cVec\u003cRefCell\u003cT\u003e\u003e\u003e。这两者都是可共享、可变的向量，但它们并不相同。 对于前者，RefCell 包裹了整个 Vec，因此整个 Vec 都是可变的。同时，在给定时间内，整个 Vec 只能有一个可变借用。这意味着你的代码不能同时从不同的 Rc 句柄上操作向量的不同元素。但是，我们可以随意地向 Vec 中添加和删除元素。这类似于一个 \u0026mut Vec\u003cT\u003e，但借用检查是在运行时进行的。 对于后者，是对单个元素的借用，但整个向量是不可变的。因此，我们可以独立地借用不同的元素，但我们不能向向量中添加或删除元素。这类似于一个 \u0026mut [T]，但同样，借用检查是在运行时进行的。 在并发程序中，我们有一个类似的情况，即 Arc\u003cMutex\u003cT\u003e\u003e，它提供了共享的可变性和所有权。 当阅读使用这些类型的代码时，要逐步进行，并查看所提供的保证/成本。 在选择组合类型时，我们必须反过来做；确定我们想要的保证，以及在组合的哪个点需要它们。例如，如果在 Vec\u003cRefCell\u003cT\u003e\u003e 和 RefCell\u003cVec\u003cT\u003e\u003e 之间进行选择，我们应该像上面那样权衡利弊，然后选择一个。 如果还是不太确定自己在开发中应该怎么选择它们可以参考下面这幅社区中的大佬给出的图 +-----------+ | Ownership | +--+--------+ +================+ | +-Static-----\u003e| T |(1) | | +================+ | | | | +================+ | +-----------+ | Local Val| Cell\u003cT\u003e |(1) +-Unique--\u003e| Borrowing +--+-Dynamic----\u003e|----------------| | +-----------+ | Ref| RefCell\u003cT\u003e |(1) | | +================+ | | | | +================+ | | Threaded | AtomicT |(2) | +-Dynamic----\u003e|----------------| | | Mutex\u003cT\u003e |(1) | | RwLock\u003cT\u003e |(1) | +================+ | | | +================+ | +-No---------\u003e| Rc\u003cT\u003e | | | +================+ | Locally +-----------+ | +-Shared--\u003e| Mutable? +--+ +================+ | +-----------+ | Val| Rc\u003cCell\u003cT\u003e\u003e | | +-Yes--------\u003e|----------------| | Ref| Rc\u003cRefCell\u003cT\u003e\u003e | | +================+ | | | +================+ | +-No---------\u003e| Arc\u003cT\u003e | | | +================+ | Shared +-----------+ | +-Between-\u003e| Mutable? +--+ +================+ Threads +-----------+ | | Arc\u003cAtomicT\u003e |(2) +-Yes--------\u003e|----------------| | Arc\u003cMutex\u003cT\u003e\u003e | | Arc\u003cRwLock\u003cT\u003e\u003e | +================+ ","date":"2022-11-12","objectID":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/:4:0","tags":["rust"],"title":"Rust 包装类型","uri":"/posts/rust%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/"},{"categories":["tool"],"content":"文章已迁移到 Notion https://nervous-petunia-ec3.notion.site/3fbc0343cb724c25a6e05d487f697ab2?v=1af10d27f4644002bb8f5fce2d75c47a\u0026pvs=4 ","date":"2022-09-04","objectID":"/posts/%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7/:0:0","tags":["tool"],"title":"开源工具","uri":"/posts/%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7/"},{"categories":["tool"],"content":"yapi是什么 YApi 是高效、易用、功能强大的 api 管理平台，旨在为开发、产品、测试人员提供更优雅的接口管理服务。可以帮助开发者轻松创建、发布、维护 API，YApi 还为用户提供了优秀的交互体验，开发人员只需利用平台提供的接口数据写入工具以及简单的点击操作就可以实现接口的管理。 ","date":"2022-08-13","objectID":"/posts/yapi/:1:0","tags":["tool"],"title":"YApi","uri":"/posts/yapi/"},{"categories":["tool"],"content":"前后端分离演变过程 ","date":"2022-08-13","objectID":"/posts/yapi/:2:0","tags":["tool"],"title":"YApi","uri":"/posts/yapi/"},{"categories":["tool"],"content":"前后端不分离 在互联网发展的早期，前后端开发时一体的，前端代码时后端代码的一部分，后端收到浏览器的请求后生成静态页面发送到浏览器。那时候的网站开发采用的是后端MVC模式即 Model: 提供/保存数据 View: 展示数据，提供用户界面 Controller: 数据处理，实现业务逻辑 前端负责项目的HTML静态文件或HTML模板文件，模板文件按照约定写好之后交给后端，由后端服务进行渲染。这种开发模式是非常麻烦的，如果HTML静态文件更改了后端需要知道前端改了哪些地方才能去更改模板文件 ","date":"2022-08-13","objectID":"/posts/yapi/:2:1","tags":["tool"],"title":"YApi","uri":"/posts/yapi/"},{"categories":["tool"],"content":"前后端分离 到了2000年以后，前端迎来了Ajax技术，从此前端不再是后端的模板，可以得到各种数据，Ajax技术促进了Web2.0的诞生，从那时起前端变得复杂起来，对前端工程师的要求越来越高。 前端的HTML/CSS/JS 完全由前端人员进行管理，交互就通过api接口，浏览器直接访问前端服务器，前端服务器返回静态资源后再由浏览器解析后向后端发起数据资源请求，数据映射到浏览器的过程由前端负责 这时候又出现了问题，前后端进行交互时发送和返回的数据的解释，所以需要文档进行指明请求的URL路径以及发送和返回的数据的含义，在这个过程种文档变得十分重要 ","date":"2022-08-13","objectID":"/posts/yapi/:2:2","tags":["tool"],"title":"YApi","uri":"/posts/yapi/"},{"categories":["tool"],"content":"文档系统之痛 前后端分离之后，后端出现了各种文档系统框架如swagger，框架通过扫描分析代码生成一个包含URL路径以及请求和返回的数据的文档系统，并且后端人员可以通过文档系统测试后端接口。 swagger这类文档系统只能提供HTTP请求测试后端返回数据，并不能接受指定HTTP请求并返回指定格式的数据，因为文档系统和后端绑定了，前端想要拿到数据只能通过像后端服务器发起HTTP请求。 如果后端没有完成指定接口并上线，那么前端开发就不能进行测试，于是前端开发又衍生出了mock系统，前端根据后端接口文档定义好数据格式就能生成整个测试数据，但是前端只能本地mock或者部署一套独立的mock系统，mock系统和接口文档系统完全是分离的，如果后端更改或增加了某个接口，文档系统和mock系统就需要重新部署。 ","date":"2022-08-13","objectID":"/posts/yapi/:3:0","tags":["tool"],"title":"YApi","uri":"/posts/yapi/"},{"categories":["tool"],"content":"YApi YApi只需要定义接口规则就能mock出数据给前端开发人员使用，并且为后端提供接口测试的功能，YApi让前后端开发变得很独立了，只需要将接口定义好，就能生成出一个mock地址，并提供向后端发起请求验证返回的数据格式的功能，前端开发在本地开发时只需要配置请求代理就能拿到正确格式的数据，后端开发根据文档完成指定接口之后就能验证接口是否满足文档需求。整个开发过程就不会出现有一方存在开发信息滞后的问题，并且如果需要更改接口前后端立马能够感知到，并且按照文档进行相应的修改。 ","date":"2022-08-13","objectID":"/posts/yapi/:4:0","tags":["tool"],"title":"YApi","uri":"/posts/yapi/"},{"categories":["tool"],"content":"部署 YApi部署有很多问题，推荐使用第三方的Docker镜像直接部署 https://github.com/fjc0k/docker-YApi ","date":"2022-08-13","objectID":"/posts/yapi/:4:1","tags":["tool"],"title":"YApi","uri":"/posts/yapi/"},{"categories":["tool"],"content":"YApi的漏洞 在使用高级Mock的时候会存在安全漏洞 { \"errcode\": 400, \"errmsg\": \"解析出错，请检查。Error: EROFS: read-only file system, mkdir '/sys/fs/cgroup/cpu/safeify'\", \"data\": null } 在官方仓库的issues中#2536，有提到这个高危漏洞 已经有人通过入更安全的 Safeify 沙箱处理这个问题：YAPI安全漏洞问题，一文全解决 ","date":"2022-08-13","objectID":"/posts/yapi/:5:0","tags":["tool"],"title":"YApi","uri":"/posts/yapi/"},{"categories":["web"],"content":"Fiber是React核心算法的重新实现，它改变了React的渲染机制，从递归渲染转变为基于任务的渲染。这种改变使得React能够更好地适应现代Web应用的需求，如更复杂的组件结构、更高的性能要求等。 JavaScript 引擎和页面渲染引擎两个线程是互斥的，当其中一个线程执行时，另一个线程只能挂起等待 如果 JavaScript 线程长时间地占用了主线程，那么渲染层面的更新就不得不长时间地等待，界面长时间不更新，会导致页面响应度变差，用户可能会感觉到卡顿 在 React 16 以前，React使用Stack Reconciler（堆栈协调器）来渲染组件，这个过程是同步且不可中断的。如果组件树的层级很深，或者组件的渲染过程复杂，递归会占用线程很长时间，导致UI渲染线程处于等待状态，用户可能会感受到页面卡顿。 ","date":"2022-07-15","objectID":"/posts/react-fiber%E6%9E%B6%E6%9E%84/:0:0","tags":["web"],"title":"React Fiber架构","uri":"/posts/react-fiber%E6%9E%B6%E6%9E%84/"},{"categories":["web"],"content":"Fiber 架构解决的问题 同步阻塞问题： React 的 Fiber 架构通过任务拆分和协作式调度的方式，将渲染过程分解成多个小任务，每个小任务只执行一小部分工作，然后将控制权交回给主线程。这样主线程可以在执行每个小任务之间处理其他任务，如响应用户交互，从而提高页面的响应性。 优化渲染性能： Fiber 架构引入了动态优先级和可中断渲染的特性。每个任务都被分配了一个权重值（优先级），React在协调过程中会动态调整任务的执行顺序，优先执行优先级高的任务。当某个任务的执行时间超过了当前帧的剩余时间时，Fiber 会暂停执行该任务，并在下一帧继续执行，从而避免阻塞整个 UI。 ","date":"2022-07-15","objectID":"/posts/react-fiber%E6%9E%B6%E6%9E%84/:1:0","tags":["web"],"title":"React Fiber架构","uri":"/posts/react-fiber%E6%9E%B6%E6%9E%84/"},{"categories":["web"],"content":"Fiber 做了什么 在 React中，主要做了以下的操作： 为每个增加了优先级，优先级高的任务可以中断低优先级的任务。然后再重新，注意是重新执行优先级低的任务 增加了异步任务，调用requestIdleCallback api，浏览器空闲的时候执行 dom diff 树变成了链表，一个 dom 对应两个 fiber（一个链表），对应两个队列，这都是为找到被中断的任务，重新执行 Fiber 是对 React核心算法（即调和过程）的重写 Fiber 是 React 内部所定义的一种数据结构，它是 Fiber 树结构的节点单位，也就是 React 16 新架构下的虚拟DOM 一个 Fiber 就是一个 JavaScript 对象，包含了元素的信息、该元素的更新操作队列、类型 type Fiber = { // 用于标记fiber的WorkTag类型，主要表示当前fiber代表的组件类型如FunctionComponent、ClassComponent等 tag: WorkTag, // ReactElement里面的key key: null | string, // ReactElement.type，调用`createElement`的第一个参数 elementType: any, // The resolved function/class/ associated with this fiber. // 表示当前代表的节点类型 type: any, // 表示当前FiberNode对应的element组件实例 stateNode: any, // 指向他在Fiber节点树中的`parent`，用来在处理完这个节点之后向上返回 return: Fiber | null, // 指向自己的第一个子节点 child: Fiber | null, // 指向自己的兄弟结构，兄弟节点的return指向同一个父节点 sibling: Fiber | null, index: number, ref: null | (((handle: mixed) =\u003e void) \u0026 { _stringRef: ?string }) | RefObject, // 当前处理过程中的组件props对象 pendingProps: any, // 上一次渲染完成之后的props memoizedProps: any, // 该Fiber对应的组件产生的Update会存放在这个队列里面 updateQueue: UpdateQueue\u003cany\u003e | null, // 上一次渲染的时候的state memoizedState: any, // 一个列表，存放这个Fiber依赖的context firstContextDependency: ContextDependency\u003cmixed\u003e | null, mode: TypeOfMode, // Effect // 用来记录Side Effect effectTag: SideEffectTag, // 单链表用来快速查找下一个side effect nextEffect: Fiber | null, // 子树中第一个side effect firstEffect: Fiber | null, // 子树中最后一个side effect lastEffect: Fiber | null, // 代表任务在未来的哪个时间点应该被完成，之后版本改名为 lanes expirationTime: ExpirationTime, // 快速确定子树中是否有不在等待的变化 childExpirationTime: ExpirationTime, // fiber的版本池，即记录fiber更新过程，便于恢复 alternate: Fiber | null, } Fiber 把渲染更新过程拆分成多个子任务，每次只做一小部分，做完看是否还有剩余时间，如果有继续下一个任务；如果没有，挂起当前任务，将时间控制权交给主线程，等主线程不忙的时候在继续执行 即可以中断与恢复，恢复后也可以复用之前的中间状态，并给不同的任务赋予不同的优先级，其中每个任务更新单元为 React Element 对应的 Fiber 节点 Fiber架构通过 requestIdleCallback 等 API 实现了异步渲染，这使得 React 能够在浏览器空闲时执行渲染任务，从而减少对用户交互的干扰。而不会影响延迟关键事件，如动画和输入响应 首先 React 中任务切割为多个步骤，分批完成。在完成一部分任务之后，将控制权交回给浏览器，让浏览器有时间再进行页面的渲染。等浏览器忙完之后有剩余时间，再继续之前 React 未完成的任务，是一种合作式调度。 该实现过程是基于 Fiber 节点实现，作为静态的数据结构来说，每个 Fiber 节点对应一个 React element，保存了该组件的类型（函数组件/类组件/原生组件等等）、对应的 DOM 节点等信息。 作为动态的工作单元来说，每个 Fiber 节点保存了本次更新中该组件改变的状态、要执行的工作。 每个 Fiber 节点有个对应的 React element，多个 Fiber节点根据如下三个属性构建一颗树： // 指向父级Fiber节点 this.return = null // 指向子Fiber节点 this.child = null // 指向右边第一个兄弟Fiber节点 this.sibling = null ","date":"2022-07-15","objectID":"/posts/react-fiber%E6%9E%B6%E6%9E%84/:2:0","tags":["web"],"title":"React Fiber架构","uri":"/posts/react-fiber%E6%9E%B6%E6%9E%84/"},{"categories":["Go"],"content":"Go 是编译型语言，不依赖虚拟机，编译成二进制的可执行程序即可运行 ","date":"2022-05-22","objectID":"/posts/how-go-works/:0:0","tags":["Go"],"title":"Go语言是如何运行的","uri":"/posts/how-go-works/"},{"categories":["Go"],"content":"可执行文件 ","date":"2022-05-22","objectID":"/posts/how-go-works/:1:0","tags":["Go"],"title":"Go语言是如何运行的","uri":"/posts/how-go-works/"},{"categories":["Go"],"content":"Go 程序的编译过程 在编译目标文件时使用 -x 参数能够看到二进制文件生成的过程 编译阶段就是将文本代码转化为目标文件，目标文件再与标准库中已经编译好的二进制文件进行链接，链接完成之后就会在临时目录将完整的二进制文件输出，然后将编译好的程序复制到目标目录，最后再删除临时目录 package main import \"fmt\" func main() { fmt.Println(\"Hello World\") } ","date":"2022-05-22","objectID":"/posts/how-go-works/:1:1","tags":["Go"],"title":"Go语言是如何运行的","uri":"/posts/how-go-works/"},{"categories":["Go"],"content":"可执行文件规范 不同的操作系统可执行文件的规范是不同的 Linux Windows MacOS ELF PE Mach-o Linux 的可执行文件 ELF(Executable and Linkable Format) 由以下几部分组成 ELF header Section header Sections https://github.com/corkami/pics/blob/master/binary/elf101/elf101.pdf ","date":"2022-05-22","objectID":"/posts/how-go-works/:1:2","tags":["Go"],"title":"Go语言是如何运行的","uri":"/posts/how-go-works/"},{"categories":["Go"],"content":"系统执行可执行文件步骤 ","date":"2022-05-22","objectID":"/posts/how-go-works/:1:3","tags":["Go"],"title":"Go语言是如何运行的","uri":"/posts/how-go-works/"},{"categories":["Go"],"content":"Go 进程的启动与初始化 CPU 无法理解文本，只能执行一条一条的二进制机器码指令，每次执行完一条指令，pc 寄存器就会指向下一条继续执行 go build -gcflags \"-N -l\" main.go go tool objdump -s \"main.\" main Go 语言编译出来的二进制程序由两部分组成，一部分是 Go 语言的 runtime，另一部分是用户代码 runtime 是为了实现额外的功能，而再程序运行时自动加载/运行的模块 Go 语言的 runtime 模块 模块名称 作用 Scheduler 调度器管理所有的 G, M, P 在后台执行调度循环 Netpoll 强罗轮询负责管理网络 FD 相关的读写、就绪事件 Memory 当代码需要内存时，负责内存分配工作 Garbage 当内存不再需要时，负责回收内存 这些模块中最核心的就是 Scheduler, 它负责串联是所有的 runtime 流程 通过 entry point 找到 Go进程的执行入口 m0 是 Go 程序启动后创建的第一个线程 ","date":"2022-05-22","objectID":"/posts/how-go-works/:2:0","tags":["Go"],"title":"Go语言是如何运行的","uri":"/posts/how-go-works/"},{"categories":["Go"],"content":"Go 调度器的由来 摘自刘丹冰的博客: https://learnku.com/articles/41728#a19960 ","date":"2022-05-22","objectID":"/posts/how-go-works/:3:0","tags":["Go"],"title":"Go语言是如何运行的","uri":"/posts/how-go-works/"},{"categories":["Go"],"content":"单进程时代 单进程时代每个程序就是一个进程，直到一个程序运行完，才能进行下一个进程，一切的程序只能串行发生 早期的单进程操作系统面临两个问题： 单一的执行流程，计算机只能一个任务一个任务处理 进程阻塞所带来的 CPU 计算时间浪费 为了解决多个进程来宏观一起执行多个任务，操作系统具有了最早的并发能力：多进程并发，当一个进程阻塞的时候，切换到另外等待执行的进程，这样就能尽量把 CPU 利用起来 ","date":"2022-05-22","objectID":"/posts/how-go-works/:3:1","tags":["Go"],"title":"Go语言是如何运行的","uri":"/posts/how-go-works/"},{"categories":["Go"],"content":"提高 CPU 的利用率 但是对于 Linux 操作系统来讲，cpu 对进程的态度和线程的态度是一样的。 CPU 调度切换的是进程和线程。尽管线程看起来很美好，但实际上多线程开发设计会变得更加复杂，要考虑很多同步竞争等问题，如锁、竞争冲突等 ","date":"2022-05-22","objectID":"/posts/how-go-works/:3:2","tags":["Go"],"title":"Go语言是如何运行的","uri":"/posts/how-go-works/"},{"categories":["Go"],"content":"利用协程来提高 CPU 利用率 多进程、多线程已经提高了系统的并发能力，但是在当今互联网并发场景下，为每个任务都创建一个线程是不现实的，因为会消耗大量的内存，因此又出现了新的问题 高内存占用 调度需要消耗大量 CPU 资源 一个线程分为内核态线程和用户态线程，一个用户态线程必须要绑定一个内核态线程，但是 CPU 并不知道有用户态线程的存在，它只知道运行的是一个内核态线程 再进行细分，内核线程依然叫线程(thread)，用户线程叫协程(co-routine) 既然一个协程(co-routine)可以绑定一个线程(thread)，那么能不能多个协程绑定一个或者多个线程上呢？ N:1 关系 N个协程绑定一个线程，优点就是协程再用户态线程即完成切换，不会陷入到内核态，这种切换非常的轻量快速。但是也有很大的缺点，1个进程的所有协程都绑定在1个线程上。缺点： 某个程序用户量硬件的多核加速能力 一旦某协程阻塞，就会造成线程阻塞，本进程的其他协程都无法执行了，因此就不具备并发的能力了 1:1 关系 1个协程绑定1个线程，最容易实现，协程的调度都由 CPU 完成了，不存在 N:1 的缺点，但是协程的创建、删除和切换的代价都由 CPU 完成，有带你略显昂贵了 M:N 关系 M 个协程绑定1个线程，是 N:1 和 1:1 类型的结合，克服了以上2种模型的缺点，但是实现起来最为复杂 协程和线程是有区别的，线程由 CPU 调度，是抢占式的，协程由用户态调度，是协作式的，一个协程让出 CPU 后，菜执行下一个协程 Go 语言的 goroutine Go 为了提供更容易使用的并发方法，使用了 goroutine 和 channel。goroutine 来自协程的概念，让一组可复用的函数运行在一组线程之上，即使有协程阻塞，改线程的其他协程也可以被 runtime 调度，转移到其他可运行的线程上。最关键的是，程序员看不到这些底层的细节，这就降低了编程的难度，提供了更容易的并发。 goroutine 非常的轻量，一个 goroutine 只占用几 KB 的内存，并且这几 KB 的内存就足够 goroutine 运行完，这就能在有限的内存空间中支持大量的 goroutine，支持了更多的并发。虽然一个 goroutine 的栈只占用几 KB，但实际是可伸缩的，如果需要更多内容，runtime 会自动为 goroutine 分配 ","date":"2022-05-22","objectID":"/posts/how-go-works/:3:3","tags":["Go"],"title":"Go语言是如何运行的","uri":"/posts/how-go-works/"},{"categories":["Go"],"content":"调度组件与调度循环 go func() { fmt.Println(\"Hello World in Goroutine!\") } 每次创建一个 Goroutine 其实是向 runtime 提交了一个计算任务，有任务产生就要有任务消费，因此 Go 的调度流程本质上是一个生产-消费的流程，也就是将一个任务提交到队列中，队列中的任务等待被取出执行 Go 语言在编译时将一个任务包装为一个 g 也就是携程，runtime/runtime2.go 源代码如下(部分代码已省略) type g struct { // Stack parameters. // stack describes the actual stack memory: [stack.lo, stack.hi). // stackguard0 is the stack pointer compared in the Go stack growth prologue. // It is stack.lo+StackGuard normally, but can be StackPreempt to trigger a preemption. // stackguard1 is the stack pointer compared in the C stack growth prologue. // It is stack.lo+StackGuard on g0 and gsignal stacks. // It is ~0 on other goroutine stacks, to trigger a call to morestackc (and crash). stack stack // offset known to runtime/cgo ... m *m // current m; offset known to arm liblink sched gobuf // param is a generic pointer parameter field used to pass // values in particular contexts where other storage for the // parameter would be difficult to find. It is currently used // in three ways: // 1. When a channel operation wakes up a blocked goroutine, it sets param to // point to the sudog of the completed blocking operation. // 2. By gcAssistAlloc1 to signal back to its caller that the goroutine completed // the GC cycle. It is unsafe to do so in any other way, because the goroutine's // stack may have moved in the meantime. // 3. By debugCallWrap to pass parameters to a new goroutine because allocating a // closure in the runtime is forbidden. param unsafe.Pointer atomicstatus uint32 stackLock uint32 // sigprof/scang lock; TODO: fold in to atomicstatus goid int64 waitsince int64 // approx time when the g become blocked lockedm muintptr } Go 最开始采用的是 GM 模型，但由于 GM 模型性能不好，在 1.1 之后进行了改进，也就有了现在的 GMP G: goroutine M: 操作系统线程 P: processor 处理器，处理 M 与 G 的关系 Is the language called Go or Golang?\rThe language is called Go. The “golang” moniker arose because the web site was originally golang.org. (There was no .dev domain then.) Many use the golang name, though, and it is handy as a label. For instance, the Twitter tag for the language is “#golang”. The language’s name is just plain Go, regardless. A side note: Although the official logo has two capital letters, the language name is written Go, not GO. ","date":"2022-05-22","objectID":"/posts/how-go-works/:4:0","tags":["Go"],"title":"Go语言是如何运行的","uri":"/posts/how-go-works/"},{"categories":["Docker"],"content":"在使用 Docker 构建镜像的时候拉取系统依赖或者更新镜系统很慢可以更换源解决 我这里使用的是 slim-buster 类型的镜像，需要更换的是apt源 RUN sed -i \"s@http://deb.debian.org@http://mirrors.aliyun.com@g\" /etc/apt/sources.list 并且 docker 镜像构建的过程中是可以设置代理的 ","date":"2022-05-13","objectID":"/posts/slim-buster-image-change-apt-source/:0:0","tags":["Docker"],"title":"Docker slim-buster Image Change Apt Source","uri":"/posts/slim-buster-image-change-apt-source/"},{"categories":null,"content":"路漫漫其修远兮，吾将上下而求索 下文出现的很多内容来自我的导师罗咏劼的推荐，也有一些是我自认为值得一读的文章或书籍 王垠的博客 怎样写一个解释器-王垠 浅谈国内高校编程语言教育 手把手教你构建 C 语言编译器 清华-rCore-Tutorial-Book-v3 编译原理-国防科技大学 我人生的前28年-王勇 构建和部署机器学习微服务的最简单方法 Web3科学家极简入门指南 Ground-Up Computer Science 王垠的手稿 用Rust实现Lua解释器 高并发的哲学原理 深入高可用架构原理与实践 The Little Book of Rust Macros The Little Book of Rust Macros 中文版 ","date":"2022-05-11","objectID":"/posts/%E9%82%A3%E4%BA%9B%E8%AF%A5%E9%A3%9F%E7%94%A8%E7%9A%84%E4%B8%9C%E8%A5%BF/:0:0","tags":null,"title":"那些美味的东西","uri":"/posts/%E9%82%A3%E4%BA%9B%E8%AF%A5%E9%A3%9F%E7%94%A8%E7%9A%84%E4%B8%9C%E8%A5%BF/"},{"categories":["Linux"],"content":"archlinux update lib error error: libmfx: signature from \"Daniel Bermond \u003cdbermond@archlinux.org\u003e\" is marginal trust :: File /var/cache/pacman/pkg/libmfx-22.1.0-1-x86_64.pkg.tar.zst is corrupted (invalid or corrupted package (PGP signature)). Do you want to delete it? [Y/n] solved pacman -Sy archlinux-keyring \u0026\u0026 pacman -Su ","date":"2022-04-24","objectID":"/posts/archlinux-signature-is-marginal-trust/:0:0","tags":["ArchLinux"],"title":"Archlinux Signature Is Marginal Trust","uri":"/posts/archlinux-signature-is-marginal-trust/"},{"categories":["Go"],"content":"gRPC client 在Dial时默认仅能解析一个TCP地址，并不能通过etcd拿到注册的server的ip地址，无法达到client与server一对多或多对多的场景，尽管可以使用nginx实现，但是当server扩缩容时nginx就显得十分鸡肋。 在grpc文档gRPC Name Resolution中记录了解决方案，虽然文档中说的内容不是很详细，但是在etcd的resolver和v3 client的源代码中解决方案已经很明确了 只需要实现Resolver和Build接口就能通过在客户端Dail的过程完成服务发现以及负载均衡 GitHub上有人实现了grpc resolver consul address的方法https://github.com/simplesurance/grpcconsulresolver 我实现的etcd版本服务注册与发现程序如下 ","date":"2022-04-15","objectID":"/posts/grpc-client-resolver-etcd/:0:0","tags":["gRPC","etcd"],"title":"gRPC client resolve etcd","uri":"/posts/grpc-client-resolver-etcd/"},{"categories":["Go"],"content":"register register.go package etcd import ( \"context\" \"fmt\" \"log\" \"time\" clientv3 \"go.etcd.io/etcd/client/v3\" ) //projectName namespace const projectName = \"projectName\" type ServiceInfo struct { Name string Addr string } type Register struct { client *clientv3.Client sInfo *ServiceInfo leaseID clientv3.LeaseID stop chan struct{} ttl int64 } //NewRegister create register func NewRegister(endpoints []string, sInfo *ServiceInfo) (*Register, error) { client, err := clientv3.New(clientv3.Config{ Endpoints: endpoints, DialTimeout: 5 * time.Second, }) if err != nil { return nil, err } return \u0026Register{ client: client, sInfo: sInfo, ttl: 10, stop: make(chan struct{}, 1), }, nil } //Close register func (r *Register) Close() { r.stop \u003c- struct{}{} } //Key return a key of the service for register to etcd func (r *Register) Key() string { return fmt.Sprintf(\"/%s/%s/%s\", projectName, r.sInfo.Name, r.sInfo.Addr) } //Value return a value of the service address for register to etcd func (r *Register) Value() string { return r.sInfo.Addr } //register service func (r *Register) register() (\u003c-chan *clientv3.LeaseKeepAliveResponse, error) { ctx := context.Background() lease, err := r.client.Grant(ctx, r.ttl) if err != nil { log.Fatalf(\"cannot create lease error: %s\", err) } r.leaseID = lease.ID _, err = r.client.Put(ctx, r.Key(), r.Value(), clientv3.WithLease(lease.ID)) if err != nil { return nil, err } return r.client.KeepAlive(ctx, lease.ID) } // revoke revoke lease func (r *Register) revoke() error { _, err := r.client.Revoke(context.TODO(), r.leaseID) if err != nil { log.Println(err) return err } log.Printf(\"register service:%s stop\\n\", r.Key()) return err } //keepAlive keep the register alive func (r *Register) keepAlive() error { ch, err := r.register() if err != nil { log.Fatalf(\"keep alive faild error: %s\", err) } for { select { case \u003c-r.stop: goto BREAK case \u003c-r.client.Ctx().Done(): return fmt.Errorf(\"register service have been closed\") case _, ok := \u003c-ch: if !ok { log.Println(\"keep alive channel closed\") return r.revoke() } } } BREAK: log.Println(\"stop lease\") return nil } //Run start register func (r *Register) Run() { if r.Key() == \"\" || r.Value() == \"\" { log.Fatalln(\"cannot read service info\") } log.Printf(\"register service {name: %s, address: %s}\\n\", r.sInfo.Name, r.sInfo.Addr) go r.keepAlive() } ","date":"2022-04-15","objectID":"/posts/grpc-client-resolver-etcd/:1:0","tags":["gRPC","etcd"],"title":"gRPC client resolve etcd","uri":"/posts/grpc-client-resolver-etcd/"},{"categories":["Go"],"content":"resolver resolver.go package etcd import ( \"context\" \"sync\" \"go.etcd.io/etcd/api/v3/mvccpb\" clientv3 \"go.etcd.io/etcd/client/v3\" \"google.golang.org/grpc/resolver\" ) type Resolver struct { sync.RWMutex Client *clientv3.Client cc resolver.ClientConn prefix string addresses map[string]resolver.Address } func NewResolver(endpoints []string) (*Resolver, error) { client, err := clientv3.New(clientv3.Config{ Endpoints: endpoints, }) if err != nil { return nil, err } r := \u0026Resolver{ Client: client, } return r, nil } //Build implement the Build interface func (r *Resolver) Build(target resolver.Target, cc resolver.ClientConn, opts resolver.BuildOptions) (resolver.Resolver, error) { r.cc = cc r.prefix = target.URL.Path go r.watcher() r.ResolveNow(resolver.ResolveNowOptions{}) return r, nil } //Scheme address scheme func (r *Resolver) Scheme() string { return \"etcd\" } func (r *Resolver) ResolveNow(resolver.ResolveNowOptions) { } func (r *Resolver) Close() { } //watcher resolver keys from etcd and watch keys func (r *Resolver) watcher() { r.addresses = make(map[string]resolver.Address) response, err := r.Client.Get(context.Background(), r.prefix, clientv3.WithPrefix()) if err == nil { for _, kv := range response.Kvs { r.setAddress(string(kv.Key), string(kv.Value)) } r.cc.UpdateState(resolver.State{ Addresses: r.getAddresses(), }) } watch := r.Client.Watch(context.Background(), r.prefix, clientv3.WithPrefix()) for response := range watch { for _, event := range response.Events { switch event.Type { case mvccpb.PUT: r.setAddress(string(event.Kv.Key), string(event.Kv.Value)) case mvccpb.DELETE: r.delAddress(string(event.Kv.Key)) } } r.cc.UpdateState(resolver.State{ Addresses: r.getAddresses(), }) } } //setAddress set the alive service address to the map func (r *Resolver) setAddress(key, address string) { r.Lock() defer r.Unlock() r.addresses[key] = resolver.Address{Addr: string(address)} } //delAddress delete the dead service address func (r *Resolver) delAddress(key string) { r.Lock() defer r.Unlock() delete(r.addresses, key) } //getAddresses get resolver address from custom resolver func (r *Resolver) getAddresses() []resolver.Address { addresses := make([]resolver.Address, 0, len(r.addresses)) for _, address := range r.addresses { addresses = append(addresses, address) } return addresses } 程序比较简单，可定制性也很强 ","date":"2022-04-15","objectID":"/posts/grpc-client-resolver-etcd/:2:0","tags":["gRPC","etcd"],"title":"gRPC client resolve etcd","uri":"/posts/grpc-client-resolver-etcd/"},{"categories":["Go"],"content":"example server.go sInfo := ServiceInfo{ Name: \"hello\", Addr: \"127.0.0.1:8080\", } r, err := NewRegister([]string{\"1.15.241.96:2379\"}, \u0026sInfo) if err != nil { panic(err) } defer r.Close() r.Run() client.go r, err := NewResolver([]string{\"1.15.241.96:2379\"}) if err != nil { panic(err) } resolver.Register(r) ctx := context.Background() target := fmt.Sprintf(\"etcd:///%s/hello\", projectName) cc, err := grpc.DialContext( ctx, target, grpc.WithDefaultServiceConfig(fmt.Sprintf(`{\"LoadBalancingPolicy\": \"%s\"}`, roundrobin.Name)), grpc.WithTransportCredentials(insecure.NewCredentials()), ) ","date":"2022-04-15","objectID":"/posts/grpc-client-resolver-etcd/:3:0","tags":["gRPC","etcd"],"title":"gRPC client resolve etcd","uri":"/posts/grpc-client-resolver-etcd/"},{"categories":["Docker"],"content":"尽管Docker的操作十分简单，但是仍然不是所有开发者都会，大多数开发者往往只关注于本身所在领域，服务部署这件事情上并不擅长。 ","date":"2022-04-06","objectID":"/posts/go%E6%93%8D%E4%BD%9Cdocker/:0:0","tags":["Docker","GO"],"title":"Go操作Docker","uri":"/posts/go%E6%93%8D%E4%BD%9Cdocker/"},{"categories":["Docker"],"content":"Go为什么能够操作Docker？ Docker使用的是C-S架构，我们开发者在终端操作Docker时，使用的是Docker的客户端。而真正控制Docker镜像、容器的都是Docker Daemon Docker官方提供了Go和Python的SDK，本文将使用Go操作Docker ","date":"2022-04-06","objectID":"/posts/go%E6%93%8D%E4%BD%9Cdocker/:1:0","tags":["Docker","GO"],"title":"Go操作Docker","uri":"/posts/go%E6%93%8D%E4%BD%9Cdocker/"},{"categories":["Docker"],"content":"先决条件 本文使用的Docker环境和Docker Swarm集群中的环境相同 本文将通过VS Code远程连接的方式，连接集群中的manger节点。 环境名称 版本 Go 1.17.6 github.com/docker/docker/client 20.10.14 Docker 20.10.14 目录结构如下 . ├── go.mod ├── go.sum ├── main.go └── Makefile 由于操作Docker需要root权限，所以使用Makefile构建 .PHONY: build build: @go build run: sudo ./\"docker-go\" clean: @go clean 创建Docker结构体 package main type Docker struct { Cli *client.Client } func main() { ctx := context.Background() cli, err := client.NewClientWithOpts(client.FromEnv) if err != nil { log.Fatal(err) } docker := Docker{Cli: cli} } ","date":"2022-04-06","objectID":"/posts/go%E6%93%8D%E4%BD%9Cdocker/:2:0","tags":["Docker","GO"],"title":"Go操作Docker","uri":"/posts/go%E6%93%8D%E4%BD%9Cdocker/"},{"categories":["Docker"],"content":"镜像 ","date":"2022-04-06","objectID":"/posts/go%E6%93%8D%E4%BD%9Cdocker/:3:0","tags":["Docker","GO"],"title":"Go操作Docker","uri":"/posts/go%E6%93%8D%E4%BD%9Cdocker/"},{"categories":["Docker"],"content":"获取镜像列表 func (d *Docker) GetImages(ctx context.Context) { imgOpt := types.ImageListOptions{All: true} imgSums, err := d.Cli.ImageList(ctx, imgOpt) if err != nil { log.Fatal(err) } for _, img := range imgSums { fmt.Printf(\"%+v\\n\", img) } } 运行make build \u0026\u0026 make run 输出如下 {Containers:-1 Created:1649160806 ID:sha256:554eaad3b8fc804b411ff398bcd4b5d59c4891a4770048270697a75a54dc65a3 Labels:map[] ParentID:sha256:5d0ab176d25dca0a7b0ee04685fa79120666d548805ad14dbb0f2fb062b5a7f2 RepoDigests:[] RepoTags:[getip:latest] SharedSize:-1 Size:11761646 VirtualSize:11761646} {Containers:-1 Created:1649160806 ID:sha256:5185163db340a8cdb581aaf2ae4c1e7003980a7709147162f005107f6dc6f441 Labels:map[] ParentID:sha256:0ac33e5f5afa79e084075e8698a22d574816eea8d7b7d480586835657c3e1c8b RepoDigests:[\u003cnone\u003e@\u003cnone\u003e] RepoTags:[\u003cnone\u003e:\u003cnone\u003e] SharedSize:-1 Size:11761646 VirtualSize:11761646} {Containers:-1 Created:1649160806 ID:sha256:5d0ab176d25dca0a7b0ee04685fa79120666d548805ad14dbb0f2fb062b5a7f2 Labels:map[] ParentID:sha256:5185163db340a8cdb581aaf2ae4c1e7003980a7709147162f005107f6dc6f441 RepoDigests:[\u003cnone\u003e@\u003cnone\u003e] RepoTags:[\u003cnone\u003e:\u003cnone\u003e] SharedSize:-1 Size:11761646 VirtualSize:11761646} {Containers:-1 Created:1649117999 ID:sha256:0ac33e5f5afa79e084075e8698a22d574816eea8d7b7d480586835657c3e1c8b Labels:map[] ParentID: RepoDigests:[alpine@sha256:4edbd2beb5f78b1014028f4fbb99f3237d9561100b6881aabbf5acce2c4f9454] RepoTags:[alpine:3.15] SharedSize:-1 Size:5574964 VirtualSize:5574964} {Containers:-1 Created:1648569764 ID:sha256:12766a6745eea133de9fdcd03ff720fa971fdaf21113d4bc72b417c123b15619 Labels:map[maintainer:NGINX Docker Maintainers \u003cdocker-maint@nginx.com\u003e] ParentID: RepoDigests:[nginx@sha256:2275af0f20d71b293916f1958f8497f987b8d8fd8113df54635f2a5915002bf1] RepoTags:[nginx:latest] SharedSize:-1 Size:141522028 VirtualSize:141522028} {Containers:-1 Created:1646356536 ID:sha256:7642119cd16177d874dcbfe1c550affd336dbe4cabb3339ef685ac1d6ec71ccc Labels:map[] ParentID: RepoDigests:[golang@sha256:5616dca835fa90ef13a843824ba58394dad356b7d56198fb7c93cbe76d7d67fe] RepoTags:[golang:1.16-alpine] SharedSize:-1 Size:301868964 VirtualSize:301868964} {Containers:-1 Created:1616097222 ID:sha256:580c0e4e98b06d258754cf28c55f21a6fa0dc386e6fe0bf67e453c3642de9b8b Labels:map[] ParentID: RepoDigests:[portainer/portainer@sha256:fb45b43738646048a0a0cc74fcee2865b69efde857e710126084ee5de9be0f3f] RepoTags:[portainer/portainer:latest] SharedSize:-1 Size:79085285 VirtualSize:79085285} 通过sudo docker images查看镜像信息 REPOSITORY TAG IMAGE ID CREATED SIZE getip latest 554eaad3b8fc 19 hours ago 11.8MB alpine 3.15 0ac33e5f5afa 30 hours ago 5.57MB nginx latest 12766a6745ee 7 days ago 142MB golang 1.16-alpine 7642119cd161 4 weeks ago 302MB portainer/portainer latest 580c0e4e98b0 12 months ago 79.1MB Q1: 为什么得到镜像数量和镜像ID是不匹配的？ A1: 通过docker images命令获取到的镜像信息有限，通过docker images -a –digests –no-trunc就能看到相同信息了 简单模拟docker images改造GetImages方法 type Img struct { Created string ID string Tag string Size string Repository string } const timeLayout = \"2006年01月02日 15:04:05\" func (d *Docker) GetImages(ctx context.Context) { imgOpt := types.ImageListOptions{All: true} imgSums, err := d.Cli.ImageList(ctx, imgOpt) if err != nil { log.Fatal(err) } var imgs []*Img for _, img := range imgSums { tagAndName := strings.Split(img.RepoTags[0], \":\") imgs = append(imgs, \u0026Img{ Created: time.Unix(img.Created, 0).Format(timeLayout), Size: fmt.Sprintf(\"%.2fMB\", float64(img.Size)/float64(1024*1024)), Tag: tagAndName[1], Repository: tagAndName[0], ID: img.ID[7:19], }) } for _, img := range imgs { fmt.Printf(\"%+v\\n\", img) } } 至于filter这里就不做了 ","date":"2022-04-06","objectID":"/posts/go%E6%93%8D%E4%BD%9Cdocker/:3:1","tags":["Docker","GO"],"title":"Go操作Docker","uri":"/posts/go%E6%93%8D%E4%BD%9Cdocker/"},{"categories":["Docker"],"content":"拉取指定镜像 这里使用了\"github.com/docker/docker/pkg/jsonmessage\"这个包，能够在控制台中输出和通过Docker客户端拉取镜像时相同的样式 func (d *Docker) ImagePull(ctx context.Context, name string) { opt := types.ImagePullOptions{} reader, err := d.Cli.ImagePull(ctx, name, opt) if err != nil { log.Fatal(err) } defer reader.Close() termFd, isTerm := term.GetFdInfo(os.Stderr) jsonmessage.DisplayJSONMessagesStream(reader, os.Stderr, termFd, isTerm, nil) } ","date":"2022-04-06","objectID":"/posts/go%E6%93%8D%E4%BD%9Cdocker/:3:2","tags":["Docker","GO"],"title":"Go操作Docker","uri":"/posts/go%E6%93%8D%E4%BD%9Cdocker/"},{"categories":["Docker"],"content":"前言 也许k8s是更好的选择，但是Docker的Swarm方案未必不值得尝试。 ","date":"2022-04-05","objectID":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/:1:0","tags":["Docker"],"title":"Docker Swarm集群","uri":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/"},{"categories":["Docker"],"content":"Docker Swarm Swarm是Docker官方提供的一款集群管理工具，其主要作用是把若干台Docker主机抽象为一个整体，并且通过一个入口统一管理这些Docker主机上的各种Docker资源。Swarm和Kubernetes比较类似，但是更加轻，具有的功能也较kubernetes更少一些。 Swarm集群提供给用户管理集群内所有容器的操作接口与使用一台Docker主机基本相同。 ","date":"2022-04-05","objectID":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/:2:0","tags":["Docker"],"title":"Docker Swarm集群","uri":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/"},{"categories":["Docker"],"content":"先决条件 一下条件满足其中一个即可 至少三台安装了Docker的服务器 至少三台安装了Docker的虚拟机虚拟机 至少三台安装了Docker并且在同一局域网内的真实主机 至少三台安装了Docker并且能够互相访问的主机 本文使用三台虚拟机进行搭建，为了便于描述，本文使用三台虚拟机系统都是Ubuntu20.04代号使用node1, node2, node3 ","date":"2022-04-05","objectID":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/:3:0","tags":["Docker"],"title":"Docker Swarm集群","uri":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/"},{"categories":["Docker"],"content":"搭建Swarm集群 ","date":"2022-04-05","objectID":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/:4:0","tags":["Docker"],"title":"Docker Swarm集群","uri":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/"},{"categories":["Docker"],"content":"初始化manger节点 sudo docker swarm init --advertise-addr 172.26.95.252 在node1命令执行后当前节点会初始化集群，并且输出其他节点加入是需要使用的token 在Dockers官方文档中描述过manager节点可以不止一个，但是本文中由于环境限制仅使用一个manger节点和两个worker节点。 ","date":"2022-04-05","objectID":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/:4:1","tags":["Docker"],"title":"Docker Swarm集群","uri":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/"},{"categories":["Docker"],"content":"node2和node3加入Swarm集群 在node2和node3中执行加入集群命令 sudo docker swarm join --token SWMTKN-1-59365h5vtnl4d7o09pw0unlswmk3u3z-ez6iildqkry77i57my 172.26.95.252:2377 命令执行成功后输出内容如下 This node joined a swarm as a worker. 在node1（manger节点）使用命令查看个节点列表 sudo docker node ls ","date":"2022-04-05","objectID":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/:4:2","tags":["Docker"],"title":"Docker Swarm集群","uri":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/"},{"categories":["Docker"],"content":"搭建私有仓库 由于本文后面章节中的内容会涉及到自建镜像的部署测试，所以需要搭建私有仓库进行部署，在node1中搭建： sudo docker run -d -p 5000:5000 -v /opt/data/registry:/var/lib/registry --restart=always --name registry registry 在node1、2、3中配置/etc/docker/daemon.json，如果没有这个文件直接创建即可 { \"registry-mirror\": [ \"https://hub-mirror.c.163.com\", \"https://mirror.baidubce.com\" ], \"insecure-registries\": [ \"172.26.95.252:5000\" ] } ","date":"2022-04-05","objectID":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/:4:3","tags":["Docker"],"title":"Docker Swarm集群","uri":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/"},{"categories":["Docker"],"content":"部署portainer/portainer 使用portainer来可视化管理Swarm集群，此处使用集群部署的方式安装。 sudo docker service create \\ --name portainer \\ --publish 9000:9000 \\ --constraint 'node.role == manager' \\ --mount type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \\ portainer/portainer \\ -H unix:///var/run/docker.sock 可以看到PORTS是*:9000-\u003e9000/tcp而不是0.0.0.0:9000-\u003e9000/tcp说明可以通过集群中任意一个节点的ip进行访问，笔者已做过测试确实可以。 此处仅展示Swarm节点列表，至于更多功能待读着自己探索了 ","date":"2022-04-05","objectID":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/:5:0","tags":["Docker"],"title":"Docker Swarm集群","uri":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/"},{"categories":["Docker"],"content":"负载均衡与副本 Swarm是自带负载均衡策略的，当我们部署容器时指定replicas参数，那么就会创建多个相同的容器副本并且执行负载均衡策略。 为了测试负载均衡笔者写了一个简单的服务，获取本机ip然后返回。 ","date":"2022-04-05","objectID":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/:6:0","tags":["Docker"],"title":"Docker Swarm集群","uri":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/"},{"categories":["Docker"],"content":"测试程序 go mod init getip package main import ( \"errors\" \"fmt\" \"net\" \"net/http\" ) func getLocalIp() (string, error) { addrs, err := net.InterfaceAddrs() if err != nil { return \"\", err } for _, addr := range addrs { if ipnet, ok := addr.(*net.IPNet); ok \u0026\u0026 !ipnet.IP.IsLoopback() { if ipnet.IP.To4() != nil \u0026\u0026 ipnet.IP.IsGlobalUnicast() { return ipnet.IP.String(), nil } } } return \"\", errors.New(\"valid local IP not found\") } func main() { server := http.Server{ Addr: \":8080\", Handler: nil, } http.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) { ipAddr, err := getLocalIp() if err != nil { w.Write([]byte(\"cannot get service ip\")) } w.Write([]byte(fmt.Sprintf(\"\u003ch1\u003e%s\u003c/h1\u003e\", ipAddr))) }) panic(server.ListenAndServe()) } 然后构建Docker镜像，因为作者虚拟机中有golang-alpine和alpine镜像所以使用多阶段构建，如果读着感觉golang-alpine下载慢可以使用golang的跨平台编译 ","date":"2022-04-05","objectID":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/:6:1","tags":["Docker"],"title":"Docker Swarm集群","uri":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/"},{"categories":["Docker"],"content":"镜像构建 FROM golang:1.16-alpine AS builder COPY . /go/src/getip WORKDIR /go/src/getip RUN go install FROM alpine:3.15 COPY --from=builder /go/bin/getip /go/bin/ EXPOSE 8080 ENTRYPOINT [\"/go/bin/getip\"] 然后构建镜像 sudo docker build -t getip -f ./deployment/Dockerfile . 给镜像打上tag sudo docker tag getip:latest 172.26.95.252:5000/getip:1.0.0 上传镜像到私有仓库 sudo docker push 172.26.95.252:5000/getip:1.0.0 ","date":"2022-04-05","objectID":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/:6:2","tags":["Docker"],"title":"Docker Swarm集群","uri":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/"},{"categories":["Docker"],"content":"部署测试 sudo docker service create --replicas 3 -p 8080:8080 --name getip 172.26.95.252:5000/getip:1.0.0 为了便于测试展示使用bash直接在命令行测试 for i in $(seq 3);do curl 172.26.95.252:8080;echo ;done 从输出结果可以看出确实执行了负载均衡策略，负载均衡策略时最简单的轮询。至于输出结果中的ip为什么不是虚拟机的ip呢？因为程序是在容器中运行的，所以返回的是容器内部的ip。 再看一下服务部署情况，确定是不是真的部署到了不同节点上 sudo docker service ps getip ","date":"2022-04-05","objectID":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/:6:3","tags":["Docker"],"title":"Docker Swarm集群","uri":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/"},{"categories":["Docker"],"content":"出错测试 在生产环境中如果服务出错异常退出了需要手动重启，会比较麻烦，但是Swarm集群的副本可以解决这个问题，Swarm集群会始终维持指定数量的副本，如果有服务出错异常退出Swarm就会立刻创建副本容器。为了验证Swarm集群是真的能维持固定数量的容器副本这里改写前面的程序，让服务过一段时间就异常退出。 package main import ( \"errors\" \"fmt\" \"math/rand\" \"net\" \"net/http\" \"time\" ) func getLocalIp() (string, error) { addrs, err := net.InterfaceAddrs() if err != nil { return \"\", err } for _, addr := range addrs { if ipnet, ok := addr.(*net.IPNet); ok \u0026\u0026 !ipnet.IP.IsLoopback() { if ipnet.IP.To4() != nil \u0026\u0026 ipnet.IP.IsGlobalUnicast() { return ipnet.IP.String(), nil } } } return \"\", errors.New(\"valid local IP not found\") } func panicError() { time.Sleep(time.Duration(rand.Intn(50)+50) * time.Second) panic(errors.New(\"break the service\")) } func main() { server := http.Server{ Addr: \":8080\", Handler: nil, } go panicError() http.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) { ipAddr, err := getLocalIp() if err != nil { w.Write([]byte(\"cannot get service ip\")) } w.Write([]byte(fmt.Sprintf(\"\u003ch1\u003e%s\u003c/h1\u003e\", ipAddr))) }) panic(server.ListenAndServe()) } 删除原有服务 sudo docker service rm getip 然后重新构建部署镜像。 sudo docker build -t getip:1.1.0 -f ./deployment/Dockerfile . sudo docker tag getip:1.1.0 172.26.95.252:5000/getip:1.1.0 sudo docker service create --replicas 3 -p 8080:8080 --name getip 172.26.95.252:5000/getip:1.1.0 仍然使用上一节测试命令，由于负载均衡不用测试了，所以循环次数改为3，每过100秒测试一次，本文一共测试了3次。 for i in $(seq 3);do curl 172.26.95.252:8080;echo ;done 三次的ip全都不同，说明就算程序异常终止退出，Swarm仍然会维持固定的副本数量。 ","date":"2022-04-05","objectID":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/:6:4","tags":["Docker"],"title":"Docker Swarm集群","uri":"/posts/docker-swarm%E9%9B%86%E7%BE%A4/"},{"categories":["环境配置"],"content":"Arch ","date":"2022-03-02","objectID":"/posts/software-tools/:0:0","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"yay 拉取AUR中的软件，项目地址：https://github.com/Jguer/yay ","date":"2022-03-02","objectID":"/posts/software-tools/:0:1","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"neovim sudo pacman -S neovim 更快的vim，学会了vim再也不想使用其他编辑器了，就是调试不方便，补全插件使用coc.nvim，coc项目地址：https://github.com/neoclide/coc.nvim ","date":"2022-03-02","objectID":"/posts/software-tools/:0:2","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"google-chrome yay -S google-chrome ","date":"2022-03-02","objectID":"/posts/software-tools/:0:3","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"visual-studio-code-bin yay -S visual-studio-code-bin 有的东西用neovim不方便的时候用一下，在Linux上轻松半透明 ","date":"2022-03-02","objectID":"/posts/software-tools/:0:4","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"LibreOffice sudo pacman -S liberoffice liberoffice-fresh-zh-cn Linux上的办公软件，功能也是十分强大 ","date":"2022-03-02","objectID":"/posts/software-tools/:0:5","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"lazygit yay -S lazygit 终端下的git工具 ","date":"2022-03-02","objectID":"/posts/software-tools/:0:6","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"zsh sudo pacman -S zsh 配合oh-my-zsh让命令写起来很爽，zsh插件使用的zinit管理 oh-my-zsh项目地址：https://github.com/ohmyzsh/ohmyzsh 拉取GitHub的oh-my-zsh会很慢，可以使用Gitee的oh-my-zsh：https://gitee.com/mirrors/oh-my-zsh，下载tools中的install.sh脚本，修改以下Default settings内容 # Default settings ZSH=${ZSH:-~/.oh-my-zsh} -REPO=${REPO:-ohmyzsh/ohmyzsh} +REPO=${REPO:-mirrors/oh-my-zsh} -REMOTE=${REMOTE:-https://github.com/${REPO}.git} +REMOTE=${REMOTE:-https://gitee.com/${REPO}.git} BRANCH=${BRANCH:-master} 然后授权执行即可。 使用zinit进行插件管理，zinit项目地址：https://github.com/zdharma/zinit ","date":"2022-03-02","objectID":"/posts/software-tools/:0:7","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"rofi sudo pacman -S rofi 配合polybar-themes更好看 polybar-themes项目地址：https://github.com/adi1090x/polybar-themes ","date":"2022-03-02","objectID":"/posts/software-tools/:0:8","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"alsa-utils pacman -S alsa-utils 终端音频控制软件，使用alsamixer命令调出 ","date":"2022-03-02","objectID":"/posts/software-tools/:0:9","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"st 十分小巧但功能强大的终端模拟器，这个终端需要自己拉取源码编译，原始的版本很丑，但是可以打补丁，在GitHub上也有打过补丁的仓库 项目地址：https://st.suckless.org/ ","date":"2022-03-02","objectID":"/posts/software-tools/:0:10","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"Alacritty sudo pacman -S alacritty rust写的用GPU渲染的终端模拟器，st和alacritty我都在使用 ","date":"2022-03-02","objectID":"/posts/software-tools/:0:11","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"ranger sudo pacman -S ranger 终端下的文件管理器，在文件夹间跳转太方便了，爱不释手 ","date":"2022-03-02","objectID":"/posts/software-tools/:0:12","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"wget sudo pacman -S wget 下载文件使用 ","date":"2022-03-02","objectID":"/posts/software-tools/:0:13","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"flameshot sudo pacman -S flameshot 截图软件，需要用命令启动 flameshot gui ","date":"2022-03-02","objectID":"/posts/software-tools/:0:14","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"feh sudo pacman -S feh 设置桌面壁纸的软件 feh --bg-scale 壁纸路径 ","date":"2022-03-02","objectID":"/posts/software-tools/:0:15","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"fcitx5 sudo pacman -S fcitx5-qt fcitx5-gtk fcitx5-configtool ","date":"2022-03-02","objectID":"/posts/software-tools/:0:16","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"neofetch sudo pacman -S neofetch 在终端下使用neofetch命令查看整体系统信息 ","date":"2022-03-02","objectID":"/posts/software-tools/:0:17","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"htop sudo pacman -S htop 终端下的进程管理器，用htop命令调出 ","date":"2022-03-02","objectID":"/posts/software-tools/:0:18","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"fzf sudo pacman -S fzf 终端下的文件查找工具 ","date":"2022-03-02","objectID":"/posts/software-tools/:0:19","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"figlet sudo pacman -S figlet 在终端下输出空心ASCII字符 ","date":"2022-03-02","objectID":"/posts/software-tools/:0:20","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"toilet sudo pacman -S toilet 终端下输出实心ASCII字符，可以附带颜色 ","date":"2022-03-02","objectID":"/posts/software-tools/:0:21","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"pyenv Linux上管理Python版本的工具，项目地址：https://github.com/pyenv/pyenv 通常用它管理Python版本，但是不用它创建虚拟环境 ","date":"2022-03-02","objectID":"/posts/software-tools/:0:22","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"reflector 用来更新pacman镜像源 sudo pacman -S reflector sudo reflector --verbose --country China --latest 15 rate --save /etc/pacman.d/mirrorlist ","date":"2022-03-02","objectID":"/posts/software-tools/:0:23","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"rsync 用于文件传输、复制、有断点续传的功能 sudo pacman -S rsync ","date":"2022-03-02","objectID":"/posts/software-tools/:0:24","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"tmux 终端复用工具 sudo pacman -S tmux ","date":"2022-03-02","objectID":"/posts/software-tools/:0:25","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"gpg 加密工具 sudo pacman -S gnupg ","date":"2022-03-02","objectID":"/posts/software-tools/:0:26","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"xf86-input-libinput xf86-input-libinput配置触控板行为 ","date":"2022-03-02","objectID":"/posts/software-tools/:0:27","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"一些别致的小东西 ","date":"2022-03-02","objectID":"/posts/software-tools/:1:0","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"nyancat ","date":"2022-03-02","objectID":"/posts/software-tools/:1:1","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"cmatrix Windows ","date":"2022-03-02","objectID":"/posts/software-tools/:1:2","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"浏览器 Chrome ","date":"2022-03-02","objectID":"/posts/software-tools/:2:0","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"Coding 编译器和解释器 Python 华为镜像地址：https://repo.huaweicloud.com/python/ 配置环境变量 python -V 设置镜像地址 pip config set global.index-url https://repo.huaweicloud.com/repository/pypi/simple node 配置环境变量 node --version 设置全局模块的存放路径、cache的路径 npm config set prefix \"D:\\soft\\nodejs\\node_global\" npm config set cache \"D:\\soft\\nodejs\\node_cache\" 拉取cnpm npm install -g cnpm --registry=https://registry.npm.taobao.org Go 设置GOROOT、GOPATH 设置GOPROXY https://goproxy.io,direct 查看环境配置 go env Gcc 设置环境变量 gcc -v JDK8 华为开源镜像站：https://repo.huaweicloud.com/openjdk/ 数据库 MySQL MongoDB mycli pip install mycli 编辑器和IDE vscode 同步配置 使用setting sync插件 修改配置文件 typora 打开设置中的markdown扩展语法中的内联公式、数学公式自动添加序号 jetbrains-toolbox-app 设置存储位置和删除旧版本 版本控制 Git ","date":"2022-03-02","objectID":"/posts/software-tools/:3:0","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["环境配置"],"content":"其他工具 ftp filezilla 终端\u0026ssh Windows terminal（微软应用商店下载） mobaxterm 数据库可视化 HeidiSQL compass AnotherRedisDesktopManager 流程图 drawio GIF录制 ScreenToGif 能对最后录制的每一帧画面进行编辑 抓包 Charles wireshark 图床 PicGo 文件查找 everything 速度极快 不可描述软件 clash 字体 nerd font 视频播放器 Potplayer Qucklook Windows应用商店下载，文件快速预览 像csv数据，code，图片都可以预览很方便 PowerToys 类似于mac上的聚焦搜索，PowerToys scoop Windows下的一个包管理器 设置scoop安装位置 管理员权限打开powershell(或者Windows Terminal) $env:SCOOP='D:\\soft\\scoop' 配置包安装的位置 $env:SCOOP_GLOBAL='F:\\GlobalScoopApps' 安装 Invoke-Expression (New-Object System.Net.WebClient).DownloadString('https://get.scoop.sh') chocolatey chocolatey是Windows下的另一个包管理器，和scoop搭配使用 安装 Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1')) 更改安装位置 安装位置暂时不知道如何直接更改，可以安装以后将目录剪切到想要安装的位置 修改环境变量中的值 更改包安装的位置 修改环境变量 oh-my-posh oh-my-posh是一个Windows下的终端美化工具，搭配Windows Terminal使用，毕竟代码不会天天写但是终端天天用 因为安装过程稀里糊涂，所以弄明白后更新 主题可以参照官方文档 ","date":"2022-03-02","objectID":"/posts/software-tools/:4:0","tags":["Github","环境配置"],"title":"software-tools","uri":"/posts/software-tools/"},{"categories":["self"],"content":"喜欢看龙珠、海贼王、火影忍者、高达，最喜欢《高达-闪光的哈撒维》和《哈尔的移动城堡》 喜欢玩 LOL、塞尔达传说 喜欢 Linux 和编程语言 ","date":"2022-03-02","objectID":"/about/:0:0","tags":["self"],"title":"关于我","uri":"/about/"},{"categories":["Github"],"content":"go开源工具合集2 ","date":"2022-02-18","objectID":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%862/:0:0","tags":["Go","Github"],"title":"go开源工具合集2","uri":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%862/"},{"categories":["Github"],"content":"grafana star数：47.1k 开放和可组合的可观察性和数据可视化平台。可视化来自Prometheus、Loki、Elasticsearch、InfluxDB、Postgres等多个来源的指标、日志和跟踪。 grafana能够帮助使用者快速部署出一套可高度自定义的可视化监控平台，相信不管是开发者还是运维人员都需要这样一个漂亮的仪表盘来监控服务的调用量。 ","date":"2022-02-18","objectID":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%862/:1:0","tags":["Go","Github"],"title":"go开源工具合集2","uri":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%862/"},{"categories":["Github"],"content":"filebrowser star数：14.7k filebrowser能够提供指定目录下的web文件管理界面，可用于上传、删除、预览、重命名和编辑文件。它允许创建多个用户，每个用户可以有自己的目录。 如果家中有linux或者树莓派搭配frp或许能够快速打造一个简易的私人网盘 ","date":"2022-02-18","objectID":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%862/:2:0","tags":["Go","Github"],"title":"go开源工具合集2","uri":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%862/"},{"categories":["Github"],"content":"gogs star数：38.5k 如果你想在家搭建一个私人的git仓库gogs一定是最优的选择，一个廉价的树莓派就能满足gogs的运行条件 ","date":"2022-02-18","objectID":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%862/:3:0","tags":["Go","Github"],"title":"go开源工具合集2","uri":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%862/"},{"categories":["Github"],"content":"gin-vue-admin star数：11.5k gin-vue-admin 是一套为快速研发准备的一整套前后端分离架构式的开源框架，旨在快速搭建中小型项目。 基于vite+vue3+gin搭建的开发基础平台，集成jwt鉴权，权限管理，动态路由，分页封装，多点登录拦截，资源权限，上传下载，代码生成器，表单生成器等开发必备功能。 值得一提的是gin-vue-admin的开发者在b站上传了项目教学视频方便开发者快速上手使用。 ","date":"2022-02-18","objectID":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%862/:4:0","tags":["Go","Github"],"title":"go开源工具合集2","uri":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%862/"},{"categories":["Github"],"content":"分享Github上有趣的go开源项目 ","date":"2022-02-17","objectID":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%861/:0:0","tags":["Go","Github"],"title":"go开源工具合集1","uri":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%861/"},{"categories":["Github"],"content":"Hugo star数：57.1k Hugo是由Go语言实现的静态网站生成器，Hexo类似。具有简单、易用、高效、易扩展、快速部署等特点。 Hugo依赖于Markdown文件前端模板，Hugo能够在在一秒内渲染出一个中等大小的网站。 Hugo通常用于构建个人博客网站、文档。 如果想构建个人博客或者长期受Hexo缓慢的速度困扰不妨尝试一下Hugo。 ","date":"2022-02-17","objectID":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%861/:1:0","tags":["Go","Github"],"title":"go开源工具合集1","uri":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%861/"},{"categories":["Github"],"content":"gotop star数：1.3k 基于终端的图形活动监视器，支持按照进程数、CPU占用率、内存占用率排序，可能对于大多数程序员来说比较熟悉的是top或htop，毫无疑问如果让我选，我肯定选择gotop htop gotop ","date":"2022-02-17","objectID":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%861/:2:0","tags":["Go","Github"],"title":"go开源工具合集1","uri":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%861/"},{"categories":["Github"],"content":"frp star数：53.3k frp是使用go语言写的一个专注于内网穿透的高性能的反向代理应用，支持 TCP、UDP、HTTP、HTTPS 等多种协议。可以将内网服务以安全、便捷的方式通过具有公网 IP 节点的中转暴露到公网。 值得一提的是作者开通了知识星球，用于分享frp相关的知识和技术。 ","date":"2022-02-17","objectID":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%861/:3:0","tags":["Go","Github"],"title":"go开源工具合集1","uri":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%861/"},{"categories":["Github"],"content":"Yarning star数：6.2k Yarning是一个SQL审核平台，支持SQL查询、SQL审核、邮件推送、用户权限管理、AutoTask自动执行等功能。 ","date":"2022-02-17","objectID":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%861/:4:0","tags":["Go","Github"],"title":"go开源工具合集1","uri":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%861/"},{"categories":["Github"],"content":"lazygit star数：25.9k lazygit是一个简单的git命令终端界面，它的交互就像vim一样通过键盘完成，但是命令更加简单，它能够帮助开发者快速高效的完成如代码提交、推送、分支合并、版本回退等git相关的命令操作 ","date":"2022-02-17","objectID":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%861/:5:0","tags":["Go","Github"],"title":"go开源工具合集1","uri":"/posts/go%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%861/"},{"categories":["tool"],"content":"前言 笔者使用hexo+nginx这套组合拳大概有两年的时间，随着使用时间的变长以及主题的样式定制化越来越多笔者发现hexo在将markdown编译为静态html时速度越来越慢了，并且还有一个问题是每次页面渲染的时候都会加载许多重复的js、css文件导致访问速度就算是在本地也非常缓慢，于是笔者决定将博客框架更换为hugo。 hugo是go语言编写的静态网站生成器，在安装部署上都十分的简单快速，尽管主题数量相对于hexo而言较少，但是主题数量也达到了一千多个。hugo在GitHub上的star数量达到了57.1k，比hexo还要高。并且hugo的更新频率远高于hexo。 ","date":"2022-02-15","objectID":"/posts/%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8hugo/:1:0","tags":["blog","Github"],"title":"Hugo + Github Action 部署静态网站","uri":"/posts/%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8hugo/"},{"categories":["tool"],"content":"安装 提示：笔者电脑环境为Windows11 x64，go版本为1.17 ","date":"2022-02-15","objectID":"/posts/%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8hugo/:2:0","tags":["blog","Github"],"title":"Hugo + Github Action 部署静态网站","uri":"/posts/%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8hugo/"},{"categories":["tool"],"content":"安装方式介绍 hugo提供了多种安装方式，并且将编译好的程序发布到了多个包管理平台，比如Homebrew、MacPorts、Chocolatey、Scoop、Snap、Aur等等、可以直接使用对应的包管理器命令安装。 hugo也发布了编译好的程序在GitHub releasess上可以下载电脑对应的版本然后将程序所在目录添加到环境变量即可。 笔者作为一个go程序员，决定使用源码编译的方式进行安装。 ","date":"2022-02-15","objectID":"/posts/%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8hugo/:2:1","tags":["blog","Github"],"title":"Hugo + Github Action 部署静态网站","uri":"/posts/%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8hugo/"},{"categories":["tool"],"content":"源码编译安装 clone源码 如果没有代理建议使用GitHub国内镜像进行clone git clone --depth=1 https://github.com/gohugoio/hugo 编译 编译之前需要注意在hugo的readme中Build and Install the Binaries from Source (Advanced Install)中提到go的版本需要大于等于1.16 编译的时候会去拉取相关依赖，所以不用提前使用go mod tidy或者go mod download检查下载相关依赖 go build 编译完成之后测试一下 可以看到有提示信息了，但是编译出来的程序有63MB releases发布的最新的Windows版本只有48MB，去掉符号表和调试信息再次编译，然后编译出来的版本只有49MB了 go build -ldflags \"-s -w\" 最后是添加环境变量，当然不添加环境变量也可以，环境变量添加完成之后可以测试一下 hugo version ","date":"2022-02-15","objectID":"/posts/%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8hugo/:2:2","tags":["blog","Github"],"title":"Hugo + Github Action 部署静态网站","uri":"/posts/%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8hugo/"},{"categories":["tool"],"content":"定制网站 初始化网站 注意：笔者并没有添加环境变量，笔者已经安装过hugo，因为写这篇文章所以使用相对路径执行编译出来的hugo.exe 使用new site命令创建site，第三个参数是创建的site存放的目录名称 .\\hugo\\hugo.exe new site site-demo # hugo new site site-demo 运行速度特别快，笔者按下回车就输出提示了，这不禁让笔者想试试hugo和hexo初始化的速度差距到底有多大，于是找来了一台“小水管”。“小水管”上面安装的是linux系统，cpu是i5 6代双核，内存是4G。 hugo： hexo： 从截图中可以看到，尽管hexo初始化静态网站所需要的依赖在本地有缓存的情况下构建整个项目依然花费了16秒 进入目录使用tree命令可以看到hugo生成的文件，其中有一个config.toml文件就是网站的配置文件 . ├── archetypes │ └── default.md ├── config.toml ├── content ├── data ├── layouts ├── static └── themes 生成文章 hugo new posts/hello-hugo.md 如果命令运行成功就会输出一个存放了生成的文章的文件地址，打开文件可以看到生成了文章标题、创建时间、是否是草稿（draft）三个信息，这三个都是hugo默认生成的，当然可以自定义生成模板 为了方便，笔者将hugo的readme内容直接复制过来，然后将draft改为false，读者可以输入任意内容进行测试 设置主题 不管是hexo还是hugo生成的静态网站项目都是没有主题样式的，都需要使用者自己设置主题，hugo的主题地址在官网就能看到https://themes.gohugo.io/ 笔者使用了Hugo PaperMod主题进行演示，按照安装说明进行安装 clone git clone https://github.com/adityatelange/hugo-PaperMod themes/PaperMod --depth=1 查看一下themes文件夹，发现确实多出了PaperMod文件夹 修改配置文件 按照官方文档的说明，这时应该修改 config.yml文件，设置theme选项为PaperMod，但是站点文件中并没有config.yml这个文件，其实在说明文档的顶部intro中已经说明，文档示例将会使用yml/yaml格式，但是在hugo生成的静态站点文件中是使用的toml格式作为配置文件的，包括PaperMod这个主题的配置文件，这时候只需要修改站点根目录下面的config.toml文件，按照toml格式将theme值设置为PaperMod即可 在安装文档的结尾还提供了一份示例配置和一份文章模板感兴趣的读着可以在站点成功启动之后自己尝试设置一下。 启动hugo服务 因为当前我们的环境还处于本地，博客文章内容还是markdown格式的，所以需要使用hugo服务来渲染markdown文章 hugo server # 在启动服务时加上-D参数就会渲染文章抬头draft字段为true的文章 # hugo server -D # 如果想要指定监听端口可以使用-p参数 # hugo server -p 3333 启动的速度非常快，在输出信息的倒数第二行提示服务是监听在本地的1313端口的，打开浏览器进入http://localhost:1313/ 可以看到在网站名称My New Hugo Site旁边还有一个夜间模式按钮，点击博客的主题就会变为黑色，点击文章标题访问详情时笔者已经有了\"起飞\"的感觉，速度非常的快 ","date":"2022-02-15","objectID":"/posts/%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8hugo/:3:0","tags":["blog","Github"],"title":"Hugo + Github Action 部署静态网站","uri":"/posts/%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8hugo/"},{"categories":["tool"],"content":"生成静态网站 现在通过hugo服务已经在本地看到了网站，如果要将网站放到服务器上面并且能够通过公网访问，通常有两种方式可以实现 将生成的站点文件夹打包上传到服务器上面，然后在服务器上面安装和本地版本相同的hugo，使用hugo服务渲染静态网站，修改服务器入站规则就可以通过ip访问到网站了。这种方式一般不推荐使用，如果服务器宕机重启就需要手动启动hugo服务，当然也可以写一个启动服务的脚本，软链接到/etc/rcN.d/下面，每次开机的时候都会自动执行脚本 将本地站点文件编译为静态文件，然后上传到服务器中让服务器软件（nginx，apache等）托管。 终端进入站点目录中，然后使用hugo命令就能将markdown文章编译为静态文件了，生成的静态文件全部放到了public文件夹下面，将public文件夹上传到服务器，然后配置nginx或apache就可以了 ","date":"2022-02-15","objectID":"/posts/%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8hugo/:4:0","tags":["blog","Github"],"title":"Hugo + Github Action 部署静态网站","uri":"/posts/%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8hugo/"},{"categories":["tool"],"content":"Github Action 由于文章存储在本地对于笔者这样有多个电脑的人来说多少是有一点不方便的，而且使用服务器每次上传文件也不太方便，所以笔者选择了使用 GitHub Action + Github Pages 服务 只需要在本地写好 GitHub Action Workflow 配置文件，然后将 hugo 生成的整个博客仓库同步到 GitHub，Action 就会被触发，Wrokflow 中需要通过 hugo 生成静态文件，由于需要使用 Github Pages 服务，所以需要将生成的静态文件推送到 {username}.github.io 仓库中 ","date":"2022-02-15","objectID":"/posts/%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8hugo/:5:0","tags":["blog","Github"],"title":"Hugo + Github Action 部署静态网站","uri":"/posts/%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8hugo/"},{"categories":["tool"],"content":"编写 workflow 配置文件 在博客项目跟路径下创建 .github/workflow/main.yaml 文件，笔者这里使用的是 https://github.com/peaceiris/actions-hugo 的 Action # main.yaml name: GitHub Pages on: push: branches: - main pull_request: jobs: deploy: runs-on: ubuntu-20.04 concurrency: group: ${{ github.workflow }}-${{ github.ref }} steps: - uses: actions/checkout@v2 with: submodules: true fetch-depth: 0 - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: '0.110.0' extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: ${{ github.ref == 'refs/heads/main' }} with: personal_token: ${{ secrets.GITHUB_TOKEN }} external_repository: pujichun/pujichun.github.io publish_dir: ./public 我们需要生成一个 Personal access token，这个 token 用于在 Action 中将生成的静态文件提交到 {username}.github.io，通过 Settings -\u003e Developer Settings -\u003e Personal access token，这个 token 名字可以随便取一个，权限选择 workflow 就可以 创建完成之后会生成一串密钥，只会创建的时候显示，然后复制保存下来，等会需要将它放到博客仓库中 进入到博客仓库中，进入到仓库的设置中，选择 Secrets and variables 中的 Actions 中，选择创建一个 Secrets，Secrets 的名字需要和 workflow 中的 personal_token 相同，上面的 workflow 中设置的是 secrets.GITHUB_TOKEN，所以这个 Secrets 的名字需要是 GITHUB_TOKEN，值就是刚才保存的 token ","date":"2022-02-15","objectID":"/posts/%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8hugo/:5:1","tags":["blog","Github"],"title":"Hugo + Github Action 部署静态网站","uri":"/posts/%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8hugo/"},{"categories":["tool"],"content":"同步仓库 当上面仓库设置完成之后就可以将仓库推送到 Github 了，推送之后就可以在仓库中的 Actions 面板中看到 workflow 开始执行，当执行完成之后就可以在 {username}.github.io 中生成提交内容，然后就可以访问 {username}.github.io 这个地址看到生成的网站 ","date":"2022-02-15","objectID":"/posts/%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8hugo/:5:2","tags":["blog","Github"],"title":"Hugo + Github Action 部署静态网站","uri":"/posts/%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8hugo/"},{"categories":["Docker"],"content":"用docker来部署服务 ","date":"2021-11-13","objectID":"/posts/docker%E9%83%A8%E7%BD%B2%E6%9C%8D%E5%8A%A1%E4%BB%AC/:0:0","tags":["Docker"],"title":"docker部署服务们","uri":"/posts/docker%E9%83%A8%E7%BD%B2%E6%9C%8D%E5%8A%A1%E4%BB%AC/"},{"categories":["Docker"],"content":"yapi 方案来源于docker-yapi repositories 镜像源 https://mirrors.aliyun.com/alpine/v3.6/main/ https://mirrors.aliyun.com/alpine/v3.6/community/ Dockerfile 用于构建yapi镜像 FROM node:12-alpine COPY repositories /etc/apk/repositories RUN npm install -g yapi-cli --registry https://registry.npm.taobao.org EXPOSE 3000 9090 init-mongo.js 用于初始化mongo，mongo的admin密码和yapi用户名密码都在这里更改 db.createUser({ user: 'admin', pwd: 'admin123456', roles: [ { role: \"root\", db: \"admin\" } ] }); db.auth(\"admin\", \"admin123456\"); db.createUser({ user: 'yapi', pwd: 'yapi123456', roles: [ { role: \"dbAdmin\", db: \"yapi\" }, { role: \"readWrite\", db: \"yapi\" } ] }); docker-compose.yml 编排容器 version: '3.1' services: mongo: image: mongo:4 restart: always environment: MONGO_INITDB_ROOT_USERNAME: root MONGO_INITDB_ROOT_PASSWORD: example MONGO_INITDB_DATABASE: yapi volumes: - ./mongo-conf:/docker-entrypoint-initdb.d - ./mongo/etc:/etc/mongo - ./mongo/data/db:/data/db ports: - 27017:27017 healthcheck: test: [\"CMD\", \"netstat -anp | grep 27017\"] interval: 2m timeout: 10s retries: 3 yapi: build: context: ./ dockerfile: Dockerfile image: yapi # 第一次启动使用 command: \"yapi server\" # 之后使用下面的命令 # command: \"node /my-yapi/vendors/server/app.js\" volumes: - ./my-yapi:/my-yapi ports: - 9090:9090 - 3000:3000 depends_on: - mongo ","date":"2021-11-13","objectID":"/posts/docker%E9%83%A8%E7%BD%B2%E6%9C%8D%E5%8A%A1%E4%BB%AC/:1:0","tags":["Docker"],"title":"docker部署服务们","uri":"/posts/docker%E9%83%A8%E7%BD%B2%E6%9C%8D%E5%8A%A1%E4%BB%AC/"},{"categories":["Docker"],"content":"kafka集群 docker-compose.yaml version: \"3\" services: zookeeper: image: wurstmeister/zookeeper container_name: zookeeper ports: - \"2181:2181\" kafka1: image: wurstmeister/kafka ports: - \"9092:9092\" environment: KAFKA_ADVERTISED_HOST_NAME: 192.168.3.249 KAFKA_CREATE_TOPICS: TestComposeTopic:4:3 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_BROKER_ID: 1 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://192.168.3.249:9092 KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092 container_name: kafka01 volumes: - /var/run/docker.sock:/var/run/docker.sock kafka2: image: wurstmeister/kafka ports: - \"9093:9093\" environment: KAFKA_ADVERTISED_HOST_NAME: 192.168.3.249 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_BROKER_ID: 2 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://192.168.3.249:9093 KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9093 container_name: kafka02 volumes: - /var/run/docker.sock:/var/run/docker.sock kafka3: image: wurstmeister/kafka ports: - \"9094:9094\" environment: KAFKA_ADVERTISED_HOST_NAME: 192.168.3.249 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_BROKER_ID: 3 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://192.168.3.249:9094 KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9094 container_name: kafka03 volumes: - /var/run/docker.sock:/var/run/docker.sock ","date":"2021-11-13","objectID":"/posts/docker%E9%83%A8%E7%BD%B2%E6%9C%8D%E5%8A%A1%E4%BB%AC/:2:0","tags":["Docker"],"title":"docker部署服务们","uri":"/posts/docker%E9%83%A8%E7%BD%B2%E6%9C%8D%E5%8A%A1%E4%BB%AC/"},{"categories":["python"],"content":"日志在开发中还是很常用的，虽然我们可以使用最原始的system.out但是这样做太简陋了，我在开发的过程中很多时候都会使用日志，在处理异常的时候或者执行某个关键方法的时候我都会使用日志，因为这样能更好的监控程序，但是使用python中的logging库还是比较麻烦的，虽然每次都是复制粘贴以前写的配置，但是改起来的时候还是挺麻烦的 ","date":"2021-08-08","objectID":"/posts/python%E6%97%A5%E5%BF%97%E5%BA%93loguru/:0:0","tags":["python"],"title":"python日志库loguru","uri":"/posts/python%E6%97%A5%E5%BF%97%E5%BA%93loguru/"},{"categories":["python"],"content":"loguru 通常我配置好logging之后在程序最开始的时候就进行加载，然后只需要在其他位置实例化logger就行 如果只要getLogger传入的字符串是相同的，那么就会返回同一个logger import logging import os import sys from logging import handlers logger = logging.getLogger(__name__) logger.setLevel(logging.DEBUG) fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\") LOGFILE = os.path.expanduser(\"log\") console_handler = logging.StreamHandler(sys.stderr) console_handler.setFormatter(fmt) logger.addHandler(console_handler) file_handler = handlers.RotatingFileHandler(LOGFILE) file_handler.setFormatter(fmt) logger.addHandler(file_handler) 上面那一堆配置其实也不复杂，但是体验确实不是很好，尤其是要监控某个函数的时候还需要写一个装饰器 但是loguru就很舒服，我不需要配置什么内容，就算使用最原始的logger也能有很好的体验 pip install loguru from loguru import logger logger.debug(\"debug\") logger.info(\"info\") logger.warning(\"warning\") logger.error(\"error\") logger.critical(\"critical\") ","date":"2021-08-08","objectID":"/posts/python%E6%97%A5%E5%BF%97%E5%BA%93loguru/:1:0","tags":["python"],"title":"python日志库loguru","uri":"/posts/python%E6%97%A5%E5%BF%97%E5%BA%93loguru/"},{"categories":["python"],"content":"错误溯源 @logger.catch def foo(a, b): return a / b foo(1, 0) 当然这个装饰器只能在我们程序终止的时候提醒我们，在实际的开发中往往都是需要让程序继续往下走的，通常会在最后做一个except Exception的处理，因为我们不可能知道全部的异常，我们能处理的大多是我们能预料到以及测试的时候遇到的异常，如果按照传统的直接输出方式信息肯定不够，如果debug的话太麻烦，所以可以使用logger.exception def foo(squ): try: return squ[5] except Exception as e: logger.exception(e) finally: return 0 print(foo([1, 2])) 这个功能真的很赞，能在测试的时候轻松的发现问题的原因 ","date":"2021-08-08","objectID":"/posts/python%E6%97%A5%E5%BF%97%E5%BA%93loguru/:1:1","tags":["python"],"title":"python日志库loguru","uri":"/posts/python%E6%97%A5%E5%BF%97%E5%BA%93loguru/"},{"categories":["Python"],"content":"实不相瞒，作为一个喜欢收集壁纸的boy，我盯上wallhaven很久了 你可能觉得我在说这种风格的 实不相瞒我其实不喜欢，我喜欢的是这种 哈哈哈~anyways！ 起因是我的电脑用的是动态壁纸（写了个自动切换的shell脚本），而一个月前我电脑上只有三张壁纸，当我看到wallhaven的壁纸时我第一想法是我该拥有它们，如果你喜欢壁纸可能也和我一样😁😁😁 想要这些壁纸一张一张的手动下载当然是行不通的，需要借助一些特殊手段 正片开始 ","date":"2021-05-19","objectID":"/posts/python-%E5%BC%82%E6%AD%A5%E4%B8%8B%E8%BD%BD%E5%A3%81%E7%BA%B8/:0:0","tags":["Python"],"title":"python 异步下载壁纸","uri":"/posts/python-%E5%BC%82%E6%AD%A5%E4%B8%8B%E8%BD%BD%E5%A3%81%E7%BA%B8/"},{"categories":["Python"],"content":"解析网页 笔者要抓取的壁纸地址：https://wallhaven.cc/search?q=like%3Arddgwm 观察列表页，发现向下滚动后，url中的page参数会改变 然后简单测试一下反爬虫（手段过于暴躁就不展示了），发现没有发爬虫，但出于尊敬还是携带上User-Agent，简单封装一下列表页的请求，一共只有8页，如果读者比较闲也可以自行破解一下total page import requests from requests.exceptions import RequestException def download_html(url) -\u003e str: headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\" } try: resp = requests.get(url, headers=headers) if resp.status_code == 200: return resp.content.decode(\"utf8\") return \"\" except RequestException: return \"\" def run(): link_list = [] for i in range(1, 9): url = f\"https://wallhaven.cc/search?q=like%3Arddgwm\u0026page={i}\" html = download_html(url) if __name__ == \"__main__\": run() 那么接下来就要解析html获取到图片url了 当打开浏览器开发者工具定位到图片的时候发现src的值是小图的链接 点击封面进去之后发现图片是这样的 很显然这个页面是由前端程序渲染出来的，这个页面不是图片，因此需要鼠标右键点击这个页面中的大图片，选中在新标签页中打开图片 当然也可以从这个页面中解析到图片链接，但是没有必要 在新打开的窗口中发现url变成了这样https://w.wallhaven.cc/full/rd/wallhaven-rddgwm.jpg，这个链接和列表页中src的值很像，简单写个函数转换一下 def image_link_process(image_link: str) -\u003e str: # https://w.wallhaven.cc/full/rd/wallhaven-rddgwm.jpg # https://th.wallhaven.cc/small/rd/rddgwm.jpg if \"small\" in image_link: image_link = image_link.replace(\"th\", \"w\") image_link = image_link.replace(\"small\", \"full\") url_path = image_link.split(\"/\") url_path[-1] = f\"wallhaven-{url_path[-1]}\" return \"/\".join(url_path) return \"\" 欧克！运行之后发现能转换成功，那么下一步就是提取到src的值，编写一个简单的脚本测试一下 import requests from lxml import etree def test(): url = \"https://wallhaven.cc/search?q=like%3Arddgwm\u0026page=1\" headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\" } resp = requests.get(url, headers=headers) html = resp.content.decode(\"utf8\") print(html, end=\"\\n\\n\\n\") element = etree.HTML(html) image_links = element.xpath('//img[@alt=\"loading\"]/@src') print(image_links) if __name__ == '__main__': test() 发现不对啊 查看输出的html，发现对应标签没有src属性，但是有data-src属性，因此提取data-src的值，将xpath表达式改为 //img[@alt=\"loading\"]/@data-src 再次运行！这下欧克了！ 接下来就是重头戏了！下载图片！ ","date":"2021-05-19","objectID":"/posts/python-%E5%BC%82%E6%AD%A5%E4%B8%8B%E8%BD%BD%E5%A3%81%E7%BA%B8/:1:0","tags":["Python"],"title":"python 异步下载壁纸","uri":"/posts/python-%E5%BC%82%E6%AD%A5%E4%B8%8B%E8%BD%BD%E5%A3%81%E7%BA%B8/"},{"categories":["Python"],"content":"下载图片 通常在用requests获取网页的时候，我们会使用text或者content.decode()的方法将请求返回的整个body读出来，但是对于wallhaven上分辨率普遍都很高的图片而言，我们应该分块读取。 因为图片传输的过程本来就是一个流式传输，使用response.get(stream=True)将推迟响应内容的下载，当我们读取的时候才会下载 因为要分块读取，所以我们需要每次读一定数量的字节的文件出来，所以要使用response.iter_content(chunk)的方式去读，一次读chunk个字节的内容 def download_image(image_link): headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\" } resp = requests.get(image_link, headers=headers, stream=True) try: if resp.status_code == 200: with open(file=f'images/{image_link.split(\"/\")[-1]}', mode=\"wb\") as f: for chunk in resp.iter_content(chunk_size=1024): f.write(chunk) print(\"下载成功\") else: print(\"下载失败\") except: print(\"下载失败\") ","date":"2021-05-19","objectID":"/posts/python-%E5%BC%82%E6%AD%A5%E4%B8%8B%E8%BD%BD%E5%A3%81%E7%BA%B8/:2:0","tags":["Python"],"title":"python 异步下载壁纸","uri":"/posts/python-%E5%BC%82%E6%AD%A5%E4%B8%8B%E8%BD%BD%E5%A3%81%E7%BA%B8/"},{"categories":["Python"],"content":"进度条 本文使用的支持进度条的第三方库为rich，rich功能十分强大 安装后在终端中使用下面的命令就能看到样例展示 python -m rich.progress 因为代码简单所以就不废话了 def download_image(image_link): headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\" } resp = requests.get(image_link, headers=headers, stream=True) try: # 获取图片总字节数 file_size = int(resp.headers['content-length']) if resp.status_code == 200: with open(file=f'images/{image_link.split(\"/\")[-1]}', mode=\"wb\") as f: # 创建并启动Progress（进度条）对象 with Progress() as progress: # 给Progress对象添加一个任务，并指定任务大小 task1 = progress.add_task(f'[red]Downloading...{f.name}', total=file_size) for chunk in resp.iter_content(chunk_size=1024): f.write(chunk) # 更新进度条 progress.update(task1, advance=1024) else: print(\"下载失败\") except: print(\"下载失败\") 通过Progress对象可以创建多条进度条 ","date":"2021-05-19","objectID":"/posts/python-%E5%BC%82%E6%AD%A5%E4%B8%8B%E8%BD%BD%E5%A3%81%E7%BA%B8/:3:0","tags":["Python"],"title":"python 异步下载壁纸","uri":"/posts/python-%E5%BC%82%E6%AD%A5%E4%B8%8B%E8%BD%BD%E5%A3%81%E7%BA%B8/"},{"categories":["Python"],"content":"完整程序如下 import requests from requests.exceptions import RequestException from lxml import etree from typing import List from rich.progress import Progress def download_html(url) -\u003e str: headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\" } try: resp = requests.get(url, headers=headers) if resp.status_code == 200: return resp.content.decode(\"utf8\") return \"\" except RequestException: return \"\" def parse_image_link(html: str) -\u003e List: element = etree.HTML(html) links = element.xpath('//img[@alt=\"loading\"]/@data-src') return links def image_link_process(image_link: str) -\u003e str: # https://w.wallhaven.cc/full/v9/wallhaven-v9gk6l.jpg # https://th.wallhaven.cc/small/v9/v9gk6l.jpg if \"small\" in image_link: image_link = image_link.replace(\"th\", \"w\") image_link = image_link.replace(\"small\", \"full\") url_path = image_link.split(\"/\") url_path[-1] = f\"wallhaven-{url_path[-1]}\" return \"/\".join(url_path) return \"\" def download_image(image_link): headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\" } resp = requests.get(image_link, headers=headers, stream=True) try: file_size = int(resp.headers['content-length']) if resp.status_code == 200: with open(file=f'images/{image_link.split(\"/\")[-1]}', mode=\"wb\") as f: with Progress() as progress: task1 = progress.add_task(f'[red]Downloading...{f.name}', total=file_size) for chunk in resp.iter_content(chunk_size=1024): f.write(chunk) progress.update(task1, advance=1024) else: print(\"下载失败\") except: print(\"下载失败\") def run(): link_list = [] for i in range(1, 9): url = f\"https://wallhaven.cc/search?q=like%3Arddgwm\u0026page={i}\" html = download_html(url) links = parse_image_link(html) link_list.extend(links) for image_link in link_list: image_link = image_link_process(image_link) download_image(image_link) if __name__ == '__main__': run() 这就完了？当然没有，可能各位读者已经想好怎么用多线程或者多进程加速了，但是啊！就是这个但是！多线程多没意思！协程~~~冲！ 提示：使用并发会导致进度条对象重复，但是并不影响效果 因为篇幅已经较长，所以笔者便不再讲述程序如何编写，公众号后台回复“wallhaven”即可获取协程版程序 ","date":"2021-05-19","objectID":"/posts/python-%E5%BC%82%E6%AD%A5%E4%B8%8B%E8%BD%BD%E5%A3%81%E7%BA%B8/:4:0","tags":["Python"],"title":"python 异步下载壁纸","uri":"/posts/python-%E5%BC%82%E6%AD%A5%E4%B8%8B%E8%BD%BD%E5%A3%81%E7%BA%B8/"},{"categories":["Git"],"content":"记录一些 git 中不常用的操作 ","date":"2021-05-06","objectID":"/posts/git%E6%93%8D%E4%BD%9C%E5%B0%8F%E6%8A%80%E5%B7%A7/:0:0","tags":["Git"],"title":"git 操作小技巧","uri":"/posts/git%E6%93%8D%E4%BD%9C%E5%B0%8F%E6%8A%80%E5%B7%A7/"},{"categories":["Git"],"content":"登录密码输入 我不喜欢使用ssh验证，并没有配置过Gitee和Github的ssh验证，只是想简简单单的输入一下密码。 因为换了新的电脑，我原来的电脑使用的是Archlinux，新电脑还是Windows，刚换电脑的时候，我还是使用Archlinux进行开发，后来需要进行数据挖掘才逐步迁移了环境，但是我基本没有用过Windows上的Git进行push，我已经忘了是什么时候的事情了，我的Gitee密码和Github密码很像，在push时输错过密码。 三点前提： 不记得是否用Windows上的git推送成功过 推送的时候肯定输错过密码 没有配置过ssh验证 因此推断是Windows或者Git保存了我输错的密码，在我push的时候就自动将用户名和密码拿去验证了，Windows下存储密码是在控制面板-\u003e用户帐户\u003e管理你的凭据-\u003eWindows凭据中，打开一看，果然发现了我有过Github和Gitee的记录，因此将凭据删除。 再次推送，出现弹窗输入账户和密码，发现推送成功。修改仓库，再次提交并推送，发现并没有输入密码，那么一定是Windows又存储了。Windows为什么要存储这个账户和密码，连接服务器时也没有存储的过程，怀疑是Git干的。 通过查看Git的官方文档，凭证存储，发现确有其事，因此找到Git安装路径中的etc文件夹，修改里面的gitconfig文件，删除[credential]配置项，然后删除Windows中的凭据，再次推送后发现Windows并没有存储了。 但是还有一个问题，每次推送输用户名和密码的时候都会有一个弹窗出来，体验很不好，使用以下命令解决 git config --global --unset credential.helper ","date":"2021-05-06","objectID":"/posts/git%E6%93%8D%E4%BD%9C%E5%B0%8F%E6%8A%80%E5%B7%A7/:1:0","tags":["Git"],"title":"git 操作小技巧","uri":"/posts/git%E6%93%8D%E4%BD%9C%E5%B0%8F%E6%8A%80%E5%B7%A7/"},{"categories":["Git"],"content":"git diff output 在做 git diff 的时候内容很少，但是总是不会输出到终端中，而是进入了一个新的页面，查到可以通过命令操作 git config pager.diff false 看来想要将 git 的一些东西直接输出可以配置 pager 参数，也可以在命令中增加 --no-pager 参数 git --no-pager \u003csubcommand\u003e \u003coptions\u003e 可以做全局配置 git config --global core.pager cat ","date":"2021-05-06","objectID":"/posts/git%E6%93%8D%E4%BD%9C%E5%B0%8F%E6%8A%80%E5%B7%A7/:2:0","tags":["Git"],"title":"git 操作小技巧","uri":"/posts/git%E6%93%8D%E4%BD%9C%E5%B0%8F%E6%8A%80%E5%B7%A7/"},{"categories":["Python"],"content":"collections模块实现了特定目标的容器，这些容器能够帮助开发者快速简洁的实现想法 ","date":"2021-04-26","objectID":"/posts/%E5%8A%A0%E9%80%9F%E4%B8%8B%E7%8F%AD%E4%B9%8Bcollections%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9D%97/:0:0","tags":["collections","Python","容器"],"title":"加速下班之collections模块","uri":"/posts/%E5%8A%A0%E9%80%9F%E4%B8%8B%E7%8F%AD%E4%B9%8Bcollections%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9D%97/"},{"categories":["Python"],"content":"namedtuple namedtuple又被称为具名元组，官方解释为“创建命名元组子类的工厂函数”，也就是说namedtuple是一个工厂函数，那么namedtuple是如何将传入的参数构建出一个类并返回的呢？其实这上面又回到了Python中一切皆对象上 namedtuple源码 def namedtuple(typename, field_names, *, rename=False, defaults=None, module=None): \"\"\"Returns a new subclass of tuple with named fields. \u003e\u003e\u003e Point = namedtuple('Point', ['x', 'y']) \u003e\u003e\u003e Point.__doc__ # docstring for the new class 'Point(x, y)' \u003e\u003e\u003e p = Point(11, y=22) # instantiate with positional args or keywords \u003e\u003e\u003e p[0] + p[1] # indexable like a plain tuple 33 \u003e\u003e\u003e x, y = p # unpack like a regular tuple \u003e\u003e\u003e x, y (11, 22) \u003e\u003e\u003e p.x + p.y # fields also accessible by name 33 \u003e\u003e\u003e d = p._asdict() # convert to a dictionary \u003e\u003e\u003e d['x'] 11 \u003e\u003e\u003e Point(**d) # convert from a dictionary Point(x=11, y=22) \u003e\u003e\u003e p._replace(x=100) # _replace() is like str.replace() but targets named fields Point(x=100, y=22) \"\"\" # Validate the field names. At the user's option, either generate an error # message or automatically replace the field name with a valid name. if isinstance(field_names, str): field_names = field_names.replace(',', ' ').split() field_names = list(map(str, field_names)) typename = _sys.intern(str(typename)) if rename: seen = set() for index, name in enumerate(field_names): if (not name.isidentifier() or _iskeyword(name) or name.startswith('_') or name in seen): field_names[index] = f'_{index}' seen.add(name) for name in [typename] + field_names: if type(name) is not str: raise TypeError('Type names and field names must be strings') if not name.isidentifier(): raise ValueError('Type names and field names must be valid ' f'identifiers: {name!r}') if _iskeyword(name): raise ValueError('Type names and field names cannot be a ' f'keyword: {name!r}') seen = set() for name in field_names: if name.startswith('_') and not rename: raise ValueError('Field names cannot start with an underscore: ' f'{name!r}') if name in seen: raise ValueError(f'Encountered duplicate field name: {name!r}') seen.add(name) field_defaults = {} if defaults is not None: defaults = tuple(defaults) if len(defaults) \u003e len(field_names): raise TypeError('Got more default values than field names') field_defaults = dict(reversed(list(zip(reversed(field_names), reversed(defaults))))) # Variables used in the methods and docstrings field_names = tuple(map(_sys.intern, field_names)) num_fields = len(field_names) arg_list = ', '.join(field_names) if num_fields == 1: arg_list += ',' repr_fmt = '(' + ', '.join(f'{name}=%r' for name in field_names) + ')' tuple_new = tuple.__new__ _dict, _tuple, _len, _map, _zip = dict, tuple, len, map, zip # Create all the named tuple methods to be added to the class namespace namespace = { '_tuple_new': tuple_new, '__builtins__': {}, '__name__': f'namedtuple_{typename}', } code = f'lambda _cls, {arg_list}: _tuple_new(_cls, ({arg_list}))' __new__ = eval(code, namespace) __new__.__name__ = '__new__' __new__.__doc__ = f'Create new instance of {typename}({arg_list})' if defaults is not None: __new__.__defaults__ = defaults @classmethod def _make(cls, iterable): result = tuple_new(cls, iterable) if _len(result) != num_fields: raise TypeError(f'Expected {num_fields} arguments, got {len(result)}') return result _make.__func__.__doc__ = (f'Make a new {typename} object from a sequence ' 'or iterable') def _replace(self, /, **kwds): result = self._make(_map(kwds.pop, field_names, self)) if kwds: raise ValueError(f'Got unexpected field names: {list(kwds)!r}') return result _replace.__doc__ = (f'Return a new {typename} object replacing specified ' 'fields with new values') def __repr__(self): 'Return a nicely formatted representation string' return self.__class__.__name__ + repr_fmt % self def _asdict(self): 'Return a new dict which maps field names to their values.' return _dict(_zip(self._fields, self)) def __getnewargs__(self): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple(self) # Modify function metadata to help with introspection and debugging for method in ( __new__, _make.__func__, _replace, __","date":"2021-04-26","objectID":"/posts/%E5%8A%A0%E9%80%9F%E4%B8%8B%E7%8F%AD%E4%B9%8Bcollections%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9D%97/:1:0","tags":["collections","Python","容器"],"title":"加速下班之collections模块","uri":"/posts/%E5%8A%A0%E9%80%9F%E4%B8%8B%E7%8F%AD%E4%B9%8Bcollections%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9D%97/"},{"categories":["Python"],"content":"Counter Counter是一种容器类型，源码很长，这里就展示了，Counter继承自dict，所以Counter的大部分功能和dict相同，但是Counter的主要功能就和名字一样，计数！也就是统计序列中的元素出现的次数 def update(self, iterable=None, /, **kwds): '''Like dict.update() but add counts instead of replacing them. Source can be an iterable, a dictionary, or another Counter instance. \u003e\u003e\u003e c = Counter('which') \u003e\u003e\u003e c.update('witch') # add elements from another iterable \u003e\u003e\u003e d = Counter('watch') \u003e\u003e\u003e c.update(d) # add elements from another counter \u003e\u003e\u003e c['h'] # four 'h' in which, witch, and watch 4 ''' # The regular dict.update() operation makes no sense here because the # replace behavior results in the some of original untouched counts # being mixed-in with all of the other counts for a mismash that # doesn't have a straight-forward interpretation in most counting # contexts. Instead, we implement straight-addition. Both the inputs # and outputs are allowed to contain zero and negative counts. if iterable is not None: if isinstance(iterable, _collections_abc.Mapping): if self: self_get = self.get for elem, count in iterable.items(): self[elem] = count + self_get(elem, 0) else: # fast path when counter is empty super().update(iterable) else: _count_elements(self, iterable) if kwds: self.update(kwds) Counter在实例化的的过程中执行__init__方法时会调用update方法，在update方法中如果传入的可迭代对象是映射类型，那么将会遍历添加对应的映射关系，因为Counter本身就继承自dict所以它自己就是一个字典，它内部也没有维护字典，所以for循环遍历添加键的计数时是self[slem]，因此这也要求如果传入的是映射类型那么需要是key(Any):value(int)形式的，如果不是映射类型，那么将会执行_count_elements函数 def _count_elements(mapping, iterable): 'Tally elements from the iterable.' mapping_get = mapping.get for elem in iterable: mapping[elem] = mapping_get(elem, 0) + 1 这个函数很简单，就干了一件事情，遍历序列中的元素，然后将该元素的计数加一 示例 from collections import Counter c = Counter([1, 2, 3, 4, 5, 10, 221, 12, 4, 2]) print(c) # Counter({2: 2, 4: 2, 1: 1, 3: 1, 5: 1, 10: 1, 221: 1, 12: 1}) c.update({7: 10}) # 将键值对按值排序后，返回指定数量的键值对，如果参数是2，那么将返回前2个 print(c.most_common(1)) # [(7, 10)] ","date":"2021-04-26","objectID":"/posts/%E5%8A%A0%E9%80%9F%E4%B8%8B%E7%8F%AD%E4%B9%8Bcollections%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9D%97/:2:0","tags":["collections","Python","容器"],"title":"加速下班之collections模块","uri":"/posts/%E5%8A%A0%E9%80%9F%E4%B8%8B%E7%8F%AD%E4%B9%8Bcollections%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9D%97/"},{"categories":["Python"],"content":"deque deque又被称作双端队列，是一个很像list的数据结构，但是它并不是继承或维护的list，相比于list实现的队列，deque实现的队列拥有更低的时间和空间复杂度。list实现在出队（pop）和插入（insert）时的空间复杂度大约为O(n)，deque在出队（pop）和入队（append）时的时间复杂度是O(1)，并且deque是线程安全的，说到线程安全，Python中的Queue内部维护的就是deque class Queue: '''Create a queue object with a given maximum size. If maxsize is \u003c= 0, the queue size is infinite. ''' def __init__(self, maxsize=0): self.maxsize = maxsize self._init(maxsize) # mutex must be held whenever the queue is mutating. All methods # that acquire mutex must release it before returning. mutex # is shared between the three conditions, so acquiring and # releasing the conditions also acquires and releases mutex. self.mutex = threading.Lock() # Notify not_empty whenever an item is added to the queue; a # thread waiting to get is notified then. self.not_empty = threading.Condition(self.mutex) # Notify not_full whenever an item is removed from the queue; # a thread waiting to put is notified then. self.not_full = threading.Condition(self.mutex) # Notify all_tasks_done whenever the number of unfinished tasks # drops to zero; thread waiting to join() is notified to resume self.all_tasks_done = threading.Condition(self.mutex) self.unfinished_tasks = 0 def _init(self, maxsize): self.queue = deque() deque为什么会被称作双端队列呢？因为deque实现了从队列两端添加和删除元素的方法。 class deque(object): \"\"\" deque([iterable[, maxlen]]) --\u003e deque object A list-like sequence optimized for data accesses near its endpoints. \"\"\" def append(self, *args, **kwargs): # real signature unknown \"\"\" Add an element to the right side of the deque. \"\"\" pass def appendleft(self, *args, **kwargs): # real signature unknown \"\"\" Add an element to the left side of the deque. \"\"\" pass def pop(self, *args, **kwargs): # real signature unknown \"\"\" Remove and return the rightmost element. \"\"\" pass def popleft(self, *args, **kwargs): # real signature unknown \"\"\" Remove and return the leftmost element. \"\"\" pass 注释已经说的很详细了，deque的源码很容易阅读，建议读者都去阅读一下，这里介绍一个非常有意思的方法rotate，用rotate方法可以将队列进行旋转（这里指的是deque） def rotate(self, *args, **kwargs): # real signature unknown \"\"\" Rotate the deque n steps to the right (default n=1). If n is negative, rotates left. \"\"\" pass 传入的参数n可以为正也可以为负，如果为负就是将左边的元素往右移动n个，如果为正就是将右边的元素往左移动n个，也就是将这个队列看作一个环 from collections import ChainMap, deque q = deque([1, 2, 3, 4]) q.rotate(1) print(q) # deque([4, 1, 2, 3]) q.rotate(1) print(q) # deque([3, 4, 1, 2]) q.rotate(1) print(q) # deque([2, 3, 4, 1]) q.rotate(-1) print(q) # deque([3, 4, 1, 2]) q.rotate(-2) print(q) # deque([1, 2, 3, 4]) ","date":"2021-04-26","objectID":"/posts/%E5%8A%A0%E9%80%9F%E4%B8%8B%E7%8F%AD%E4%B9%8Bcollections%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9D%97/:3:0","tags":["collections","Python","容器"],"title":"加速下班之collections模块","uri":"/posts/%E5%8A%A0%E9%80%9F%E4%B8%8B%E7%8F%AD%E4%B9%8Bcollections%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9D%97/"},{"categories":["Python"],"content":"ChainMap 一个 ChainMap 将多个字典或者其他映射组合在一起，创建一个单独的可更新的视图。什么意思呢？ 在Python中的字典里，因为键值是映射关系，底层是hash表，因此一个键只能映射到一个值，但是ChainMap通过维护一个列表来实现多个键共存，也就是他们还在原来的字典中。 class ChainMap(_collections_abc.MutableMapping): ''' A ChainMap groups multiple dicts (or other mappings) together to create a single, updateable view. The underlying mappings are stored in a list. That list is public and can be accessed or updated using the *maps* attribute. There is no other state. Lookups search the underlying mappings successively until a key is found. In contrast, writes, updates, and deletions only operate on the first mapping. ''' def __init__(self, *maps): '''Initialize a ChainMap by setting *maps* to the given mappings. If no mappings are provided, a single empty dictionary is used. ''' self.maps = list(maps) or [{}] # always at least one map 可以看到在__init__方法中，接收一个参数maps，这个参数是不定长参数，也就是说可以传多个dict，如果什么参数都没有传，那么将是一个列表中包含一个空字典。 这个过程我们可以通过下面的函数简单理解 def chain(*maps): cha = list(maps) or [{}] return cha a = {'x': 1, 'z': 3} b = {'y': 2, 'z': 4} print(chain(a, b)) # [{'x': 1, 'z': 3}, {'y': 2, 'z': 4}] ChainMap示例如下 from collections import ChainMap a = {'x': 1, 'z': 3} b = {'y': 2, 'z': 4} chain1 = ChainMap() chain2 = ChainMap(a) chain3 = ChainMap(a, b) print(chain1) # ChainMap({}) print(chain2) # ChainMap({'x': 1, 'z': 3}) print(chain3) # ChainMap({'x': 1, 'z': 3}, {'y': 2, 'z': 4}) 至于输出不一样是因为源码中的__repr__方法导致的 @_recursive_repr() def __repr__(self): return f'{self.__class__.__name__}({\", \".join(map(repr, self.maps))})' ChainMap在进行遍历的时候往往只会找一个键进行取值，比如上面的程序中两个dict都有z，但是在遍历的时候只会取到第一个 from collections import ChainMap a = {'x': 1, 'z': 3} b = {'y': 2, 'z': 4} chain = ChainMap(a, b) for k, v in chain.items(): print(k,v) # y 2 # z 3 # x 1 当对ChainMap修改的时候亦是如此 注意：前面说过，ChainMap中list维护的是字典的引用，因此如果对ChainMap进行修改，将会使原来dict中的值改变 from collections import ChainMap a = {'x': 1, 'z': 3} b = {'y': 2, 'z': 4} chain = ChainMap(a, b) chain.maps[0]['x'] = 10 print(a) # {'x': 10, 'z': 3} 注意，这里的maps不是方法，就是内部维护的列表，所以尽量不要调用 ","date":"2021-04-26","objectID":"/posts/%E5%8A%A0%E9%80%9F%E4%B8%8B%E7%8F%AD%E4%B9%8Bcollections%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9D%97/:4:0","tags":["collections","Python","容器"],"title":"加速下班之collections模块","uri":"/posts/%E5%8A%A0%E9%80%9F%E4%B8%8B%E7%8F%AD%E4%B9%8Bcollections%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9D%97/"},{"categories":["Python"],"content":"OrderedDict OrderedDict继承了Python中的dict，又被称为有序字典，但是在Python3.6开始dict已经是有序的了，而且操作起来效率更高 from collections import OrderedDict dic1 = dict() dic1[\"name\"] = \"张三\" dic1[\"age\"] = \"18\" dic1[\"height\"] = \"180\" dic1[\"sex\"] = \"1\" dic1[\"glasses\"] = \"1\" dic1[\"hair\"] = \"长\" dic2 = OrderedDict() dic2[\"name\"] = \"张三\" dic2[\"age\"] = \"18\" dic2[\"height\"] = \"180\" dic2[\"sex\"] = \"1\" dic2[\"glasses\"] = \"1\" dic2[\"hair\"] = \"长\" print(dic1) print(dic2) 读者可以自行验证 笔者这里就介绍OrderedDict的原因有以下几个方法 popitem(last=True)，last参数可以设置pop的方向 from collections import OrderedDict dic2 = OrderedDict() dic2[\"name\"] = \"张三\" dic2[\"age\"] = \"18\" dic2[\"height\"] = \"180\" dic2[\"sex\"] = \"1\" dic2[\"glasses\"] = \"1\" dic2[\"hair\"] = \"长\" while dic2: print(dic2.popitem(last=False)) move_to_end(key, last=True)， from collections import OrderedDict dic2 = OrderedDict() dic2[\"name\"] = \"张三\" dic2[\"age\"] = \"18\" dic2[\"height\"] = \"180\" dic2[\"sex\"] = \"1\" dic2[\"glasses\"] = \"1\" dic2[\"hair\"] = \"长\" print(dic2) # OrderedDict([('name', '张三'), ('age', '18'), ('height', '180'), ('sex', '1'), ('glasses', '1'), ('hair', '长')]) dic2.move_to_end(\"name\") print(dic2) # OrderedDict([('age', '18'), ('height', '180'), ('sex', '1'), ('glasses', '1'), ('hair', '长'), ('name', '张三')]) __reversed__，在Python3.8之前dict是没有__reversed__方法的，虽然现在有了，不过本文还是提一下 from collections import OrderedDict dic2 = OrderedDict() dic2[\"name\"] = \"张三\" dic2[\"age\"] = \"18\" dic2[\"height\"] = \"180\" dic2[\"sex\"] = \"1\" dic2[\"glasses\"] = \"1\" dic2[\"hair\"] = \"长\" print(dic2) for item in reversed(dic2.items()): print(item) ","date":"2021-04-26","objectID":"/posts/%E5%8A%A0%E9%80%9F%E4%B8%8B%E7%8F%AD%E4%B9%8Bcollections%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9D%97/:5:0","tags":["collections","Python","容器"],"title":"加速下班之collections模块","uri":"/posts/%E5%8A%A0%E9%80%9F%E4%B8%8B%E7%8F%AD%E4%B9%8Bcollections%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9D%97/"},{"categories":["Python"],"content":"defaultdict defaultdict和OrderedDict一样是dict的一个子类，就和名字一样，在初始化的时候我们可以指定键值对的value是什么类型的 from collections import defaultdict d = defaultdict(list) d[\"person\"].append(\"张三\") d[\"person\"].append(\"李四\") d[\"class\"].append(\"1班\") d[\"class\"].extend([\"二班\", \"三班\"]) print(d) # defaultdict(\u003cclass 'list'\u003e, {'person': ['张三', '李四'], 'class': ['1班', '二班', '三班']}) 要把value设置为什么类型，初始化的时候指定好就行了 ","date":"2021-04-26","objectID":"/posts/%E5%8A%A0%E9%80%9F%E4%B8%8B%E7%8F%AD%E4%B9%8Bcollections%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9D%97/:6:0","tags":["collections","Python","容器"],"title":"加速下班之collections模块","uri":"/posts/%E5%8A%A0%E9%80%9F%E4%B8%8B%E7%8F%AD%E4%B9%8Bcollections%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9D%97/"},{"categories":["Python"],"content":"如果经常使用Python应该能注意到Python中经常出现with关键字。 ","date":"2021-04-24","objectID":"/posts/python%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E5%99%A8/:0:0","tags":["Python"],"title":"Python上下文管理器","uri":"/posts/python%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E5%99%A8/"},{"categories":["Python"],"content":"前言 在程序中当文件资源使用完之后我们需要将文件对象关闭，数据库连接操作完成之后需要将数据库关闭，线程池、连接池、进程池使用完之后我们需要将资源对象放回队列中维护，socket长链接用完之后我们需要断开连接。 在写程序的时候我们总是需要考虑资源的回收，在Go语言中经常要用到defer对资源进行管理，在Python中我们可以考虑的方式很多，如__del__协议，但是使用__del__协议在一些场景下并不能简单优雅的完成需求，因为我们需要花更多的精力去构建一个能用的对象以及控制对象的销毁，如果遇到循环引用和多引用将导致无法正常销毁实例。而使用上下文协议将会使程序的健壮性更强。 ","date":"2021-04-24","objectID":"/posts/python%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E5%99%A8/:1:0","tags":["Python"],"title":"Python上下文管理器","uri":"/posts/python%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E5%99%A8/"},{"categories":["Python"],"content":"上下文管理器协议 在Python中任何实现了__enter__和__exit__的类实例化的对象都可以被称作上下文管理器对象，上下文管理器对象可以被with关键字进行管理，通常with管理上下文管理器的基本语法如下 class ContextManagerObject: def __enter__(self): pass def __exit__(self): pass with ContextManagerObject as cmo: ... with关键字后面跟实现上线文管理器协议的对象，然后__enter__方法会被执行，得到的结果将赋值给as后的变量，当with的作用域结束后将执行对象的__exit__方法 ","date":"2021-04-24","objectID":"/posts/python%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E5%99%A8/:2:0","tags":["Python"],"title":"Python上下文管理器","uri":"/posts/python%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E5%99%A8/"},{"categories":["Python"],"content":"手动实现open方法 示例： class Open(object): def __init__(self, file, mode, encoding, **kwargs) -\u003e None: self.file = file self.mode = mode self.encoding = encoding self.kwargs = kwargs def __enter__(self): print(\"执行__enter__\") self.f = open(file=self.file, mode=self.mode, encoding=self.encoding, **self.kwargs) return self.f def __exit__(self, exc_type, exc_value, traceback): print(\"执行__exit__\") self.f.close() with Open('./demo', mode=\"w\", encoding=\"utf8\") as f: f.write(\"Hello World!\") 在上面的程序中尝试着模仿内建的open去实现了一个支持上下文管理器协议的Open类，这个类型的对象只要被with进行管理，就会执行__enter__方法，当作用域范围结束之后就会执行__exit__方法，因此我们不需要手动再去关闭文件，这样让我们的程序的健壮性以及可维护性都得到了提高。在协程中如果想要这样使用，需要实现__aenter__方法和__aexit__方法。 ","date":"2021-04-24","objectID":"/posts/python%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E5%99%A8/:3:0","tags":["Python"],"title":"Python上下文管理器","uri":"/posts/python%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E5%99%A8/"},{"categories":["Python"],"content":"contextmanager装饰器 上面提到的上下文管理器协议是需要在类中实现的，如果想将类中的方法，或者是将一个普通的函数变为可以通过with关键字进行管理，就可以使用contextmanager装饰器装饰，contextmanager装饰器可以让被装饰的普通函数变为一个可以被with关键字管理的上下文管理器对象。 from contextlib import contextmanager @contextmanager def open_file(file: str, mode: str, encoding: str): # 使用with关键字之后返回yield后面的内容 f = open(file=file, mode=mode, encoding=encoding) try: yield f # with作用域结束后 finally: f.close() with open_file(file='./demo', mode='w', encoding='utf8') as f: f.write('Hello World') ","date":"2021-04-24","objectID":"/posts/python%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E5%99%A8/:3:1","tags":["Python"],"title":"Python上下文管理器","uri":"/posts/python%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E5%99%A8/"},{"categories":["Python"],"content":"contextmanager原理 从装饰器的原理中可以知道，contextmanager应该是将函数包装成了一个实现了上下文管理器协议的对象，然后将这个对象的引用指向了这个函数名。因此看似是在调用函数，其实是在调用这个类。 contextmanager源码 def contextmanager(func): \"\"\"@contextmanager decorator. Typical usage: @contextmanager def some_generator(\u003carguments\u003e): \u003csetup\u003e try: yield \u003cvalue\u003e finally: \u003ccleanup\u003e This makes this: with some_generator(\u003carguments\u003e) as \u003cvariable\u003e: \u003cbody\u003e equivalent to this: \u003csetup\u003e try: \u003cvariable\u003e = \u003cvalue\u003e \u003cbody\u003e finally: \u003ccleanup\u003e \"\"\" @wraps(func) def helper(*args, **kwds): return _GeneratorContextManager(func, args, kwds) return helper 从源码中可以看到的确是这样，contextmanager将被装饰函数作为参数传给了_GeneratorContextManager这个类进行实例化 _GeneratorContextManager源码 class _GeneratorContextManager(_GeneratorContextManagerBase, AbstractContextManager, ContextDecorator): \"\"\"Helper for @contextmanager decorator.\"\"\" def _recreate_cm(self): # _GCM instances are one-shot context managers, so the # CM must be recreated each time a decorated function is # called return self.__class__(self.func, self.args, self.kwds) def __enter__(self): # do not keep args and kwds alive unnecessarily # they are only needed for recreation, which is not possible anymore del self.args, self.kwds, self.func try: return next(self.gen) except StopIteration: raise RuntimeError(\"generator didn't yield\") from None def __exit__(self, type, value, traceback): if type is None: try: next(self.gen) except StopIteration: return False else: raise RuntimeError(\"generator didn't stop\") else: if value is None: # Need to force instantiation so we can reliably # tell if we get the same exception back value = type() try: self.gen.throw(type, value, traceback) except StopIteration as exc: # Suppress StopIteration *unless* it's the same exception that # was passed to throw(). This prevents a StopIteration # raised inside the \"with\" statement from being suppressed. return exc is not value except RuntimeError as exc: # Don't re-raise the passed in exception. (issue27122) if exc is value: return False # Likewise, avoid suppressing if a StopIteration exception # was passed to throw() and later wrapped into a RuntimeError # (see PEP 479). if type is StopIteration and exc.__cause__ is value: return False raise except: # only re-raise if it's *not* the exception that was # passed to throw(), because __exit__() must not raise # an exception unless __exit__() itself failed. But throw() # has to raise the exception to signal propagation, so this # fixes the impedance mismatch between the throw() protocol # and the __exit__() protocol. # # This cannot use 'except BaseException as exc' (as in the # async implementation) to maintain compatibility with # Python 2, where old-style class exceptions are not caught # by 'except BaseException'. if sys.exc_info()[1] is value: return False raise raise RuntimeError(\"generator didn't stop after throw()\") _GeneratorContextManager是多继承，但是这里的关键是上下文管理器协议，在__enter__方法中return了一个next(self.gen)，我们外部定义的函数是一个生成器，可以通过next函数从执行结果中取值，因此这个self.gen可能就是外部函数执行的结果。而事实也是如此 _GeneratorContextManagerBase源码 class _GeneratorContextManagerBase: \"\"\"Shared functionality for @contextmanager and @asynccontextmanager.\"\"\" def __init__(self, func, args, kwds): self.gen = func(*args, **kwds) self.func, self.args, self.kwds = func, args, kwds # Issue 19330: ensure context manager instances have good docstrings doc = getattr(func, \"__doc__\", None) if doc is None: doc = type(self).__doc__ self.__doc__ = doc # Unfortunately, this still doesn't provide good help output when # inspecting the created context manager instances, since pydoc # currently bypasses the instance docstring and shows the docstring # for the class instead. # See http://bugs.python.org/issue19404 for more details. 在_GeneratorContextManagerBase的源码中，初始化函数__init__就执行了外部传入的func函数，然后赋值给了self.gen 当with语句块结束之后，将执行__exit__中的内容，因此继续使用next(self.gen)将执行外部被装饰函数中yield中的内容 而AbstractContextManager就是多态 class AbstractContextManager(abc.ABC): \"\"\"An abstract base class for context managers.\"\"\" __class_getitem__ = classmethod(GenericAlias) def __enter__(","date":"2021-04-24","objectID":"/posts/python%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E5%99%A8/:3:2","tags":["Python"],"title":"Python上下文管理器","uri":"/posts/python%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E5%99%A8/"},{"categories":["设计模式"],"content":"单例模式，属于创建类型的一种常用的软件设计模式。通过单例模式的方法创建的类在当前进程中只有一个实例 ","date":"2021-04-21","objectID":"/posts/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/:0:0","tags":["设计模式","Python","Go"],"title":"单例模式","uri":"/posts/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"常见的单例模式应用场景 日志对象：一般来说日志对象是单例的，因为程序在运行的时候控制日志的输出、追加都是由一个日志对象进行维护的 电脑回收站：不管是mac还是Windows的回收站（废纸篓）都是单例的 Windows任务管理器 线程池、数据库连接池：去创建过多的线程池或者连接池都会对程序的性能造成影响，并且线程池或数据库连接池已经维护了很多的线程对象或者连接对象，没必要重复创建销毁 网站ip访问计数器 以上都是一些常见的单例模式的应用场景 ","date":"2021-04-21","objectID":"/posts/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/:1:0","tags":["设计模式","Python","Go"],"title":"单例模式","uri":"/posts/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"Python的__new__实现单例模式 Python是面向对象语言，要实现单例模式，只需要控制类的实例化过程即可 class IPCounter(object): _instance = None _count = 0 def __new__(cls): # 判断是否存在当前类的实例对象 if not cls._instance: # 如果不存在则创建该类的实例对象 cls._instance = super().__new__(cls) # 返回这个类的实例对象 return cls._instance @classmethod def add(cls): cls._count += 1 @classmethod @property def count(cls): return cls._count ip_counter1 = IPCounter() ip_counter1.add() ip_counter2 = IPCounter() print(ip_counter2.count) # 2 print(id(ip_counter1) == id(ip_counter2)) # True 上面的程序中为什么count要设置为类变量？ 如果设置为实例变量，那么每次初始化对象的时候count都会改变 class IPCounter(object): _instance = None def __new__(cls): if not cls._instance: cls._instance = super().__new__(cls) return cls._instance def __init__(self): self.count = 0 def add(self): self.count += 1 ip_counter1 = IPCounter() print(id(ip_counter1)) # 2098428296976 ip_counter1.add() ip_counter2 = IPCounter() print(ip_counter1.count) # 0 print(ip_counter2.count) # 0 print(id(ip_counter1) == id(ip_counter2)) # True 上面的程序运行后ip_counter1和ip_counter2的count都会变为0，虽然对象的id地址并没有改变，但是在实例化ip_counter2的时候__init__方法被重新执行了一遍，所以count应该是类变量，而不是实例变量 注意：上面的例子说明了一个问题，当我们通过单例模式实现实例化的对象都唯一的，内存中不会存在第二个该类型的对象，因此只要我们对该对象有任何修改都会影响到其他层面（并发情况下）的控制 ","date":"2021-04-21","objectID":"/posts/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/:2:0","tags":["设计模式","Python","Go"],"title":"单例模式","uri":"/posts/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"Python装饰器实现单例模式 上面的程序中的确是已经实现了单例，但是由于对象属性存在可能变更的问题，尽管在程序运行中不去改变属性就没有什么问题，但是为了考虑到这种特殊情况就必须不对属性进行赋值 因此我们可以维护一个全局唯一的哈希表，将实例化好的对象存储到哈希表中，如果再次进行实例化就返回已经实例化好的对象，这样就起到了防止属性被修改的作用 def singleton(cls): _instance = {} def _singleton(*args, **kwargs): # 判断这个类有没有对象 if cls not in _instance: # 如果没有则创建一个对象,并保存到字典当中 _instance[cls] = cls(*args, **kwargs) # 将实例对象返回 return _instance[cls] return _singleton @singleton class IPCounter(object): def __init__(self) -\u003e None: self.count = 0 def add(self): self.count += 1 ip_counter1 = IPCounter() ip_counter1.add() ip_counter2 = IPCounter() print(ip_counter2.count) # 1 print(id(ip_counter1) == id(ip_counter2)) # True 装饰器这种方法虽然解决了对象属性的问题，但是笔者并不推荐，笔者更推荐空值对象实例化的方式实现单例模式 ","date":"2021-04-21","objectID":"/posts/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/:3:0","tags":["设计模式","Python","Go"],"title":"单例模式","uri":"/posts/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"Go实现单例模式 package main import \"fmt\" type IPCounter struct { Count int } // 定义一个instance变量用于保存IPCounter的地址 var instance *IPCounter func (c *IPCounter) Add() { c.Count += 1 } func Init() *IPCounter { // 如果IPCounter已经有实例了，那么就会返回该实例的地址 if instance == nil { instance = \u0026IPCounter{Count: 0} } return instance } func main() { ipCounter1 := Init() ipCounter1.Add() fmt.Println(ipCounter1.Count) ipCounter2 := Init() fmt.Println(ipCounter2.Count) fmt.Println(ipCounter1.Count) } 因为Go中有指针的存在，所以完成单例模式的设计会很方便，但是Go是并发性很强的语言，如果在并发的情况下创建该类型的对象，那么在最开始访问的几个协程在进行判断的时候instance都是nil，因此需要确保函数只被执行一次 sync.Once的Do方法能够确保函数只执行一次 package main package main import ( \"fmt\" \"sync\" ) type IPCounter struct { Count int } var ( once sync.Once c *IPCounter ) func (c *IPCounter) Add() { c.Count += 1 } func Init() *IPCounter { // 将初始化的逻辑放到匿名函数中 once.Do(func() { fmt.Println(\"初始化中\") c = new(IPCounter) c.Add() }) fmt.Println(\"初始化已完成\") return c } func main() { for i := 0; i \u003c 10; i++ { ipConfig := Init() fmt.Println(ipConfig) } fmt.Println(c.Count) // 1 } ","date":"2021-04-21","objectID":"/posts/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/:4:0","tags":["设计模式","Python","Go"],"title":"单例模式","uri":"/posts/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"categories":["数据处理"],"content":"pandas数据处理小技巧1——增长率 有如下一组数据data.csv，需要计算每一年的增长率 2015,2016,2017,2018,2019 14.0,13.583333333333334,11.25,9.5,6.583333333333333 53.0,53.75,52.583333333333336,47.75,41.916666666666664 2.0,1.5324999999999998,1.325,1.2166666666666668,1.075 183.0,132.16666666666666,131.33333333333334,139.25,125.75 108.0,98.5,88.58333333333333,80.66666666666667,68.08333333333333 64.0,62.916666666666664,56.083333333333336,50.583333333333336,42.666666666666664 8047587.043913767,9061608.73437349,10532812.13233023,11761045.58411076,17765141.70873344 18170803.33249806,2764507.352052998,9064101.361637741,23820561.03652501,30151960.60064027 -2600910.093767846,-1484838.60873719,-1423809.746289836,-4193999.136869889,-19551559.66273892 -13962845.8889949,-4658095.704427481,-2210798.896100461,-16929512.17251258,9417823.196205355 先读取数据 import pandas as pd data = pd.read_excel('data.csv') data.head() 如果使用循环，就会陷入pandas处理数据的误区中，pandas是用C开发的Python第三方库，使用pandas进行数据处理是非常快的。 ","date":"2021-04-17","objectID":"/posts/pandas%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%B0%8F%E6%8A%80%E5%B7%A7%E4%B8%80/:0:0","tags":["Python","数据处理"],"title":"pandas数据处理小技巧1","uri":"/posts/pandas%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%B0%8F%E6%8A%80%E5%B7%A7%E4%B8%80/"},{"categories":["数据处理"],"content":"shift方法移动数据 数据横向移动一位 data.shift(periods=1, axis=1) 然后做差，将原始数据向后移动一位，再做商就能得到增长率 (data - data.shift(periods=1, axis=1))/data.shift(periods=1, axis=1) ","date":"2021-04-17","objectID":"/posts/pandas%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%B0%8F%E6%8A%80%E5%B7%A7%E4%B8%80/:1:0","tags":["Python","数据处理"],"title":"pandas数据处理小技巧1","uri":"/posts/pandas%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%B0%8F%E6%8A%80%E5%B7%A7%E4%B8%80/"},{"categories":["数据处理"],"content":"diff方法做差分 前面做差值的过程其实就是一阶差分，这个过程可以通过diff方法完成 data.diff(periods=1, axis=1) 后面的过程不再计算 ","date":"2021-04-17","objectID":"/posts/pandas%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%B0%8F%E6%8A%80%E5%B7%A7%E4%B8%80/:2:0","tags":["Python","数据处理"],"title":"pandas数据处理小技巧1","uri":"/posts/pandas%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%B0%8F%E6%8A%80%E5%B7%A7%E4%B8%80/"},{"categories":["数据处理"],"content":"pct_change方法直接计算 pct_change方法可以用来直接计算变化率 data.pct_change(periods=1, axis=1) ","date":"2021-04-17","objectID":"/posts/pandas%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%B0%8F%E6%8A%80%E5%B7%A7%E4%B8%80/:3:0","tags":["Python","数据处理"],"title":"pandas数据处理小技巧1","uri":"/posts/pandas%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%B0%8F%E6%8A%80%E5%B7%A7%E4%B8%80/"},{"categories":["环境配置"],"content":"Kafka是一个高吞吐率的分布式订阅消息队列，传统的关系存储型数据库，如MySQL、MongoDB都有一个痛点，那就是性能较差，在高并发访问的情况下很可能会奔溃掉。其实redis中有一个用于消息订阅的数据结构queue，但是redis中的数据是放到内存当中的，如果消息量过大很可能会造成内存奔溃，而且如果机器发生故障，内存中的数据很可能会清空。 对比于redis，Kafka中发布消息和消费消息的速度都相对较慢，但是Kafka也足以支撑住并发，而且Kafka收到消息后是存储到硬盘当中的，因此就算是服务器宕机也不会造成数据的丢失。 ","date":"2021-04-09","objectID":"/posts/kafka%E4%BD%BF%E7%94%A8/:0:0","tags":["Kafka","Python","Go","并发"],"title":"Kafka安装与使用","uri":"/posts/kafka%E4%BD%BF%E7%94%A8/"},{"categories":["环境配置"],"content":"Kafka介绍 在Kafka中消息的存储与读取被称作生产与消费，Kafka存储的消息都是有时效性的（这个时间可以由开发者自己设置），Kafka就像一个消息的库存市场，消息被分类生产之后发送到Kafka，然后开始分类消费，如果没有消息就会一直轮询 在Kafka集群中，每个Broker就是一个服务器节点，每个Topic都有一个Leader，当生产数据的时候会生产到Leader的服务器，然后Leader会将数据发送给Follower会做一个备份，消费的时候也是从Leader所在服务器中取。 Partition是分区，当并发往Kafka中生产数据的时候如果有多个Partition会将数据分布发送给指定的Partition，也就是一个Topic中的数据将被存储在不同的Partition中。 ","date":"2021-04-09","objectID":"/posts/kafka%E4%BD%BF%E7%94%A8/:1:0","tags":["Kafka","Python","Go","并发"],"title":"Kafka安装与使用","uri":"/posts/kafka%E4%BD%BF%E7%94%A8/"},{"categories":["环境配置"],"content":"Windows平台 ","date":"2021-04-09","objectID":"/posts/kafka%E4%BD%BF%E7%94%A8/:2:0","tags":["Kafka","Python","Go","并发"],"title":"Kafka安装与使用","uri":"/posts/kafka%E4%BD%BF%E7%94%A8/"},{"categories":["环境配置"],"content":"下载 Kafka在所有平台的安装都是同一个安装包，到官网下载编译好的即可：http://kafka.apache.org/ Kafka的启动依赖于ZooKeeper组件，在较新的Kafka安装包中已经带了ZooKeeper，只要配置好启动就行。 下载完成之后解压 然后需要建立一个存放数据的文件夹，因为是在Windows平台下实验使用，所以文件夹就建在安装目录即可 然后再创建两个文件夹分布给Zookeeper和Kafka使用 接着进入config目录修改配置文件 首先修改Zookeeper的配置文件zookeeper.properties，将dataDir修改为刚刚创建的zk_data文件夹 dataDir=D:/soft/kafka_2.12/data/zk_data 然后修改Kafka的配置文件server.properties，将log.dirs修改为刚刚创建的kafka_log文件夹 log.dirs=D:/soft/kafka_2.12/data/kafka_log ","date":"2021-04-09","objectID":"/posts/kafka%E4%BD%BF%E7%94%A8/:2:1","tags":["Kafka","Python","Go","并发"],"title":"Kafka安装与使用","uri":"/posts/kafka%E4%BD%BF%E7%94%A8/"},{"categories":["环境配置"],"content":"启动Kafka 在启动Kafka前需要先启动Zookeeper，在此就不配置环境变量了，通过命令在安装路径启动即可，需要注意的是在Windows下启动脚本都在bin目录下的windows目录下，Linux的是在bin路径下 启动Zookeeper和Kafka都需要指定配置文件 .\\bin\\windows\\zookeeper-server-start.bat .\\config\\zookeeper.properties 然后启动Kafka .\\bin\\windows\\kafka-server-start.bat .\\config\\server.properties ","date":"2021-04-09","objectID":"/posts/kafka%E4%BD%BF%E7%94%A8/:2:2","tags":["Kafka","Python","Go","并发"],"title":"Kafka安装与使用","uri":"/posts/kafka%E4%BD%BF%E7%94%A8/"},{"categories":["环境配置"],"content":"Python实现生产与消费 因为pykafka已经很久没更新了，所以这里使用kafka-python操作Kafka pip install kafka-python 生产者 producer_demo.py from kafka import KafkaProducer from time import sleep def start_producer(): producer = KafkaProducer(bootstrap_servers='127.0.0.1:9092') for i in range(0, 100000): msg = f'msg is {i}' producer.send(topic='FreedomAndDiscipline', value=msg.encode('utf-8')) sleep(3) if __name__ == '__main__': start_producer() 消费者 consumer_demo.py from kafka import KafkaConsumer def start_consumer(): consumer = KafkaConsumer('FreedomAndDiscipline', bootstrap_servers='127.0.0.1:9092') for msg in consumer: print(msg) if __name__ == '__main__': start_consumer() 因为这里并没有设置从Topic开始的位置开始读，所以当consumer_demo.py启动之后只能接收到producer_demo.py后面发送来的数据，可以看到下面的输出内容中第一条消息是msg is 47 ConsumerRecord(topic='FreedomAndDiscipline', partition=0, offset=2, timestamp=1618372466839, timestamp_type=0, key=None, value=b'msg is 47', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=9, serialized_header_size=-1) ConsumerRecord(topic='FreedomAndDiscipline', partition=0, offset=3, timestamp=1618372469851, timestamp_type=0, key=None, value=b'msg is 48', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=9, serialized_header_size=-1) ConsumerRecord(topic='FreedomAndDiscipline', partition=0, offset=4, timestamp=1618372472865, timestamp_type=0, key=None, value=b'msg is 49', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=9, serialized_header_size=-1) ConsumerRecord(topic='FreedomAndDiscipline', partition=0, offset=5, timestamp=1618372475876, timestamp_type=0, key=None, value=b'msg is 50', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=9, serialized_header_size=-1) ConsumerRecord(topic='FreedomAndDiscipline', partition=0, offset=6, timestamp=1618372478879, timestamp_type=0, key=None, value=b'msg is 51', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=9, serialized_header_size=-1) ConsumerRecord(topic='FreedomAndDiscipline', partition=0, offset=7, timestamp=1618372481886, timestamp_type=0, key=None, value=b'msg is 52', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=9, serialized_header_size=-1) ConsumerRecord(topic='FreedomAndDiscipline', partition=0, offset=8, timestamp=1618372484894, timestamp_type=0, key=None, value=b'msg is 53', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=9, serialized_header_size=-1) ConsumerRecord(topic='FreedomAndDiscipline', partition=0, offset=9, timestamp=1618372487894, timestamp_type=0, key=None, value=b'msg is 54', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=9, serialized_header_size=-1) 如果需要从该Topic最开始的位置读，需要设置Partition的位置，然后告诉连接对象offset从哪开始 from kafka import KafkaConsumer def start_consumer(): consumer = KafkaConsumer(bootstrap_servers='127.0.0.1:9092') # 创建Partition对象 tp = TopicPartition('FreedomAndDiscipline', 0) # 给连接对象指定Partition对象 consumer.assign([tp]) # 告诉Kafka消费对象要从指定Partition对象的指定offset开始消费 consumer.seek(tp, offset=0) for msg in consumer: print(msg) if __name__ == '__main__': start_consumer() 现在得到的就是topic为FreedomAndDiscipline，partition为0，offset为0的结果 ConsumerRecord(topic='FreedomAndDiscipline', partition=0, offset=0, timestamp=1618372897061, timestamp_type=0, key=None, value=b'msg is 0', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=8, serialized_header_size=-1) ConsumerRecord(topic='FreedomAndDiscipline', partition=0, offset=1, timestamp=1618372900064, timestamp_type=0, key=None, value=b'msg is 1', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=8, serialized_header_size=-1) ConsumerRecord(topic='FreedomAndDiscipline', partition=0, offset=2, timestamp=1618372903074, timestamp_type=0, key=None, value=b'msg is 2', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=8, serialized_header_size=-1) ConsumerRecord(topic='FreedomAndDiscipline', partition=0, offset=3, timestamp=1618372","date":"2021-04-09","objectID":"/posts/kafka%E4%BD%BF%E7%94%A8/:2:3","tags":["Kafka","Python","Go","并发"],"title":"Kafka安装与使用","uri":"/posts/kafka%E4%BD%BF%E7%94%A8/"},{"categories":["环境配置"],"content":"Go实现生产与消费 生产者程序 package main import ( \"fmt\" \"github.com/Shopify/sarama\" \"log\" \"time\" ) func main() { // Kafka配置对象 config := sarama.NewConfig() // 消息发送成功将消息发送成功的信息发送到Successes channel config.Producer.Return.Successes = true // 配置partition分区位置为指定分区 config.Producer.Partitioner = sarama.NewManualPartitioner // 初始化Producer对象 producer, err := sarama.NewSyncProducer([]string{\"localhost:9092\"}, config) if err != nil { panic(err) } defer func() { if err := producer.Close(); err != nil { log.Println(\"Failed to close producer:\", err) } }() for i := 1; i \u003c 1000; i++ { // 消息设置 msg := \u0026sarama.ProducerMessage{ Topic: \"HelloKafka\", Partition: 0, Value: sarama.StringEncoder(fmt.Sprintf(\"msg is %d\", i)), } // 发送消息 partition, offset, err := producer.SendMessage(msg) if err != nil { panic(err) } if partition != 0 { log.Fatalf(\"message should have been produced to partition 0! but hane been produced to partition %d\", partition) } log.Printf(\"Produced message to partition %d with offset %d\", partition, offset) time.Sleep(time.Second*10) } } 消费者程序 package main import ( \"fmt\" \"github.com/Shopify/sarama\" \"log\" ) func main() { // 初始化消费对象 consumer, err := sarama.NewConsumer([]string{\"127.0.0.1:9092\"}, nil) if err != nil { panic(err) } // 获取指定主题下的partition列表 partitionList, err := consumer.Partitions(\"HelloKafka\") if err != nil { panic(err) } // 遍历partition列表，从partition中消费消息 for _, partition := range partitionList { topic := \"HelloKafka\" // 初始化PartitionConsumer对象 partitionConsumer, err := consumer.ConsumePartition(topic, partition, sarama.OffsetOldest) if err != nil { panic(err) } defer partitionConsumer.AsyncClose() for msg := range partitionConsumer.Messages() { log.Printf(\"Partition:%d Offset:%d Key:%v Value:%v\", msg.Partition, msg.Offset, msg.Key, msg.Value) } } } 在消费者中，如果不想要获取以前的消息，只获取新的消息只需要在ConsumePartition方法中设置offset为sarama.OffsetNewest即可 ","date":"2021-04-09","objectID":"/posts/kafka%E4%BD%BF%E7%94%A8/:2:4","tags":["Kafka","Python","Go","并发"],"title":"Kafka安装与使用","uri":"/posts/kafka%E4%BD%BF%E7%94%A8/"},{"categories":["环境配置"],"content":"使用Docker搭建Kafka集群 在上面的程序中我们都是在单机环境下进行的，在实际的生产环境中几乎都是在集群中进行的，但是使用虚拟机耗费的内存太大，因此使用docker来搭建Kafka集群进行实验 待更新 ","date":"2021-04-09","objectID":"/posts/kafka%E4%BD%BF%E7%94%A8/:3:0","tags":["Kafka","Python","Go","并发"],"title":"Kafka安装与使用","uri":"/posts/kafka%E4%BD%BF%E7%94%A8/"},{"categories":["Python"],"content":"在并发的场景下，不可避免的就是MySQL的访问问题，由于MySQL的性能并不强，因此在进行并发访问的时候往往会使用连接池。 在Python中异步操作的时候性能还算是比较强的，因此建立一个连接池是必须的。数据库连接池就是在程序启动的时候就去创建一定数量的数据库连接对象，然后在向连接池申请连接的时候返回一个连接对象，数据库操作完成之后并不会立刻释放这个连接对象，而是将这个连接对象放回连接池中。 在多线程中和协程中建立的连接池是不同的，多线程下可以使用dbutils或者twisted这两个第三方库建立连接池，在协程中可以使用aiomysql这个第三方库建立异步连接池 ","date":"2021-04-08","objectID":"/posts/python-mysql%E8%BF%9E%E6%8E%A5%E6%B1%A0/:0:0","tags":["Python","MySQL"],"title":"Python MySQL连接池","uri":"/posts/python-mysql%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"categories":["Python"],"content":"多线程版 dbutils 本文使用的dbutils版本为2.0，在写本文时为最新版本 pip install dbutils pymysql 初始化线程池 import pymysql from dbutils.pooled_db import PooledDB config = { \"creator\": pymysql, \"host\": \"127.0.0.1\", \"port\": 3306, \"user\": \"username\", \"password\": \"pw\", \"db\": \"database\", \"charset\": \"utf8\", \"mincached\": 5, \"maxcached\": 8, \"maxconnections\": 50, 'cursorclass': pymysql.cursors.DictCursor } pool = PooledDB(**config) 获取连接对象和游标对象 conn = pool.connection() cursor = conn.cursor() 查询数据库版本 cursor.execute(\"SELECT VERSION()\") conn.commit() print(cursor.fetchone()) 当我们使用完之后需要关闭游标然后释放连接对象 cursor.close() conn.close() 这里conn.close()并不是将连接池关闭了，只是将这个连接对象退回到连接池中。 上面的连接池建立的写法在单个线程中还能使用，当进入了多线程的并发情况下就会有问题 示例db.py from dbutils.pooled_db import PooledDB import pymysql from contextlib import contextmanager class MysqlPool(object): pool: PooledDB _instance = None def __new__(cls, *args, **kwargs): if not cls._instance: config = { \"creator\": pymysql, \"host\": \"127.0.0.1\", \"port\": 3306, \"user\": \"username\", \"password\": \"passwd\", \"db\": \"database\", \"charset\": \"utf8\", \"mincached\": 5, \"maxcached\": 8, \"maxconnections\": 50, 'cursorclass': pymysql.cursors.DictCursor } cls.pool = PooledDB(**config) cls._instance = super().__new__(cls) return cls._instance @classmethod @contextmanager def conn(cls): _conn = cls.pool.connection() yield _conn _conn.commit() _conn.close() p = MysqlPool() with p.conn() as conn: cursor = conn.cursor() cursor.execute(\"SELECT VERSION()\") print(cursor.fetchone()) cursor.close() 虽然在上面的程序中将调用方法加工为了可以通过上下文管理器打开的程序，但是每次使用的时候都要用with方法打开，而且还需要关闭cursor，比较繁琐 from dbutils.pooled_db import PooledDB import pymysql from contextlib import contextmanager from functools import wraps class MysqlPool(object): pool: PooledDB _instance = None def __new__(cls, *args, **kwargs): if not cls._instance: config = { \"creator\": pymysql, \"host\": \"127.0.0.1\", \"port\": 3306, \"user\": \"pujic\", \"password\": \"a\", \"db\": \"demo\", \"charset\": \"utf8\", \"mincached\": 5, \"maxcached\": 8, \"maxconnections\": 50, 'cursorclass': pymysql.cursors.DictCursor } cls.pool = PooledDB(**config) cls._instance = super().__new__(cls) return cls._instance # 使用classmethod装饰器让该方法成为类方法，从而可以访问到pool @classmethod # 使用contextmanager装饰器让conn成为上下文管理器对象 @contextmanager def conn(cls): _conn = cls.pool.connection() yield _conn _conn.commit() _conn.close() p = MysqlPool() def db_connection(func): @wraps(func) def wrapper(*args, **kwargs): with p.conn() as conn: try: cursor = conn.cursor() result = func(*args, **kwargs, cursor=cursor) return result except Exception as e: print(e) # 如果发生异常就回滚 conn.rollback() finally: cursor.close() return wrapper @db_connection def test(cursor): cursor.execute(\"SELECT VERSION()\") print(cursor.fetchone()) test() 上面的程序还存在一个小问题，当程序最开始并发访问的时候肯定会导致单例模式失效，所以在实例化的过程中最好加上锁，这里就不在演示了 为什么上面的程序中不在MysqlPool类中实现__enter__和__exit__方法，而是使用contextmanager装饰器将conn方法变为上下文管理器对象，那是因为如果在类中实现上下文管理器协议，那么拿到的connection或者cursor都将全局唯一，那么在并发下就造成全局只有一个connection的问题，具体细节可以看单例模式和上下文管理器协议， ","date":"2021-04-08","objectID":"/posts/python-mysql%E8%BF%9E%E6%8E%A5%E6%B1%A0/:1:0","tags":["Python","MySQL"],"title":"Python MySQL连接池","uri":"/posts/python-mysql%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"categories":["Python"],"content":"异步协程版 aiomysql本身提供了线程池可供使用者进行操作，因为是异步创建的连接池，所以需要把整个创建过程封装到协程中，官方推荐使用create_pool这个工厂函数来创建连接池，并且连接池也支持异步上下文管理器，cursor也支持异步上下文管理器，但是连接池并不是单例的，所以我们只需要确保连接池在整个事件循环执行的过程中是单例的即可 本文使用的aiomysql版本为0.0.21，在写本文时为最新版本 实例aiodb.py import aiomysql import asyncio class MysqlPool: pool = None @classmethod async def init_pool(cls): try: if not cls.pool: _pool = await aiomysql.create_pool( minsize=5, # 连接池最小值 maxsize=20, # 连接池最大值 host='127.0.0.1', port=3306, user='username', password='passwd', db='database', autocommit=True, # 自动提交模式 ) cls.pool = _pool except Exception as e: print(e) async def test(): p1 = MysqlPool() await p1.init_pool() p2 = MysqlPool() print(id(p1.pool)) print(id(p2.pool)) async with p1.pool.acquire() as conn: async with conn.cursor() as cur: await cur.execute(\"SELECT VERSION()\") print(await cur.fetchone()) if __name__ == '__main__': loop = asyncio.get_event_loop() loop.run_until_complete(test()) 此时通过MysqlPool使用的连接池虽然是单例的了，但是MysqlPool并不是，所以还需要控制MysqlPool的实例化过程，同时也要让整个数据库的操作过程变得简单 import aiomysql import asyncio from functools import wraps class MysqlPool: _pool = None _instance = None def __new__(cls, *args, **kwargs): if not cls._instance: cls._instance = super().__new__(cls) return cls._instance @classmethod async def init_pool(cls): try: if not cls._pool: cls._pool = await aiomysql.create_pool( minsize=5, # 连接池最小值 maxsize=20, # 连接池最大值 host='127.0.0.1', port=3306, user='pujic', password='a', db='demo', autocommit=True, # 自动提交模式 ) except Exception as e: print(e) @classmethod async def pool(cls): # 也可以将连接池初始化的步骤放到类实例化后，减少初始化检查 await cls.init_pool() return cls._pool p1 = MysqlPool() def db_connection(func): @wraps(func) async def wrapper(*args, **kwargs): pool = await p1.pool() async with pool.acquire() as conn: async with conn.cursor() as cursor: try: result = await func(*args, **kwargs, cursor=cursor) return result except Exception as e: print(e) await conn.rollback() return wrapper @db_connection async def test(cursor): await cursor.execute(\"SELECT VERSION()\") print(await cursor.fetchone()) if __name__ == '__main__': loop = asyncio.get_event_loop() loop.run_until_complete(test()) 到此MySQL的访问连接池就基本完成了 ","date":"2021-04-08","objectID":"/posts/python-mysql%E8%BF%9E%E6%8E%A5%E6%B1%A0/:2:0","tags":["Python","MySQL"],"title":"Python MySQL连接池","uri":"/posts/python-mysql%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"categories":["Python"],"content":"结语 以上程序会在并发中出现的存在多个实例的问题就交给读者进行解决了 Python既然是面向对象语言，在进行工程化的程序开发时就更应该对程序进行高度的封装，避免使用重复冗余的代码，很多人在学习Python的时候总是感觉写起来很爽，那是因为学习过程中用到的东西基本都是Python社区的开发者们进行高度封装后的包，或者是Python标准库中高度封装好的包。当进入工程化开发的时候却发现看不懂别人的代码，自己的代码维护起来很困难，一改就是改几个关联文件。作为一个Python开发者，在进行Python程序学习和开发的时候应该将Python视为工程化语言，而不是脚本语言 ","date":"2021-04-08","objectID":"/posts/python-mysql%E8%BF%9E%E6%8E%A5%E6%B1%A0/:3:0","tags":["Python","MySQL"],"title":"Python MySQL连接池","uri":"/posts/python-mysql%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"categories":["数据处理"],"content":"我们在几乎所有的数据处理场景中，用pandas将数据读入内存之后要做的第一件事都是查看数据的描述 可以使用info方法来查看数据信息，包括字段的类型，字段的条数，字段是否包含空值。这是没有处理过的数据，我们可以看到许多的object类型的，这个object就是字符串 在上面的数据中大部分都是分类变量，连续性变量只有室、厅、建筑面积、房屋年限、关注人数、每平价格 但是因为并没有处理过，所以能够读取后直接识别的只有4个字段 data.describe() 对数据进行描述性分析得到的结果也仅仅是这四个字段 我的老师说过能用图展示东西就不要用表格展示，但是如果要生成一套详细的报告图表还是需要几行代码的，于是不得不介绍一个用于数据探索性分析的第三方库pandas-profiling，直接上结果 import pandas_profiling pandas_profiling.ProfileReport(data) 😓报告结果太长，图片太大就展示几张吧！ ","date":"2021-03-21","objectID":"/posts/pandas%E6%95%B0%E6%8D%AE%E6%8F%8F%E8%BF%B0%E6%80%A7%E5%88%86%E6%9E%90/:0:0","tags":["pandas","数据处理"],"title":"pandas数据描述性分析","uri":"/posts/pandas%E6%95%B0%E6%8D%AE%E6%8F%8F%E8%BF%B0%E6%80%A7%E5%88%86%E6%9E%90/"},{"categories":["数据库"],"content":"因为作业的原因我需要使用MySQL5.7来存储数据，但是在AUR上只能安装mariadb。我也尝试通过MySQL官网打包的通用安装包安装，但是一直失败，于是只能通过docker来运行MySQL了。 ","date":"2021-03-20","objectID":"/posts/python%E8%BF%9E%E6%8E%A5docker%E4%B8%AD%E8%BF%90%E8%A1%8Cmysql/:0:0","tags":["docker","python","mysql"],"title":"python连接docker中运行mysql","uri":"/posts/python%E8%BF%9E%E6%8E%A5docker%E4%B8%AD%E8%BF%90%E8%A1%8Cmysql/"},{"categories":["数据库"],"content":"安装docker 安装 sudo pacman -S docker 启动 systenctl start docker 将当前用户添加到docker用户组 sudo gpasswd -a pujic docker 退出当前用户重新登录 运行hello-world docker run hello-world ","date":"2021-03-20","objectID":"/posts/python%E8%BF%9E%E6%8E%A5docker%E4%B8%AD%E8%BF%90%E8%A1%8Cmysql/:1:0","tags":["docker","python","mysql"],"title":"python连接docker中运行mysql","uri":"/posts/python%E8%BF%9E%E6%8E%A5docker%E4%B8%AD%E8%BF%90%E8%A1%8Cmysql/"},{"categories":["数据库"],"content":"创建MySQL容器 拉取MySQL5.7镜像（速度太慢可以配置阿里云镜像地址） docker pull mysql:5.7 创建MySQL配置文件 sudo mkdir -p /opt/mysql/{conf, data} sudo nvim /opt/mysql/conf/my.cnf my.cnf添加如下内容 [mysqld] user=mysql character-set-server=utf8 default_authentication_plugin=mysql_native_password [client] default-character-set=utf8 [mysql] default-character-set=utf8 创建MySQL5.7容器 docker run -d -p 3306:3306 --privileged=true -v /opt/mysql/conf/my.cnf:/etc/mysql/my.cnf -v /opt/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=p --name mysql_container mysql:5.7 参数说明 参数 解释 -p 端口映射，冒号前面的端口是向外暴漏的端口 –privileged=true 容器内的root拥有真正root权限，否则容器内root只是外部普通用户权限 -v 文件和文件夹映射 -e MYSQL_ROOT_PASSWORD 设置root用户的密码 –name mysql_container 容器的名称，可以直接通过容器名称关闭启动容器 ","date":"2021-03-20","objectID":"/posts/python%E8%BF%9E%E6%8E%A5docker%E4%B8%AD%E8%BF%90%E8%A1%8Cmysql/:2:0","tags":["docker","python","mysql"],"title":"python连接docker中运行mysql","uri":"/posts/python%E8%BF%9E%E6%8E%A5docker%E4%B8%AD%E8%BF%90%E8%A1%8Cmysql/"},{"categories":["数据库"],"content":"配置MySQL 进入容器内的命令行 docker exec -t mysql_container bash 登录MySQL mysql -u root -p 创建MySQL用户 CREATE USER 'pujic'@'%' IDENTIFIED BY 'a'; 对用户进行授权 GRANT ALL ON *.* TO 'pujic'@'%'; ","date":"2021-03-20","objectID":"/posts/python%E8%BF%9E%E6%8E%A5docker%E4%B8%AD%E8%BF%90%E8%A1%8Cmysql/:3:0","tags":["docker","python","mysql"],"title":"python连接docker中运行mysql","uri":"/posts/python%E8%BF%9E%E6%8E%A5docker%E4%B8%AD%E8%BF%90%E8%A1%8Cmysql/"},{"categories":["数据库"],"content":"Python连接测试 import pymysql conn = pymysql.Connection(host='localhost', port=3306, user='pujic', password='a') cursor = conn.sursor() cursor.execute('SELECT VERSION()') print(cursor,fetchone()) 成功输出版本信息说明已经配置完成 ","date":"2021-03-20","objectID":"/posts/python%E8%BF%9E%E6%8E%A5docker%E4%B8%AD%E8%BF%90%E8%A1%8Cmysql/:4:0","tags":["docker","python","mysql"],"title":"python连接docker中运行mysql","uri":"/posts/python%E8%BF%9E%E6%8E%A5docker%E4%B8%AD%E8%BF%90%E8%A1%8Cmysql/"},{"categories":["数据处理"],"content":"在数据处理的时候经常会遇到时间序列数据，笔者有幸在4次竞赛6次集训中遇到过7次时序型数据，其中竞赛中有2次需要对数据进行聚合，集训中有2次需要对数据进行聚合。 通常我们拿到的数据都是存储为了excel文件或者csv文件，pandas在读取数据时如果没有指定数据类型，读取出来的非数值型数据都将是object类型（也就是字符串，和Python中数据类有一些区别） 通过DataFrame的info方法查看数据的基本信息 import pandas as pd data = pd.read_excel(\"./file.xlsx\") data.info() 如果读入的数据中有2000-01-01、2000-01-01 12:00、2000-01这样的数据肯定会被定义为object类型。 ","date":"2021-03-19","objectID":"/posts/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/:0:0","tags":["pandas","数据处理"],"title":"时间序列数据处理","uri":"/posts/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"},{"categories":["数据处理"],"content":"聚合 有下面一段csv数据，需要计算出每个月的销售额和销量 date,国庆,双十一,双十二,元旦,销售额,库存,平均售价,标价,折扣,新潮程度,销量 2018-10-01,1,0,0,0,2060.0,630,89.56521606445312,98.75,0.9069895148277283,0,23 2018-10-02,1,0,0,0,1498.75,617,88.1617660522461,98.75,0.8927773833274841,0,17 2018-10-03,1,0,0,0,1367.5,600,91.16666412353516,98.75,0.9232067465782166,0,15 2018-10-04,1,0,0,0,1062.5,591,88.54166412353516,98.75,0.8966244459152222,0,12 2018-10-05,1,0,0,0,730.0,583,91.25,98.75,0.9240506291389465,0,8 2018-10-06,1,0,0,0,1332.5,572,88.83333587646484,98.75,0.8995780944824219,0,15 2018-10-07,1,0,0,0,728.75,590,91.09375,98.75,0.922468364238739,0,8 2018-11-11,0,1,0,0,843.75,482,84.375,98.75,0.8544303774833679,0,10 2018-12-12,0,0,1,0,501.25,367,71.60713958740234,98.75,0.7251355648040771,0,7 2018-12-30,0,0,0,1,520.0,354,86.66666412353516,98.75,0.8776370882987976,0,6 首先读入数据，然后检查一下数据类型 import pandas as pd df = pd.read_csv(\"./data.csv\") df.head() df.info() 此时可以看到date字段并不是datetime类型，因此需要将date字段转为datetime类型，需要使用pandas中的to_datetime函数 df[\"date\"] = pd.to_datetime(df[\"date\"]) df['date'] 此时date字段已经转换为了datetime类型，但是此时我们想像pandas中操作时间序列数据那样操作这组数据还需要一些设置，pandas中时间序列数据的索引是时间 df.index = df[\"date\"] del df[\"date\"] 那么此时这个DataFrame就是一组时间序列数据了 使用resample方法对数据进行重采样，和groupby有一点像 df.resample(\"M\").sum() 计算出来的结果中，索引已经变为了这个月的最后一天 每个月的销售额和销量已经计算出来了，尽管其他数据聚合的方法有些不对，平均售价、标价、应该取均值。库存应该取这个月最后一天的数据。新潮程度可以取均值也可以不要。节假日应该是这个月有节假日就是1，没有就是0，也可以求和，看这一个月有几天是节假日。当然其他字段如何操作应该具体看需求。 def last_data(temp): return temp[-1] def is_festival(temp): if 1 in temp.values: return 1 return 0 df.resample(\"M\").agg({ \"国庆\": is_festival, \"双十一\": is_festival, \"双十二\": is_festival, \"元旦\": is_festival, \"库存\": last_data, \"平均售价\": \"mean\", \"标价\": \"mean\", \"折扣\": \"mean\", \"销售额\": \"sum\", \"销量\": \"sum\" }) 常见的时间频率 字段 说明 Y 年 Q 季度 M 月 W 周 D 天 H 小时 更多重采样方法可以阅读官方文档：https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html ","date":"2021-03-19","objectID":"/posts/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/:1:0","tags":["pandas","数据处理"],"title":"时间序列数据处理","uri":"/posts/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"},{"categories":["数据处理"],"content":"计算时间差 有下面一组数据 title,brand,buy_time,km,speedbox,displacement,es_price,new_price,city,year_type,level,suv,horsepower,fuel,length,width,height,drive 大众 宝来 2014款 1.6L 自动时尚型,一汽-大众,2014-08,3.82,自动,1.6L,8.0,13.0,北京,2014款,紧凑型,0,105马力,汽油,4523,1775,1467,前置前驱 福特 福睿斯 2015款 1.5L 自动时尚型,长安福特,2015-12,2.35,自动,1.5L,7.8,13.0,北京,2015款,紧凑型,0,113马力,汽油,4587,1825,1490,前置前驱 大众 速腾 2012款 1.6L 自动舒适型,一汽-大众,2012-05,6.67,自动,1.6L,7.0,16.4,北京,2012款,紧凑型,0,105马力,汽油,4644,1778,1482,前置前驱 奔驰C级 2011款 C 200 CGI 时尚型,北京奔驰,2013-01,11.83,自动,1.8T,15.0,42.1,北京,2011款,中型,0,184马力,汽油,4591,1770,1444,前置后驱 大众 帕萨特 2013款 2.0TSI DSG御尊版,上汽大众,2013-11,8.95,自动,2.0T,13.0,27.7,北京,2013款,中型,0,200马力,汽油,4870,1834,1472,前置前驱 哈弗H6 2011款 2.0L 手动两驱精英型,长城汽车,2012-11,5.17,手动,2.0L,4.5,11.5,北京,2011款,紧凑型,1,133马力,汽油,4640,1825,1690,前置前驱 起亚K3 2013款 1.6L 自动GLS,东风悦达起亚,2014-07,6.58,自动,1.6L,7.0,13.5,北京,2013款,紧凑型,0,128马力,汽油,4600,1780,1445,前置前驱 奔驰C级 2011款 C 200 CGI 时尚型,北京奔驰,2012-08,7.33,自动,1.8T,16.0,42.1,北京,2011款,中型,0,184马力,汽油,4591,1770,1444,前置后驱 奔驰E级 2015款 改款 E 260 L,北京奔驰,2016-01,10.36,自动,2.0T,32.0,53.0,北京,2015款,中大型,0,211马力,汽油,5024,1854,1477,前置后驱 本田 缤智 2016款 1.8L CVT两驱先锋型,广汽本田,2016-08,0.87,自动,1.8L,12.5,16.7,北京,2016款,小型,1,136马力,汽油,4294,1772,1605,前置前驱 我需要计算每款汽车到今年已经买了多少年 df = pd.read_csv(\"ershouche.csv\") df[\"buy_time\"] = pd.to_datetime(df[\"buy_time\"]) df[\"buy_time\"] 因为这个时间是我们需要拿来计算的，整组数据也不需要我们聚合，所以不用将索引设置为时间，pandas中已经将时间格式化了，自动补上了天 首先我们要获取到每条数据对应的年份 df[\"buy_time\"].dt.year 我们可以通过时间序列类型的dt方法获取到年月日 然后需要获取当前的年份，当然我们可以直接使用2021，但是为了程序在任何年份都够准确就需要调用函数来完成 import datetime this_year = datetime.datetime.now().year 然后直接做差就行，因为得到的年份都是整数类型 this_year - df[\"buy_time\"].dt.year 如果想要计算到现在买了多少天该怎么做呢？日期直接相减即可 datetime.datetime.now() - df[\"buy_time\"] 得到的天数并不能直接拿来分析，需要转换为数值型 day_diff = datetime.datetime.now() - df[\"buy_time\"] day_diff.map(lambda x: x.days) 但是时间天数差似乎不太准确，因为原数据中只给到了月份，直接用当前天数计算月份似乎不太准确，用年份差乘以12再加上当前的月份差即可 now = datetime.datetime.now() (now.year - df[\"buy_time\"].dt.year) * 12 + (now.month - df[\"buy_time\"].dt.month) 其他的在pandas中格式日期时间、时间序列插值等等笔者都遇到的场景不多，笔者遇到需要格式化日期时间的场景个人认为是当时在处理数据的时候没有操作好，如果笔者后续遇到其他在实际数据处理场景中用很方便的方法再进行补充 ","date":"2021-03-19","objectID":"/posts/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/:2:0","tags":["pandas","数据处理"],"title":"时间序列数据处理","uri":"/posts/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"},{"categories":["数据处理"],"content":"数据分组聚合在数据分析中是经常出现的一个问题，分组之后往往伴随着聚合，但偶尔分组也会被用来做数据筛选，但是通过分组做数据筛选的方法肯定不是很好的筛选方法 ","date":"2021-03-13","objectID":"/posts/pandas%E6%95%B0%E6%8D%AE%E8%81%9A%E5%90%88/:0:0","tags":["pandas","数据处理"],"title":"pandas数据分组聚合","uri":"/posts/pandas%E6%95%B0%E6%8D%AE%E8%81%9A%E5%90%88/"},{"categories":["数据处理"],"content":"groupby分组 在pandas中我们通常操作的都是DataFrame和Series进行数据处理，但是大量的场景都是在DataFrame下进行的，不过pandas中不管是DataFrame还是Series都是有聚合方法的我们可以通过groupby方法进行分组 import pandas as pd df = pd.DataFrame({ \"A\": [\"k1\", \"k2\", \"k1\", \"k3\", \"k2\"], \"B\": [1, 2, 3, 4, 5], \"C\": [6, 7, 8, 9, 10] }) 在上面的数据中A列是每组数据的编号，需要通过A列对数据进行分组。也就是说此时需要将编号相同的数据放到一起，如果使用筛选的方法进行，那么要操作三次，并且还需要事先知道有哪些不同的编号。 这样的问题经常出现在数据分析当中，我们拿到的数据通常并不是只有一个分组，可能外面有一个大类，大类中又包含小类。比如说大类是水果、饮料干粮都是大类，水果中又包含需要削皮的水果和不需要削皮的水果已经可以削皮也可以不削皮就吃的水果，需要削皮的水果有芒果、火龙果等等 当然这是将问题放到了生活当中，我们由此完全可以推断当我们遇到真实数据分析问题的时候问题的复杂性，当然在模型建立的过程这些数据处理问题通常只是会耗费一些时间就能解决。 回到问题中，下面的代码使用过数据筛选的方法将k1、k2、k3筛选出来 group = [] for key in df[\"A\"].value_counts().index: group.append(df[df[\"A\"]==key]) 这种方法无可厚非的增强了程序的通用性，但是操作list无疑会将耗时提升 因此我们肯定需要借助pandas中的数据结构 group = df.groupby(\"A\") 此时得到的group就是一个pandas中的DataFrameGroupBy对象，有这个对象其实我们已经可以进行下一步的处理了，但是这个对象中包含的东西很有意思 for item in group: print(item) group遍历出来的每一个对象都是一个tuple，每一个tuple中包含两个元素，第一个元素就是这个组的分组依据，也就是key，第二个元素就是分组得到的结果，其实到这里从产生的结果来看就很有意思了，在cookbook一书中的第一章节就有关于dict分组的内容，如果想对pandas有更深入的理解可以通过cookbook中的相关内容感受一下 ","date":"2021-03-13","objectID":"/posts/pandas%E6%95%B0%E6%8D%AE%E8%81%9A%E5%90%88/:1:0","tags":["pandas","数据处理"],"title":"pandas数据分组聚合","uri":"/posts/pandas%E6%95%B0%E6%8D%AE%E8%81%9A%E5%90%88/"},{"categories":["数据处理"],"content":"聚合 通过上面的程序得到的分组结果肯定是需要通过聚合函数运算的，比如求和、均值 group.mean() group.sum() 注意：此时聚合后的表格的index是A列，因此如果需要存储的时候需要保留index，其他合并等操作需要注意index 当前的聚合方法已经能够处理一些基本数据，但是在进行数据分析的时候往往数据形式并不是单一的，有一些连续性性的数值变量我们可能需要进行求和或者平均，如果遇到分变量的时候就平均（具体方法看案例），两种类型的数值变量在聚合的时候使用的聚合方法可能不会相同，因此这个时候我们需要根据字段指定聚合函数 import pandas as pd df2 = pd.DataFrame({ \"A\": [\"k1\", \"k2\", \"k1\", \"k3\", \"k2\"], \"B\": [1, 2, 3, 4, 5], \"C\": [6, 7, 8, 9, 10], \"D\": [1, 2, 1, 3, 2] }) 现在的表格中有一个分类字段D，A字段中同一个类别的字段现在需要求和，但是为了保证类别值不变D字段需要求均值，这时候需要使用agg方法 df2.groupby(\"A\").agg({\"B\": \"sum\", \"C\": \"sum\", \"D\": \"mean\"}) 使用agg方法可以使用多个聚合函数 df2.groupby(\"A\").agg({\"B\": [\"mean\", \"sum\"], \"C\": [\"mean\", \"sum\"], \"D\": \"mean\"}) 如果聚合函数是我们自定义的，使用的时候不需要加引号 ","date":"2021-03-13","objectID":"/posts/pandas%E6%95%B0%E6%8D%AE%E8%81%9A%E5%90%88/:2:0","tags":["pandas","数据处理"],"title":"pandas数据分组聚合","uri":"/posts/pandas%E6%95%B0%E6%8D%AE%E8%81%9A%E5%90%88/"},{"categories":["数据处理"],"content":"当我们在做数据分析或者数据挖掘的时候难免会遇到需要合并多个表格的情况，在pandas中要连接表格的姿势非常之多，合适的场景下先择合适的方案将加快我们数据处理的过程。 以下案例均为实际数据处理场景中遇到的问题简化而来。 注意：在所有的数据处理文章中，我并不会讲解函数的所有参数，也不会深入讲解函数的原理，如果想了解该函数更详细的内容建议阅读官方文档 ","date":"2021-03-09","objectID":"/posts/pandas%E8%BF%9E%E6%8E%A5%E8%A1%A8%E6%A0%BC/:0:0","tags":["数据处理"],"title":"pandas合并表格与索引设置","uri":"/posts/pandas%E8%BF%9E%E6%8E%A5%E8%A1%A8%E6%A0%BC/"},{"categories":["数据处理"],"content":"merge 合并维度相同的表 假设有如下两个表格df1和df2，其中A列表示不同的学生，B列和C列表示改学生对应的科目的成绩，现在需要将两个表格进行合并得到每位学生的平均成绩。 import pandas as pd df1 = pd.DataFrame({ \"A\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"], \"B\": [56, 18, 67, 98, 66, 82] }) df2 = pd.DataFrame({ \"A\": [\"f\", \"d\", \"e\", \"c\", \"a\", \"b\"], \"C\": [78, 47, 90, 81, 46, 79] }) df1、df2数据如下 因为这个问题不只是一个简答的堆叠或绑定，这是一个合并排序的过程，所以需要使用pandas中的merge函数 pd.merge(df1, df2, on=\"A\").mean(axis=1) 计算结果如下 结算结果明显是一个Series，但是通过缩影来找值实在是不方便，我希望可以直接出现人名对应着成绩的数据形式 先将结果的值赋给一个变量 s = pd.merge(df1, df2, on=\"A\").mean(axis=1) 然后设置索引的值 s.index = df1['A'] 那么我还想给s设置一个name因为可能还需要和其他表的数据进行合并进行计算 s.name = \"平均成绩\" merge函数有一个参数how，指的是合并(连接)的方式：inner(内连接)，left(左外连接)，right(右外连接)，outer(全外连接)，默认为inner。 inner只返回两个表中连接字段相等的行 left返回包括左表中的所有记录和右表中连接字段相等的行 outer和inner相反，right和left相反 合并维度不同的表 如果我们需要计算B、C科目的平均成绩，但是如果C科目成绩小于60的同学计算平均成绩的时候不考虑C科目的平均成绩。这样的情况下我们最好是不要先进行表的合并操作，虽然可以先将表合并后再按C科目的成绩进行计算，但是这样会多出很多的Python代码，因为pandas是C写的Python库，所以在操作数据的时候能不用Python中的语法就不要使用，我们可以先对df中的数据进行筛选，然后合并后计算的工作pandas自己就帮我们完成了 import pandas as pd df1 = pd.DataFrame({ \"A\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"], \"B\": [56, 18, 67, 98, 66, 82] }) df2 = pd.DataFrame({ \"A\": [\"f\", \"d\", \"e\", \"c\", \"a\", \"b\"], \"C\": [78, 47, 90, 81, 46, 79] }) df2 = df2[df2[\"C\"]\u003e60] df1、df2数据如下 pd.merge(df1, df2, on=\"A\") 连接结果如下 得到的结果维度丢失了，这种情况下直接使用merge是有问题的，因为merge默认连接方式是inner，所以只能返回A字段相等的行，所以需要使用left pd.merge(df1, df2, on=\"A\", how=\"left\") 连接结果如下 然后我们就可以直接求均值了，因为pandas够聪明，在计算均值的时候如果有值，那么这个值就不会被带入计算 pd.merge(df1, df2, on=\"A\", how=\"left\").mean(axis=1) 计算结果如下图 以多个列为基准进行合并 以多个列为基准进行合并，那么只需要将merge函数的on参数值指定为列表 import pandas as pd df1 = pd.DataFrame({ \"A\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"], \"B\": [97, 98, 99, 100, 101, 102], \"C\": [56, 18, 67, 98, 66, 82] }) df2 = pd.DataFrame({ \"A\": [\"f\", \"d\", \"e\", \"c\", \"a\", \"d\"], \"B\": [102, 122, 101, 99, 97, 100], \"D\": [78, 47, 90, 81, 46, 79] }) 以上面的表格为例，要合并df1和df2，并且要df1和df2中的A、B列能够对上，其实就是一个合并筛选数据的操作，因此合并方式默认为inner就可以 pd.merge(df1, df2, on=[\"A\", \"B\"]) ","date":"2021-03-09","objectID":"/posts/pandas%E8%BF%9E%E6%8E%A5%E8%A1%A8%E6%A0%BC/:1:0","tags":["数据处理"],"title":"pandas合并表格与索引设置","uri":"/posts/pandas%E8%BF%9E%E6%8E%A5%E8%A1%A8%E6%A0%BC/"},{"categories":["数据处理"],"content":"concat 纵向合并 在https://mp.weixin.qq.com/s/A76oeBfjuUCNuVhQgAzSgA一文中就涉及了表格的合并操作，这种合并相当于堆叠，concat函数中要连接的表格需要放到序列中 改变一下数据，现在是想要像df1中补充数据 import pandas as pd df1 = pd.DataFrame({ \"A\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"], \"B\": [56, 18, 67, 98, 66, 82] }) df2 = pd.DataFrame({ \"A\": [\"g\", \"h\", \"i\", \"j\", \"k\", \"l\"], \"B\": [78, 47, 90, 81, 46, 79] }) 这时候需要将这两个表进行合并，很显然是进行上下合并的，这时候我们就需要使用concat函数进行合并 pd.concat([df1, df2]) concat函数默认是在纵向进行合并，但是合并之后发现有一个问题，index的顺序是不对的，在我们纵向合并表格的时候并不会有什么影响，但是这个问题在一些情况下是需要解决的，关于索引的问题后面会讲解。 横向合并 有时候我们需要按索引横向合并两个表格的数据 import pandas as pd df1 = pd.DataFrame({ \"A\": [1, 2, 3, 4, 5, 6], \"B\": [1, 2, 3, 4, 5, 6] }) df2 = pd.DataFrame({ \"C\": [1, 2, 3, 4, 5, 6], \"D\": [1, 2, 3, 4, 5, 6] }) pd.concat([df1, df2], axis=1) 此时是在索引相同的情况，如果索引不同呢？有时候需要合并的数据是我们筛选出来的时候，不同的表格的索引大概率就是不同的 import pandas as pd df1 = pd.DataFrame({ \"A\": [1, 2, 3, 4, 5, 6], \"B\": [1, 2, 3, 4, 5, 6] }) df2 = pd.DataFrame( { \"C\": [1, 2, 3, 4, 5, 6], \"D\": [1, 2, 3, 4, 5, 6] }, index=range(1, 7) ) 再次用concat合并表格 pd.concat([df1, df2], axis=1) 那么这时候表格就错位了，此时我们可以通过重置索引的方式解决 赋值index属性 df2.index = range(df2.shape[0]) reset_index方法重置索引。reset_index方法有一个drop参数，表示是否删除原索引，默认为False，如果是False那么原来的索引将会并入DataFrame中 df2.reset_index() df2.reset_index(drop=True) 上面重置索引的两种方法已经能够解决绝绝大部分问题，至于其他设置索引的方法就不介绍了，笔者常用方法就这两种 注意：concat用来合并DataFrame和Series都是可以的，而且合并对象中的元素并不是只能两个，这里就不过多讲解 ","date":"2021-03-09","objectID":"/posts/pandas%E8%BF%9E%E6%8E%A5%E8%A1%A8%E6%A0%BC/:2:0","tags":["数据处理"],"title":"pandas合并表格与索引设置","uri":"/posts/pandas%E8%BF%9E%E6%8E%A5%E8%A1%A8%E6%A0%BC/"},{"categories":["数据处理"],"content":"append append的用法就比较简单了，大多数情况下是用来像原有的表中追加数据的，这个追加只能是纵向的追加，因为DataFrame的结构定位是类似于关系型数据库的纵向表格（当然有的时候也会横向进行操作） 使用concat中追加成绩的案例 import pandas as pd df1 = pd.DataFrame({ \"A\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"], \"B\": [56, 18, 67, 98, 66, 82] }) df2 = pd.DataFrame({ \"A\": [\"g\", \"h\", \"i\", \"j\", \"k\", \"l\"], \"B\": [78, 47, 90, 81, 46, 79] }) df1.append(df2) 如果要重置索引，可以直接设置ignore_index参数的值为True df1.append(df2, ignore_index=True) ","date":"2021-03-09","objectID":"/posts/pandas%E8%BF%9E%E6%8E%A5%E8%A1%A8%E6%A0%BC/:3:0","tags":["数据处理"],"title":"pandas合并表格与索引设置","uri":"/posts/pandas%E8%BF%9E%E6%8E%A5%E8%A1%A8%E6%A0%BC/"},{"categories":["数据处理"],"content":"总结 merge：需要按照某列或者多列进行合并的时候使用 concat：需要简单粗暴的进行合并的时候使用（最常用） append：只适用于追加数据的时候使用（不常用，大多数情况还是使用concat） ","date":"2021-03-09","objectID":"/posts/pandas%E8%BF%9E%E6%8E%A5%E8%A1%A8%E6%A0%BC/:4:0","tags":["数据处理"],"title":"pandas合并表格与索引设置","uri":"/posts/pandas%E8%BF%9E%E6%8E%A5%E8%A1%A8%E6%A0%BC/"},{"categories":["数据处理"],"content":"在参加某比赛的时候遇到过给出的训练数据集大小达到近9个G的csv文件，但是这9个G的数据中的数据不一定都是我需要的，我需要对文件进行筛选。这样的情况需要将数据都读进内存中，如果内存够大，那么完全没问题（我尝试过至少要32G），这样一次性全量读取数据集并且操纵这个数据集对电脑的要求是比较高的，那么这时候我们可以使用分块读取数据。 ","date":"2021-03-09","objectID":"/posts/pandas%E8%AF%BB%E5%8F%96%E5%A4%A7%E6%96%87%E4%BB%B6%E7%AD%9B%E9%80%89%E6%95%B0%E6%8D%AE/:0:0","tags":["pandas","数据处理"],"title":"pandas读取大文件筛选数据","uri":"/posts/pandas%E8%AF%BB%E5%8F%96%E5%A4%A7%E6%96%87%E4%BB%B6%E7%AD%9B%E9%80%89%E6%95%B0%E6%8D%AE/"},{"categories":["数据处理"],"content":"python中分块读取文件 数据分块读取或许在python中很陌生，因为绝大多数人读取数据都是采用的read方法或readline方法 file = open(file=\"filename\", mode=\"r\", encoding=\"utf8\") text = file.read() print(text) file.close() file = open(file=\"filename\", mode=\"r\", encoding=\"utf8\") while line:=file.readline(): print(line) file.close() 到这里我们的问题基本就解决了，可以按行读取9个G的csv然后对文件进行筛选，但是这样的效率及其低下，可以试想一下9个G的csv有多少行，python的速度是很慢的，因此我们一次读取应该读取更多行。 如果我需要读取一个1TB的文件，这个文件只有一行，但是这个文件比较特殊，每个段落都是以\u003c--!--\u003e作为的分隔符，我要将每个段落读取出来然后存储到数据库中 这时候readline就不能解决问题了，这时候我们需要使用read方法，也许是python通常都是被当作脚本语言，所以很少有人会去发掘里面的一些问题，其实在其他语言中，如Go、Java、C++在学习文件读取的时候都会讲按字节读取，但是python基本不会，在python中我如果要读取一个文件的前5个字符（如果要读取前5个字节那么读取方式就为rb），那么只需要给read方法指定size就行 现在假设这1TB的文件内容如下，文件名为source qwertyuiop\u003c--!--\u003easdfghjkl\u003c--!--\u003ezxcvbnm file = open(file=\"./source\", mode=\"r\", encoding=\"utf8\") t = file.read(5) print(t) file.close() 这样就读取了前5个字符那么当我们指定了size之后那么这个文件对象中就会存在一个文件位置的类似于指针的对象，我们可以将这个文件对象理解为一个生成器，就像readline一样，只不过我们每次读取这个文件时有4行变为了我们指定的字符数。 file = open(file=\"./source\", mode=\"r\", encoding=\"utf8\") while t := file.read(3): print(t) 那么我们就可以一点一点的读取文件，将读取到的内容拼接起来，如在拼接的字符串中发现了\u003c--!--\u003e那么就做一个切片将字符串进行切割 secret = \"\u003c--!--\u003e\" file = open(file=\"./source\", mode=\"r\", encoding=\"utf8\") buffer = \"\" while chunk := file.read(5): buffer += chunk postion = buffer.find(secret) if postion \u003e= 0: print(buffer[:postion]) buffer = buffer[postion+len(secret)] print(buffer) file.close() 至于最后的输出是因为我们的文件读到最后之后如果没有分隔符的话那么最后的字符是不会在循环中被输出的。当然我们也可以将判断是否读到文件末尾的逻辑移到循环中，那么在循环中就能输出了。 那么到这里我们的问题基本就已经讲完了，我们可以将这个循环封装为一个生成器函数，那么这样我们操作的空间将变得更大。 def seek(file: IO, secret: str) -\u003e Iterator: buffer = \"\" while chunk := file.read(5): buffer += chunk postion = buffer.find(secret) if postion \u003e= 0: yield buffer[:postion] buffer = buffer[postion+len(secret)] yield buffer ","date":"2021-03-09","objectID":"/posts/pandas%E8%AF%BB%E5%8F%96%E5%A4%A7%E6%96%87%E4%BB%B6%E7%AD%9B%E9%80%89%E6%95%B0%E6%8D%AE/:1:0","tags":["pandas","数据处理"],"title":"pandas读取大文件筛选数据","uri":"/posts/pandas%E8%AF%BB%E5%8F%96%E5%A4%A7%E6%96%87%E4%BB%B6%E7%AD%9B%E9%80%89%E6%95%B0%E6%8D%AE/"},{"categories":["数据处理"],"content":"pandas分块读取文件 在pandas中读取表类文件的时候有一个参数chunksize，只要指定了这个参数的数值，那么得到的结果就不是一个DataFrame对象，而是一个TextFileReader，这个对象是一个生成器对象 import pandas as pd reader = pd.read_csv('训练用数据.csv', chunksize=1000) for chunk in reader: print(chunk) 上面的程序中指定的chunksize=1000表示1次读取1000行，那么第二次读的时候就是从1001行开始读取。 那么我们就能将读取出的内容做一个筛选，并且由于每次只读取了100行，这样占用的计算机内存将会被控制得比较小。 这里我就简化我的工作过程，我需要的目标数据是存放在另外一个csv中的某一类列里，但是我就生成一个列表来进行筛选 import pandas as pd reader = pd.read_csv('训练用数据.csv', chunksize=1000) df = pd.DataFrame() t = list(range(1, 2000, 7)) for chunk in reader: chunk = chunk[chunk[\"小区编号\"].isin(t)] df = pd.concat([df, chunk], axis=0) 那么这里小区编号正好是我生成的t中的数值 ","date":"2021-03-09","objectID":"/posts/pandas%E8%AF%BB%E5%8F%96%E5%A4%A7%E6%96%87%E4%BB%B6%E7%AD%9B%E9%80%89%E6%95%B0%E6%8D%AE/:2:0","tags":["pandas","数据处理"],"title":"pandas读取大文件筛选数据","uri":"/posts/pandas%E8%AF%BB%E5%8F%96%E5%A4%A7%E6%96%87%E4%BB%B6%E7%AD%9B%E9%80%89%E6%95%B0%E6%8D%AE/"},{"categories":["Go"],"content":" gRPC是可以在任何环境中运行的现代开源高性能RPC框架。 它可以通过可插拔的支持来有效地连接数据中心内和跨数据中心的服务，以实现负载平衡，跟踪，运行状况检查和身份验证。 它也适用于分布式计算的最后一英里，以将设备，移动应用程序和浏览器连接到后端服务。 ","date":"2021-02-28","objectID":"/posts/grpc%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8/:0:0","tags":["rpc","Go"],"title":"grpc框架使用","uri":"/posts/grpc%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8/"},{"categories":["Go"],"content":"Protocol Buffers Protocol Buffer是一种支持多平台、多语言、可扩展的的数据序列化机制，相较于XML来说，protobuf更小更快更简单，支持自定义的数据结构，用protobu编译器生成特定语言的源代码，如C++、Java、Python，目前protoBuf对主流的编程语言都提供了支持,非常方便的进行序列化和反序列化。 安装 下载编译器https://github.com/protocolbuffers/protobuf/releases 然后将编译器添加到环境变量中 定义传输数据 syntax = \"proto3\"; package messages; message DataRequest { string id = 1; } message DataPesponse { string data = 1; } service Greeter { rpc SayHello (DataRequest) returns (DataPesponse); } 编译proto文件","date":"2021-02-28","objectID":"/posts/grpc%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8/:1:0","tags":["rpc","Go"],"title":"grpc框架使用","uri":"/posts/grpc%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8/"},{"categories":["Go"],"content":"在传统的Web服务中，通常是一个服务器完成一套或多套部署方案，但是随着用户的快速增长，一个服务器上的服务要完成的工作量越来越大而且也越来越复杂，就算使用多线程进行处理，但是因为业务逻辑的复杂性导致线程进行等待的时间变得很长，那么这时候就需要对业务进行拆分，将复杂的业务拆分为一个一个的小业务。比如大多数用户对网站首页发起请求的时候，如果所有业务逻辑都在一个服务上，不考虑其他页面的业务逻辑，就单纯请求首页的压力都是巨大的，而且一旦业务复杂，想要在后端实现并发难度是十分大的，但是如果我们将一个个业务进行解耦出来，那么业务逻辑将变得简单简单清晰，同时相对应的一些服务将能够容易的并发起来 随着微服务架构的兴起，RPC的应用越来越广泛 什么是RPC 在分布式计算，远程过程调用（英语：Remote Procedure Call，缩写为 RPC）是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一个地址空间（通常为一个开放网络的一台计算机）的子程序，而程序员就像调用本地程序一样，无需额外地为这个交互作用编程（无需关注细节）。RPC是一种服务器-客户端（Client/Server）模式，经典实现是一个通过发送请求-接受回应进行信息交互的系统，与rpc相对应的并不是传统的web服务，而是本地调用（如本地函数调用）。 将本地调用转换为远程过程调用将会面临各种问题 因为RPC是计算机通信协议，所以可以使用任何语言编写服务端或者客户端，只要协议能够对上，那么客户端就能请求到相应的内容。 ","date":"2021-02-26","objectID":"/posts/go-rpc/:0:0","tags":["rpc","Go"],"title":"go-rpc","uri":"/posts/go-rpc/"},{"categories":["Go"],"content":"本地过程调用 定义一个函数，然后在main函数中直接调用 func add(a, b int) int { total := a + b return rotal } func main() { total = add(1, 2) fmt.Plintln(total) } 调用过程： 将1和2压入add函数的栈 进入add函数，从栈中取出1和2的值分别赋值给a和b 执行a+b将结果赋值给局部变量total并压栈 将栈中的值取出来赋值给total ","date":"2021-02-26","objectID":"/posts/go-rpc/:1:0","tags":["rpc","Go"],"title":"go-rpc","uri":"/posts/go-rpc/"},{"categories":["Go"],"content":"远程调用过程面临的问题 在远程调用时，我们需要执行的函数体是在远程的机器上的，也就是说，函数在另一个进程中执行 Call的id映射：首先客户端需要告诉服务器，需要调用的函数，这里函数和进程ID存在一个映射，客户端远程调用时，需要查一下函数，找到对应的ID，然后执行函数的代码 序列化和反序列化：客户端需要把本地参数传给远程函数，本地调用的过程中，直接压栈即可，但是在远程调用过程中不在同一个内存里，无法直接传递函数的参数，因此需要客户端把参数转换成字节流，传给服务端，然后服务端将字节流转换成自身能读取的格式，是一个序列化和反序列化的过程。 网络传输：远程调用往往是用在网络上，客户端和服务端时通过网络链接的，所有的数据都需要通过网络传输，因此就需要有一个网络传输层。网络传输层需要把Call id和序列化后的参数字节流传给服务端，然后再把序列化后的调用结果传回客户端。只要完成这两者都可以作为传输层使用。 ","date":"2021-02-26","objectID":"/posts/go-rpc/:2:0","tags":["rpc","Go"],"title":"go-rpc","uri":"/posts/go-rpc/"},{"categories":["Go"],"content":"rpc开发的要素 rpc开发的四大要素 RPC技术在架构设计上有四部分组成，分别是：客户端、客户端存根、服务端、服务端存根 **客户端(Client)：**服务调用发起方，也称为服务消费者。 **客户端存根(Client Stub)：**该程序运行在客户端所在的计算机机器上，主要用来存储要调用的服务器的地址，另外，该程序还负责将客户端请求远端服务器程序的数据信息打包成数据包，通过网络发送给服务端Stub程序；其次，还要接收服务端Stub程序发送的调用结果数据包，并解析返回给客户端。 **服务端(Server)：**远端的计算机机器上运行的程序，其中有客户端要调用的方法。 **服务端存根(Server Stub)：**接收客户Stub程序通过网络发送的请求消息数据包，并调用服务端中真正的程序功能方法，完成功能调用；其次，将服务端执行调用的结果进行数据处理打包发送给客户端Stub程序。 如果我们想要在网络中的任意两台计算机上实现远程调用过程，要解决很多问题，如： 两台物理机器在网络中要建立稳定可靠的通信连接。 两台服务器的通信协议的定义问题，即两台服务器上的程序如何识别对方的请求和返回结果。也就是说两台计算机必须都能够识别对方发来的信息，并且能够识别出其中的请求含义和返回含义，然后才能进行处理。这其实就是通信协议所要完成的工作 在上述图中，通过1-10的步骤图解的形式，说明了rpc每一步的调用过程。具体描述为： 1、客户端想要发起一个远程过程调用，首先通过调用本地客户端Stub程序的方式调用想要使用的功能方法名； 2、客户端Stub程序接收到了客户端的功能调用请求，将客户端请求调用的方法名，携带的参数等信息做序列化操作，并打包成数据包。 3、客户端Stub查找到远程服务器程序的IP地址，调用Socket通信协议，通过网络发送给服务端。 4、服务端Stub程序接收到客户端发送的数据包信息，并通过约定好的协议将数据进行反序列化，得到请求的方法名和请求参数等信息。 5、服务端Stub程序准备相关数据，调用本地Server对应的功能方法进行，并传入相应的参数，进行业务处理。 6、服务端程序根据已有业务逻辑执行调用过程，待业务执行结束，将执行结果返回给服务端Stub程序。 7、服务端Stub程序将程序调用结果按照约定的协议进行序列化。 8、服务端通过网络将数据发送回客户端。 9、客户端Stub程序接收到服务端Stub发送的返回数据，**对数据进行反序列化操作，**并将调用返回的数据传递给客户端请求发起者。 10、客户端请求发起者得到调用结果。 Go实现一个简单的rpc server端 type HelloService struct{} func (s *HelloService) Hello(request string, reply *string) error { *reply = \"Hello \" + request return nil } func main() { // 创建一个server listener, err := net.Listen(\"tcp\", \":9988\") if err != nil { panic(err) } //注册服务 err = rpc.RegisterName(\"HelloService\", \u0026HelloService{}) if err != nil { panic(err) } //启动服务 conn, err := listener.Accept() if err != nil { panic(err) } rpc.ServeConn(conn) } 上面的程序中完成了一个简单的server端，首先建立一个tcp服务。然后通过go语言中自带的rpc库完成服务的注册，在服务注册的时候传递一个服务名，这个服务名可以是任意名字，但是在开发时一般使用提供服务的结构体的名字，第二个参数就是一个结构体的实例化对象，这个对象和这个服务明就进行了一个捆绑。最后tcp服务进行监听，将收到的连接交给rpc来处理。 client端 func main() { // 拨号 conn, err := rpc.Dial(\"tcp\", \":9988\") if err != nil { panic(err) } reply := new(string) // 调用 err = conn.Call(\"HelloService.Hello\", \"pujic\", reply) if err != nil { panic(err) } fmt.Printf(\"%s\", *reply) } 上面的程序中首先通过rpc创建了一个client对象。然后通过client对象进行拨号，Call方法接收三个参数，第一个就是要调用的服务名和方法，第二个参数就是server端中收到的request参数，第三个参数就是server端收到的reply参数。最后拿到的结果在reply中，所以reply需要传递指针。 上面的服务有一个问题，连接只能建立一次，因此需要使用循环来一只监听 type HelloService struct{} func (s *HelloService) Hello(request string, reply *string) error { *reply = \"Hello \" + request return nil } func main() { // 创建一个server listener, err := net.Listen(\"tcp\", \":9988\") if err != nil { panic(err) } //注册服务 err = rpc.RegisterName(\"HelloService\", \u0026HelloService{}) if err != nil { panic(err) } //启动服务 for { conn, err := listener.Accept() if err != nil { panic(err) } go rpc.ServeConn(conn) } } 这样修改以后这个server就具备了处理并发的能力 ","date":"2021-02-26","objectID":"/posts/go-rpc/:3:0","tags":["rpc","Go"],"title":"go-rpc","uri":"/posts/go-rpc/"},{"categories":["Go"],"content":"修改序列化和反序列化协议 在上面的程序中，序列化和反序列化的协议使用的是Gob编码，这个编码是Go语言独有的，如果其他语言想要调用，就需要相应的语言完成Gob编码，这就让问题变得很复杂了，因此我们需要更换编码，现在绝大多数的语言都是支持JSON编码协议的，所以可以将上面的server编码协议换为JSON type HelloService struct{} func (s *HelloService) Hello(request string, reply *string) error { *reply = \"Hello \" + request return nil } func main() { // 创建一个server listener, err := net.Listen(\"tcp\", \":9988\") if err != nil { panic(err) } // 服务注册 err = rpc.RegisterName(\"HelloService\", \u0026HelloService{}) if err != nil { panic(err) } // 启动服务 for { conn, err := listener.Accept() if err != nil { panic(err) } go rpc.ServeCodec(jsonrpc.NewServerCodec(conn)) } } server端就不适用默认的ServeConn了，而是使用带有编解码能力的ServeCodec，然后使用jsonrpc进行编解码 客户端的修改就会稍微复杂一点，因为编解码的方式换了，原来直接使用rpc拨号是因为需要使用rpc完成编解码，但是现在通过jsonrpc完成编解码，所以不需要使用rpc进行拨号，直接使用net进行拨号。（这里说的rpc是go中rpc这个包） func main() { conn, err := net.Dial(\"tcp\", \":9988\") if err != nil { panic(err) } reply := new(string) // client := rpc.NewClientWithCodec(jsonrpc.NewClientCodec(conn)) err = client.Call(\"HelloService.Hello\", \"pujic\", reply) if err != nil { panic(err) } fmt.Printf(\"%s\", *reply) } 那么既然使用了JSON的协议，不妨验证一番别的语言是否能够调用，在调用之前我们需要先知道客户端到底发送了什么给服务端， 进入jsonrpc.NewClientCodec的源码 func NewClientCodec(conn io.ReadWriteCloser) rpc.ClientCodec { return \u0026clientCodec{ dec: json.NewDecoder(conn), enc: json.NewEncoder(conn), c: conn, pending: make(map[uint64]string), } } 这个方法返回的是一个clientCodec对象，再进入这个clientCodec对象 type clientCodec struct { dec *json.Decoder // for reading JSON values enc *json.Encoder // for writing JSON values c io.Closer // temporary work space req clientRequest resp clientResponse // JSON-RPC responses include the request id but not the request method. // Package rpc expects both. // We save the request method in pending when sending a request // and then look it up by request ID when filling out the rpc Response. mutex sync.Mutex // protects pending pending map[uint64]string // map request id to method name } 与协议相关的内容我们不需要关系，看到req和resp，这两个对象用来保存请求传输的数据和返回的数据，这里我们只需要关注req，因为resp程序会帮我们处理，要在其他语言中调用，关键就是这个req，查看clientRequest type clientRequest struct { Method string `json:\"method\"` Params [1]interface{} `json:\"params\"` Id uint64 `json:\"id\"` } 这是一个结构体，这个Method就是需要调用的方法，Params是参数，这个Id并不是Call Id 它只是一个编号， ","date":"2021-02-26","objectID":"/posts/go-rpc/:3:1","tags":["rpc","Go"],"title":"go-rpc","uri":"/posts/go-rpc/"},{"categories":["数据库"],"content":"前言 不管是在Windows下还是在Linux，我需要看MySQL中的数据的时候我总是会情不自禁的在终端中连接MySQL，然后写SQL语句查看表，或者做在里面实验一些SQL语句。 我偶尔也会用一些MySQL的可视化工具，比如HeidiSQL 它会把所有的查询语句显示在下方，这样其实还是比较方便学习的，虽然Navicat也有回显SQL语句的功能，但是没有HeidiSQL方便一些，当然HeidiSQL甚至能从Windows应用商店中下载。 我看到网上有人说在mysql.cnf文件中添加auto-rehash参数，但是可能是我操作不当，没有成功，当然也无所谓，有更好的解决方案。 ","date":"2021-02-02","objectID":"/posts/mycli%E6%8F%92%E4%BB%B6/:1:0","tags":["MySQL"],"title":"mycli 插件","uri":"/posts/mycli%E6%8F%92%E4%BB%B6/"},{"categories":["数据库"],"content":"mycli 这是一个Python写的功能强大的MySQL命令行补全插件，并且是免费开源的。 mycli项目地址：https://github.com/dbcli/mycli 补全体验和用vim写代码时很像，通过tab键就能补全 ","date":"2021-02-02","objectID":"/posts/mycli%E6%8F%92%E4%BB%B6/:1:1","tags":["MySQL"],"title":"mycli 插件","uri":"/posts/mycli%E6%8F%92%E4%BB%B6/"},{"categories":["数据库"],"content":"安装 因为是Python写的，所以在各个平台下都可以通过pip安装 pip3 install mycli ","date":"2021-02-02","objectID":"/posts/mycli%E6%8F%92%E4%BB%B6/:1:2","tags":["MySQL"],"title":"mycli 插件","uri":"/posts/mycli%E6%8F%92%E4%BB%B6/"},{"categories":["数据库"],"content":"连接 如果是当前系统用户名和MySQL用户名相同且只需要使用某个数据库那么可以直接连接 mycli database_name 提示输入密码就能进入 使用参数连接，参数和MySQL连接的参数相同 mycli -u username -h 127.0.0.1 -P 3307 database_name 参数说明如下 Options: -h, --host TEXT Host address of the database. -P, --port INTEGER Port number to use for connection. Honors $MYSQL_TCP_PORT. -u, --user TEXT User name to connect to the database. -S, --socket TEXT The socket file to use for connection. -p, --password TEXT Password to connect to the database. --pass TEXT Password to connect to the database. --ssh-user TEXT User name to connect to ssh server. --ssh-host TEXT Host name to connect to ssh server. --ssh-port INTEGER Port to connect to ssh server. --ssh-password TEXT Password to connect to ssh server. --ssh-key-filename TEXT Private key filename (identify file) for the ssh connection. --ssh-config-path TEXT Path to ssh configuration. --ssh-config-host TEXT Host for ssh server in ssh configurations (requires paramiko). --ssl-ca PATH CA file in PEM format. --ssl-capath TEXT CA directory. --ssl-cert PATH X509 cert in PEM format. --ssl-key PATH X509 key in PEM format. --ssl-cipher TEXT SSL cipher to use. --ssl-verify-server-cert Verify server's \"Common Name\" in its cert against hostname used when connecting. This option is disabled by default. -V, --version Output mycli's version. -v, --verbose Verbose output. -D, --database TEXT Database to use. -d, --dsn TEXT Use DSN configured into the [alias_dsn] section of myclirc file. --list-dsn list of DSN configured into the [alias_dsn] section of myclirc file. --list-ssh-config list ssh configurations in the ssh config (requires paramiko). -R, --prompt TEXT Prompt format (Default: \"\\t \\u@\\h:\\d\u003e \"). -l, --logfile FILENAME Log every query and its results to a file. --defaults-group-suffix TEXT Read MySQL config groups with the specified suffix. --defaults-file PATH Only read MySQL options from the given file. --myclirc PATH Location of myclirc file. --auto-vertical-output Automatically switch to vertical output mode if the result is wider than the terminal width. -t, --table Display batch output in table format. --csv Display batch output in CSV format. --warn / --no-warn Warn before running a destructive query. --local-infile BOOLEAN Enable/disable LOAD DATA LOCAL INFILE. --login-path TEXT Read this path from the login file. -e, --execute TEXT Execute command and quit. --init-command TEXT SQL statement to execute after connecting. --charset TEXT Character set for MySQL session. --help Show this message and exit. dsn连接mycli mysql://[username[:password]@][host][:port][/database_name] mysql://username:password@host:3306/database_name ","date":"2021-02-02","objectID":"/posts/mycli%E6%8F%92%E4%BB%B6/:1:3","tags":["MySQL"],"title":"mycli 插件","uri":"/posts/mycli%E6%8F%92%E4%BB%B6/"},{"categories":["数据库"],"content":"界面 界面还是很好看的，支持大小写的补全，如果是小写就补全小写，如果是大写就补全大写，并且还有一让人一用就上瘾的功能，在上面的语句中我只打了DA，但是后面的TABASES也显示出来了只是颜色比较浅，这时候就可以用方向键右键直接补全，因为这个语句我写过了，mycli有了历史记录，所以会有一个提示，这个和oh-my-zsh的插件很像。 ","date":"2021-02-02","objectID":"/posts/mycli%E6%8F%92%E4%BB%B6/:1:4","tags":["MySQL"],"title":"mycli 插件","uri":"/posts/mycli%E6%8F%92%E4%BB%B6/"},{"categories":["数据库"],"content":"配置 在mycli项目中有一个myclirc的文件，经常玩Linux的朋友肯定会有想法，这个文件就是mycli的配置文件的模板文件，那么我们可以通过修改mycli的配置文件来做一些定制化，那么配置文件在哪呢？配置文件在用户目录下，文件名为.myclirc，也就是~/.myclirc，Windows用户通常是在C:\\Users\\xxx下 修改SQL语法高亮颜色，可以参照https://www.mycli.net/syntax进行修改，比如我要将颜色改为perldoc，需要修改配置文件中syntax_style的值 +syntax_style = defualt -syntax_style = perldoc 再次打开mycli颜色就改变了 注意：在Windows下一些颜色无法正常显示 修改提示符，mycli的默认提示符是\u003e符号，感觉不是很好看，需要修改配置文件中prompt 的值 +prompt = ‘\\t \\u@\\h:\\d\u003e -prompt = ‘\\t \\u@\\h:\\d▶ ``` 下面一行的prompt_continuation是mycli换行提示符，默认是-\u003e因为我字体的原因会将这个符号结合为一个箭头，如果不喜欢可以自行修改，再次打开后提示符就变了 修改登录提示信息，默认的登录提示信息会输出mysql version、mycli version、Chat、Mail、Home还有对贡献人员的感谢，我不想要这些提示，但是在配置文件中似乎没有找到可以修改的选项，那么就得从源码下手了，需要修改安装的这个包的源代码，如果需要修改需要找到Python安装位置，然后进入Lib\\site-packages\\mycli文件夹下，修改main.py 成功在561行找到需要修改的内容，我要更改Chat、Mail、Home以及对贡献人员的感谢，前面三个直接改就行，感谢的贡献人员名字获取调用了thanks_picker函数，传入的内容是文件地址，如果不要直接注释掉就行，当然也可以将这两个文件中的名字改为自己的名字，但是这样不太尊作者以及对这个项目有贡献的人员，因此我只是简单的注释掉了 保存退出！ 再次进入mycli可以看到提示信息已经改变了！ 使用vi模式，作为程序员或多或少听说过vim，但是vim不是编辑器吗？和mycli有什么关系，mycli的命令行默认使用的是emacs模式，也就是键盘键位绑定了emacs的键位，可能没用过emacs的人感觉不到，但是这个键位绑定是可以修改的，可以改为vi。如果改为了vi模式，那么就可以使用vi的快捷键，当按下esc键的时候就进入了NORMAL模式，在这个模式下可以使用h、l左右移动光标，k、j上下移动，使用w让光标移向前动一个单词，使用b让光标向后移动一个单词，当然还有很多的快速移动的光标的方法，mycli中能实现的快捷键不多，但是能满足正常的移动修改，需要修改配置文件中的key_bindings的值 +key_bindings = emacs -key_bindings = vi 因为mycli是开源的自己可以clone下来玩玩，做一些有意思的修改！ ","date":"2021-02-02","objectID":"/posts/mycli%E6%8F%92%E4%BB%B6/:1:5","tags":["MySQL"],"title":"mycli 插件","uri":"/posts/mycli%E6%8F%92%E4%BB%B6/"},{"categories":["数据库"],"content":"记录一下我比较喜欢的几个MongoDB可视化工具 ","date":"2021-02-01","objectID":"/posts/mongo%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7/:0:0","tags":["MongoDB"],"title":"Mongo可视化工具","uri":"/posts/mongo%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7/"},{"categories":["数据库"],"content":"前言 MongoDB的命令行查询界面实在是太丑了，虽然可以用printjson让输出美化一点，但是我感觉命令的确是繁琐。 上面的示例中的输出确实不好看，不是说不美观，而是影响到了我查看数据，如果要使用printjson那么就需要一条一条的传递给printjson ![]https://pujichun-blog-1301170821.cos.ap-chengdu.myqcloud.com/img/20220405231031.png) 需要先定义一个变量来保存查询结果返回的迭代器，然后一条一条的取，虽然好看了很多，但是写起来繁杂，而且不是很清楚，我并不是一个JavaScript程序员，我仅仅是想查看一下我抓取的数据，那么这时候就一定是需要使用可视化工具了。 ","date":"2021-02-01","objectID":"/posts/mongo%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7/:1:0","tags":["MongoDB"],"title":"Mongo可视化工具","uri":"/posts/mongo%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7/"},{"categories":["数据库"],"content":"robo3t robo3t是一个免费的MongoDB可视化工具，程序是用C++编写的，速度很快，能够满足我的需求 robo3t还支持编写查询语句，使用起来也较为灵活，但是功能较少，当我可能需要将一些数据导出的时候robo3t并没有这个功能，这时候我需要编写一个脚本，虽然并不会花费我多少时间，但是感觉还是有些不方便，但是并不妨碍robo3t速度之快。 robo3t的官网：https://robomongo.org/ 下载的时候要点击到Download页面中下载，Download Your Double Pack会一起下载Studio 3T，Studio 3T是付费的，功能要强大得多，但是我并没有用过，因为我不需要。 进入下载页面之后选择Download Robo 3T Only 选择对应得系统，然后随便填写一下信息，嗯随便填写！，然后点击Download for ...， 然后选择程序包，exe是安装引导程序，zip解压即用，我通常都是选择zip 解压之后就是这些文件，然后点击robo3t.exe就能打开程序，也可以创建一个快捷方式方便启动 打开软件之后点击Create就能创建连接，填写连接名称以及地址就可以了，如果要连接服务器需要选择SSH ","date":"2021-02-01","objectID":"/posts/mongo%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7/:2:0","tags":["MongoDB"],"title":"Mongo可视化工具","uri":"/posts/mongo%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7/"},{"categories":["数据库"],"content":"MongoDBCompass 这个软件可能很多人没有听说过，这是MongoDB官方推出的MongoDB可视化软件，这个软件是开源免费的。这个软件的推出，大有秀一下MongoDB功能强大的意味。 界面和功能相比于robo3t要更好一些，支持将集合导出为JSON或CSV，支持导入数据，支持将查询语句导出为Java、JavaScript、C#、Python的语法，甚至支持一些复杂的聚合操作。唯一的缺点就是启动速度比较慢，因为这个软件是JavaScript写的。 Compass需要到MongoDB官网下载：https://www.mongodb.com/ 这是下载地址：https://www.mongodb.com/try/download/compass 选择版本、平台就可以下载了 如果是zip包，解压后会有一个MongoDBCompass文件，双击就可以启动了 连接的时候需要按照一定的格式填写连接地址：mongodb://user:passward@host:port user：mongodb的用户名 password：用户密码 host：主机地址 port：mongodb端口 如果用户校验可以不用填user:password@ 比如我是本地连接而且不需要用户校验那么我在这里填写的就是mongodb://localhost:27017，如果端口不是27017那么自行更改就行。如果需要连接服务器的mongodb那么填上对应的字段信息即可。 ","date":"2021-02-01","objectID":"/posts/mongo%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7/:3:0","tags":["MongoDB"],"title":"Mongo可视化工具","uri":"/posts/mongo%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7/"},{"categories":["数据库"],"content":"adminMongo adminMongo是一个开源的MongoDB可视化的Web服务项目，程序是用JavaScript写的，在GitHub上有3.7k的star adminMongo项目地址：https://github.com/mrvautin/adminMongo 可以在releases中下载发行版，但是我更推荐将项目clone下来自己玩玩，虽然这个项目已经很久没有更新了，但是在功能上还是能满足基本的需求。 git clone --depth=1 https://github.com/mrvautin/adminMongo.git clone的时候加上深度这个参数会克隆得快一点。 我直接clone到了用户目录下，方便我能快速的通过命令行工具打开。 因为是JavaScript写的所以后端需要依赖node，并且拉取依赖也需要使用npm和node，如果没有node的话安装即可 因为adminMongo的依赖还是比较多，建议使用cnpm来拉取依赖 npm install -g cnpm --registry=https://registry.npm.taobao.org 如果安装失败也没关系，继续使用npm也是一样的 命令行工具进入到adminMongo的目录后拉取依赖 npm install cnpm install 上面的命令只需要执行一个，如果没有cnpm需要执行第一个，如果有cnpm两个都行，命令执行后就会拉取依赖，因为我已经拉取过了，所以显示的是全部依赖都安装了 依赖拉取完之后可以执行npm start 或node app就能启动了，可以看到监听的是1234这个端口，浏览器访问就行 第一次访问是连接列表 填写上连接信息之后点击Add connection 点击Connect就能连接了 整个界面还是十分的好看的，只是功能没有那么多，如果项目要添加新的连接可以点击右上角的Connections 如果想要更换端口需要自己配置一下config文件夹下的app.json文件，比如说将端口改为8899 然后重新启动adminMongo 这时候访问localhost:8899就能出现界面了，更多的配置可以参考README ","date":"2021-02-01","objectID":"/posts/mongo%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7/:4:0","tags":["MongoDB"],"title":"Mongo可视化工具","uri":"/posts/mongo%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7/"},{"categories":["Go"],"content":"文章迁移到了 Notion 中 https://nervous-petunia-ec3.notion.site/Goroutine-f74ce82e31bb46179cfd678e7f693504?pvs=4 ","date":"2021-01-11","objectID":"/posts/goroutine/:0:0","tags":["Go"],"title":"goroutine","uri":"/posts/goroutine/"},{"categories":["Python"],"content":"提到鸭子类型很多人第一反应都是“有一只鸟，走路像鸭子，叫起来像鸭子，那么它就是鸭子。”，这句话没有问题，描述的也的确是鸭子类型，但是在Python中鸭子类型到底是怎么样的呢？ Python是一门极度灵活且多变的语言，想要实现什么功能都很简单，当然我这里指的是单一功能，或单一问题，很多人学了一点Python就以为自己掌握了，但是却没有解决实际问题的能力，代码可维护性极差，代码通用性不强。 接着上面的鸭子类型，网上搜索Python鸭子类型，得出的的通常都是下面的程序 class Duck: def __init__(self, name: str): self.name = name def quack(self): print(\"ga~ga~\") class Bird: def __init__(self, name: str): self.name = name def quack(self): print(\"ga~ga~\") 这个的确是和鸭子类型的描述是一样的，但是读了这个代码能做什么吗，什么也不能做，稍微好一点的文章会有下面这样示例代码 class Duck: def __init__(self, name: str): self.name = name def quack(self): print(\"ga~ga~\") class Bird: def __init__(self, name: str): self.name = name def quack(self): print(\"ga~ga~\") def quack(duck): duck.quack() if __name__ == '__main__': d1 = Duck(\"d1\") b1 = Bird(\"b1\") quack(d1) quack(b1) 这里就是定义了一个接口实际业务上的接口quack，而不是程序中的interface，这个接口就负责调用传进来的对象的quack()方法，这样的程序勉强说了一点鸭子类型的应用，但是还是没有说清楚Python中的鸭子类型。 在看Python中真正的鸭子类型之前，先看一下Go语言中的鸭子类型。 type Duck interface { quack() } type duck struct { name string } type bird struct { name string } func (d duck) quack() { fmt.Println(\"ga~ga~\") } func (b bird) quack() { fmt.Println(\"jiu~jiu~\") } func quack(d Duck) { d.quack() } func main() { b := bird{name: \"b1\"} d := duck{name: \"d1\"} quack(b) quack(d) } 在Go语言中鸭子类型表现得就很明确了，有quack方法的struct就是Duck类型，而且可以发现其实结构体的quack方法中的内容可以是任意的，所以有一只鸟，走路像鸭子，叫起来像鸭子，那么它就是鸭子。描述的更应该是一种行为，也就是对象的方法，那么在Python中鸭子类型应该怎么实现呢？ 在Python中我们要限定一种类必须有某种方法，那么我们就需要使用抽象类型和抽象方法，首先定义一个抽象基类Duck，定义的这个抽象基类就像其他语言中的interface一样 import abc class Duck(metaclass=abc.ABCMeta): def __init__(self, name: str): self.name = name @abc.abstractmethod def quack(self): pass 只要继承了Duck这个抽象基类的类，都要实现这个quack方法 class Chicken(Duck): def quack(self): print(\"guo~guo~\") class Bird(Duck): def quack(self): print(\"qiu~qiu~\") 甚至因为定义了抽象基类，下面定义好的quack接口中的参数可以很明确的指定类型 import abc class Duck(metaclass=abc.ABCMeta): def __init__(self, name: str): self.name = name @abc.abstractmethod def quack(self): pass class Chicken(Duck): def quack(self): print(\"guo~guo~\") class Bird(Duck): def quack(self): print(\"qiu~qiu~\") def quack(d: Duck): d.quack() if __name__ == '__main__': d1 = Chicken(\"d1\") b1 = Bird(\"b1\") quack(d1) quack(b1) 那么到这里Python中的鸭子类型就差不多了，需要注意的是鸭子类型重要的是类的行为而不是类的内容，内容可以是千变万化的，但是行为的协议必须统一。 ","date":"2020-12-28","objectID":"/posts/%E9%B8%AD%E5%AD%90%E7%B1%BB%E5%9E%8B/:0:0","tags":["Python","Go"],"title":"鸭子类型","uri":"/posts/%E9%B8%AD%E5%AD%90%E7%B1%BB%E5%9E%8B/"},{"categories":["Python"],"content":"记录了我对cookbook的学习和理解。下方所有笔记均记载在jupyter notebook中， 由于直接渲染markdown效果不佳，所以直接上传到了Gitee和Github ","date":"2020-12-15","objectID":"/posts/cookbook%E7%AC%AC%E4%B8%80%E7%AB%A0-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/:0:0","tags":["Python"],"title":"cookbook第一章-数据结构和算法","uri":"/posts/cookbook%E7%AC%AC%E4%B8%80%E7%AB%A0-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/"},{"categories":["Python"],"content":"设计模式：对软件中普遍存在（反复出现）的各种问题，所提出的解决方案。每一个设计模式都系统地命名、解释和评价了面向对象系统中一个重要的和重复出现的设计。 面向对象三大特性： 封装 继承 多态 ","date":"2020-12-13","objectID":"/posts/python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/:0:0","tags":["Python","Python进阶"],"title":"Python设计模式","uri":"/posts/python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"categories":["Python"],"content":"创建性模式 ","date":"2020-12-13","objectID":"/posts/python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/:1:0","tags":["Python","Python进阶"],"title":"Python设计模式","uri":"/posts/python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"categories":["Python"],"content":"简单工厂模式 不直接向客户端暴露对象创建的实现细节，而是通过一个工厂类来负责创建产品类的实例 角色： 工厂角色（Creator） 抽象产品角色（Product） 具体产品角色（Concrete Product） \"\"\" 简单工厂模式 \"\"\" from abc import ABCMeta, abstractmethod class PayMent(metaclass=ABCMeta): \"\"\" abstract class 抽象产品角色 \"\"\" @abstractmethod def pay(self, money:int): pass class AliPay(PayMent): \"\"\" 具体产品角色 \"\"\" def pay(self, money:int): print(f\"通过支付宝支付了{money}元\") class WeChatPay(PayMent): \"\"\" 具体产品角色 \"\"\" def pay(self, money:int): print(f\"通过微信支付了{money}元\") class PaymentFactory(): \"\"\" 工厂角色 \"\"\" def create_payment(self, method:str) -\u003e PayMent: if method == \"alipay\": return AliPay() elif method == \"wechat\": return WeChatPay() else: raise TypeError(f\"No such payment named {method}\") if __name__ == \"__main__\": pf = PaymentFactory() pm = pf.create_payment(\"wechat\") pm.pay(100) 优点： 隐藏了对象创建的实现细节 客户端不需要修改代码 缺点 违反了单一职责原则，将几种创建逻辑放到一个工厂类中 当添加新产品是，需要修改工厂类代码，违反了开闭原则 ","date":"2020-12-13","objectID":"/posts/python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/:1:1","tags":["Python","Python进阶"],"title":"Python设计模式","uri":"/posts/python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"categories":["Python"],"content":"工厂方法模式 定义一个用于创建对象的接口，让子类决定实例化哪一个产品 角色： 抽象工厂角色（Creator） 具体工厂角色（Concrete Creator） 抽象产品角色（Product） 具体产品角色（Concrete Product） \"\"\" 工厂模式 \"\"\" from abc import ABCMeta, abstractmethod class PayMent(metaclass=ABCMeta): \"\"\" abstract class 抽象产品角色 \"\"\" @abstractmethod def pay(self, money: int): pass class AliPay(PayMent): \"\"\" 具体产品角色 \"\"\" def pay(self, money: int): print(f\"通过支付宝支付了{money}元\") class WeChatPay(PayMent): \"\"\" 具体产品角色 \"\"\" def pay(self, money: int): print(f\"通过微信支付了{money}元\") class PaymentFactory(metaclass=ABCMeta): \"\"\" 抽象工厂角色 \"\"\" @abstractmethod def create_payment(self): pass class AlipyFactory(PaymentFactory): \"\"\" 具体工厂角色 \"\"\" def create_payment(self): return AliPay() class WechatPayFactory(PaymentFactory): \"\"\" 具体工厂角色 \"\"\" def create_payment(self): return WeChatPay() if __name__ == \"__main__\": pf = WechatPayFactory() pm = pf.create_payment(\"wechat\") pm.pay(100) 优点： 每个具体产品都对应一个具体的工厂类，不需要修改代码 隐藏了对象创建的实现细节 缺点 每增加一个具体产品类，就必须增加一个相应的具体工厂类 ","date":"2020-12-13","objectID":"/posts/python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/:1:2","tags":["Python","Python进阶"],"title":"Python设计模式","uri":"/posts/python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"categories":["Python"],"content":"抽象工厂模式 定义一个工厂类接口，让工厂子类来创建一系列相关或相互依赖的对象 相比工厂方法模式，抽象工厂模式中的每一个具体工厂类都生产一套产品 角色： 抽象工厂角色（Creator） 具体工厂角色（Concrete Creator） 抽象产品角色（Product） 具体产品角色（Concrete Product） from abc import abstractmethod, ABCMeta # 抽象产品 class PhoneShell(metaclass=ABCMeta): @abstractmethod def show_shell(self): pass class CPU(metaclass=ABCMeta): @abstractmethod def show_cpu(self): pass class OS(metaclass=ABCMeta): @abstractmethod def show_os(self): pass # 抽象工厂 class PhoneFactory(metaclass=ABCMeta): @abstractmethod def make_shell(self): pass @abstractmethod def make_cpu(self): pass @abstractmethod def make_os(self): pass # 具体产品 class XiaoMiShell(PhoneShell): def show_shell(self): print(\"小米机壳\") class HuaweiShell(PhoneShell): def show_shell(self): print(\"华为机壳\") class AppleShell(PhoneShell): def show_shell(self): print(\"苹果机壳\") class SnapDragon(CPU): def show_cpu(self): print(\"骁龙CPU\") class MediaTek(CPU): def show_cpu(self): print(\"联发科CPU\") class AppleCPU(CPU): def show_cpu(self): print(\"联发科CPU\") class Android(OS): def show_os(self): print(\"安卓系统\") class IOS(OS): def show_os(self): print(\"IOS系统\") # 具体工厂 class AppleFactory(PhoneFactory): def make_cpu(self): return AppleCPU() def make_shell(self): return AppleShell() def make_os(self): return IOS() class XiaoMiFactory(PhoneFactory): def make_cpu(self): return SnapDragon() def make_shell(self): return XiaoMiShell() def make_os(self): return Android() # 客户端 class Phone: def __init__(self, cpu: CPU, os: OS, shell: PhoneShell): self.cpu = cpu self.os = os self.shell = shell def show_info(self): print(\"手机信息\") self.cpu.show_cpu() self.shell.show_shell() self.os.show_os() def make_phone(factory: PhoneFactory) -\u003e Phone: cpu = factory.make_cpu() os = factory.make_os() shell = factory.make_shell() return Phone(cpu, os, shell) if __name__ == '__main__': p1 = make_phone(XiaoMiFactory()) p1.show_info() 优点： 将客户端与类的具体实现相分离 每个工厂创建了一个完整的产品系列，使得易于交换产品系列 有利于产品的一致性 缺点 难以支持新种类的（抽象产品） ","date":"2020-12-13","objectID":"/posts/python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/:1:3","tags":["Python","Python进阶"],"title":"Python设计模式","uri":"/posts/python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"categories":["Python"],"content":"建造者模式 将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示 建造者模式与抽象工厂模式相似，也是用来创建复杂对象。主要区别是建造者模式着重一步步构建一个复杂对象，而抽象工厂模式着重于多个系列产品对象 角色 抽象建造者（Builder） 具体建造者（Concrete Builder） 指挥者（Director） 产品（Product） from abc import ABCMeta, abstractmethod # 产品 class Player: def __init__(self, face=None, body=None, arm=None, leg=None) -\u003e None: self.face = face self.body = body self.arm = arm self.leg = leg def __str__(self) -\u003e str: return f\"{self.face}, {self.body}, {self.arm}, {self.leg}\" # 抽象建造者 class PlayerBuilder(metaclass=ABCMeta): @abstractmethod def __init__(self) -\u003e None: self.player = Player() @abstractmethod def build_face(self): pass @abstractmethod def build_body(self): pass @abstractmethod def build_arm(self): pass @abstractmethod def build_leg(self): pass # 具体建造者 class CatBuild(PlayerBuilder): def __init__(self) -\u003e None: self.player = Player() def build_face(self): self.player.face = \"圆脸\" def build_body(self): self.player.body = \"苗条\" def build_arm(self): self.player.arm = \"小短手\" def build_leg(self): self.player.leg = \"小短腿\" class DogBuild(PlayerBuilder): def __init__(self) -\u003e None: self.player = Player() def build_face(self): self.player.face = \"傻脸\" def build_body(self): self.player.body = \"胖\" def build_arm(self): self.player.arm = \"小短手\" def build_leg(self): self.player.leg = \"小短腿\" # 指挥者控制组装顺序 class PlayerDirector: def build_player(self, builder: PlayerBuilder) -\u003e Player: builder.build_body() builder.build_face() builder.build_arm() builder.build_leg() return builder.player if __name__ == \"__main__\": builder = CatBuild() director = PlayerDirector() p = director.build_player(builder) print(p) 优点 隐藏了一个产品的内部结构何装配过程 将构造代码与表示代码分开 可以对构造过程进行更精细的控制 ","date":"2020-12-13","objectID":"/posts/python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/:1:4","tags":["Python","Python进阶"],"title":"Python设计模式","uri":"/posts/python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"categories":["Python"],"content":"单例模式 保证一个类只有一个实例，并提供一个访问它的全局访问点 角色： 单例（Singleton） from abc import ABCMeta, abstractmethod class Singleton: def __new__(cls, *args): if not hasattr(cls, \"_instance\"): cls._instance = super(Singleton, cls).__new__(cls) return cls._instance class MyClass(Singleton): def __init__(self, a) -\u003e None: self.a = a if __name__ == \"__main__\": m1 = MyClass(10) m2 = MyClass(20) print(m1.a) print(m2.a) print(id(m1)) print(id(m2)) 优点： 对唯一实例的受控访问 单例相当于全局变量，但防止了命名空间被污染 ","date":"2020-12-13","objectID":"/posts/python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/:1:5","tags":["Python","Python进阶"],"title":"Python设计模式","uri":"/posts/python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"categories":["Linux"],"content":"Ubuntu环境下操作 环境：Ubuntu 20.04.1 示例：hexo博客框架生成的静态文件 ","date":"2020-11-23","objectID":"/posts/nginx%E6%89%98%E7%AE%A1%E9%9D%99%E6%80%81%E6%96%87%E4%BB%B6/:0:0","tags":["Linux","nginx"],"title":"nginx托管静态文件","uri":"/posts/nginx%E6%89%98%E7%AE%A1%E9%9D%99%E6%80%81%E6%96%87%E4%BB%B6/"},{"categories":["Linux"],"content":"hexo框架生成静态文件并上传 hexo g 生成的静态文件在public文件夹中 将public文件夹打包为zip格式，上传到服务器，我这里放到的是用户目录下，我是root用户 如果静态文件中有中文名字，解压时需要指定编码 unzip -O GBK public.zip 然后检查public文件夹下是否还是public文件夹 ","date":"2020-11-23","objectID":"/posts/nginx%E6%89%98%E7%AE%A1%E9%9D%99%E6%80%81%E6%96%87%E4%BB%B6/:0:1","tags":["Linux","nginx"],"title":"nginx托管静态文件","uri":"/posts/nginx%E6%89%98%E7%AE%A1%E9%9D%99%E6%80%81%E6%96%87%E4%BB%B6/"},{"categories":["Linux"],"content":"安装nginx 安装openssl apt install openssl apt install libssl-dev wget http://nginx.org/download/nginx-1.18.0.tar.gz 解压文件夹，进入目录 ./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module make \u0026\u0026 make install ln -s /usr/local/nginx/sbin/nginx /usr/bin/nginx 启动nginx nginx ","date":"2020-11-23","objectID":"/posts/nginx%E6%89%98%E7%AE%A1%E9%9D%99%E6%80%81%E6%96%87%E4%BB%B6/:0:2","tags":["Linux","nginx"],"title":"nginx托管静态文件","uri":"/posts/nginx%E6%89%98%E7%AE%A1%E9%9D%99%E6%80%81%E6%96%87%E4%BB%B6/"},{"categories":["Linux"],"content":"配置nginx 修改配置文件 nvim /usr/local/nginx/conf/nginx.conf 将第一行用户修改为当前用户 -user nobody; +user root; 修改location中托管文件地址 -root /root/html; +root /root/public; 保存退出后nginx重新加载配置文件 nginx -s reload 然后访问公网ip就能看到页面了 ","date":"2020-11-23","objectID":"/posts/nginx%E6%89%98%E7%AE%A1%E9%9D%99%E6%80%81%E6%96%87%E4%BB%B6/:0:3","tags":["Linux","nginx"],"title":"nginx托管静态文件","uri":"/posts/nginx%E6%89%98%E7%AE%A1%E9%9D%99%E6%80%81%E6%96%87%E4%BB%B6/"},{"categories":["Python"],"content":"协程是属于编程语言中很高级而且很难的知识点 ","date":"2020-11-14","objectID":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/:0:0","tags":["异步IO"],"title":"协程和异步IO","uri":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/"},{"categories":["Python"],"content":"并发 并发是指一个时间段内，有几个程序在同一个cpu上运行，但是任意时刻只有一个程序在cpu上运行 ","date":"2020-11-14","objectID":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/:0:1","tags":["异步IO"],"title":"协程和异步IO","uri":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/"},{"categories":["Python"],"content":"并行 并行是指任意时刻点上，有多个程序同时运行在多个cpu上，并行的数量和cpu的数量是一样的 ","date":"2020-11-14","objectID":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/:0:2","tags":["异步IO"],"title":"协程和异步IO","uri":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/"},{"categories":["Python"],"content":"同步 同步是指代码调用I/O操作时，必须等待I/O造作完成时候才能返回的调用方式 ","date":"2020-11-14","objectID":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/:0:3","tags":["异步IO"],"title":"协程和异步IO","uri":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/"},{"categories":["Python"],"content":"异步 异步是指代码调用I/O操作时，不必等I/O操作完成就返回的调用方式 ","date":"2020-11-14","objectID":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/:0:4","tags":["异步IO"],"title":"协程和异步IO","uri":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/"},{"categories":["Python"],"content":"阻塞 阻塞是指调用函数时当前线程被挂起 ","date":"2020-11-14","objectID":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/:0:5","tags":["异步IO"],"title":"协程和异步IO","uri":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/"},{"categories":["Python"],"content":"非阻塞 非阻塞是指调用函数时当前线程不会被挂起，而是立即返回 ","date":"2020-11-14","objectID":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/:0:6","tags":["异步IO"],"title":"协程和异步IO","uri":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/"},{"categories":["Python"],"content":"Unix下五种I/O模型 阻塞式I/O 非阻塞式I/O 非阻塞式I/O调用后虽然立马返回了，但是却要去不停的询问数据是否准备好 I/O多路复用 select其实也是阻塞式的，但是它可以监听多个句柄，如果这时候同时发起了100个socket请求，那么只要有一个返回了就会返回调用 信号驱动式I/O 信号驱动式I/O就是数据报准备好之后发送一个可调用信号 异步I/O 异步IO省掉了将数据从内核复制到用户空间的时间 由于异步IO编码难度很高，所以现在比较成熟的框架都是使用的I/O多路复用 ","date":"2020-11-14","objectID":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/:1:0","tags":["异步IO"],"title":"协程和异步IO","uri":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/"},{"categories":["Python"],"content":"select select函数监视的文件描述符分3类，分别是writefds、readfds和exceptfds，调用后select函数会阻塞，直到有描述符就绪（有数据可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以通过遍历fdset，来找到就绪的描述符。 select目前机会在所有平台上支持，其良好的跨平台支持也是它的一个优点，select的一个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但是这样也会造成效率降低。 ","date":"2020-11-14","objectID":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/:1:1","tags":["异步IO"],"title":"协程和异步IO","uri":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/"},{"categories":["Python"],"content":"pull 不同于select使用三个位图来表示三个fdset的方式，poll使用一个pollfd的指针实现。 pollfd结构包含了要监视的event和发生的event ，不再使用select“参数-值”传递的方式。同时，pollfd并没有最大数量限制(但是数量过大后性能也是会下降)。和select函数一 样，poll返回后,需要轮询pollfd来获取就绪的描述符。 从上面看，select和poll都需要在返回后,通过遍历文件描述符来获取已经就绪的socket。事实上,同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降 ","date":"2020-11-14","objectID":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/:1:2","tags":["异步IO"],"title":"协程和异步IO","uri":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/"},{"categories":["Python"],"content":"epoll epoll是在Linux2.6内核中提出的,是之前的select和poll的增强版本。相对于select和poll来说, epoll更加灵活,没有描述符限制。epoll使用一 个文件描述符管理多个描述符,将用户关系的文件描述符的事件存放到内核的一个事件表中,这样在用户空间和内核空间的copy只需一次。在并发高但是连接活跃度不是很高的情况下select没有epoll好，但是在并发性不高但是连接很活跃时select比epoll好 ","date":"2020-11-14","objectID":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/:1:3","tags":["异步IO"],"title":"协程和异步IO","uri":"/posts/%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5io/"},{"categories":["Python"],"content":"Python中的多线程编程并不是真的慢 ","date":"2020-11-09","objectID":"/posts/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/:0:0","tags":["多线程","多进程"],"title":"Python多线程与多进程","uri":"/posts/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/"},{"categories":["Python"],"content":"Python中的GIL GIL: global interpreter lock即全局解释器锁 Python中的一个线程对应于C语言中的一个线程，无法将多个线程映射到多个CPU上 GIL使得同一时刻只有一个线程运行在一个CPU上执行字节码，只是多个线程之间的切换速度非常的快，所以我们感受不到线程之间的切换 但是当GIL释放的时候线程就不是安全的了 import threading total = 0 def add(): global total for i in range(1000000): total += 1 def desc(): global total for i in range(1000000): total -= 1 thread1 = threading.Thread(target=add) thread2 = threading.Thread(target=desc) thread1.start() thread2.start() thread1.join() thread2.join() print(total) 两次运行上面的程序，第一次结果是439838，第二次结果是-787195，说明开启的两个线程并没有一直占有CPU资源，而是进行了切换，所以很明显在程序运行的过程中GIL被释放了 GIL会根据执行的字节码行数以及时间片释放GIL，GIL在遇到IO操作的时候也会主动释放，因此Python在进行IO操作的时候多线程是很适用的 ","date":"2020-11-09","objectID":"/posts/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/:1:0","tags":["多线程","多进程"],"title":"Python多线程与多进程","uri":"/posts/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/"},{"categories":["Python"],"content":"Thread 在最开始的时候计算机能够调度的最小单元是进程，但是进程对计算机资源消耗非常大，后来就演变出了线程。 对于IO操作来说，多线程和多线程的差别不大 Python中有两种方式实现多线程 ","date":"2020-11-09","objectID":"/posts/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/:2:0","tags":["多线程","多进程"],"title":"Python多线程与多进程","uri":"/posts/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/"},{"categories":["Python"],"content":"通过Thread类实例化 import time import threading def get_detail_html(): print(\"get detail html started\") time.sleep(2) print(\"get deatil html end\") def get_list(): print(\"get list html started\") time.sleep(2) print(\"get list html end\") if __name__ == \"__main__\": thread1 = threading.Thread(target=get_list) thread2 = threading.Thread(target=get_detail_html) start_time = time.time() thread1.start() thread2.start() print(f\"time = {time.time()-start_time}\") 运行这个程序的时候结果如下 get list html startedget detail html startedtime = 0.000997304916381836 get deatil html end get list html end 上面的时间是负数，如果按照正常的理解逻辑两个线程并行的时候使用的时间应该是2秒，但是为什么时间接近0呢 其实在这两个线程之外还有另外一个主线程，线程之间可以并行，那么创建的两个线程在并行的时候那么主线程也是可以并行的，当主线程结束的时候两个线程还没结束 我们可以设置守护线程，当主线程结束的时候其他线程都结束 thread1.setDaemon(True) thread2.setDaemon(True) import time import threading def get_detail_html(): print(\"get detail html started\") time.sleep(4) print(\"get deatil html end\") def get_list(): print(\"get list html started\") time.sleep(2) print(\"get list html end\") if __name__ == \"__main__\": thread1 = threading.Thread(target=get_list) thread2 = threading.Thread(target=get_detail_html) # thread1.setDaemon(True) thread2.setDaemon(True) start_time = time.time() thread1.start() thread2.start() print(f\"time = {time.time()-start_time}\") 运行上面的程序 get list html startedget detail html started time = 0.0009975433349609375 get list html end 可以看到当主线程结束的时候thread2这个线程也强制结束了 但是我们想等待两个线程完成之后才输出时间，那么可以使用join()方法 import time import threading def get_detail_html(): print(\"get detail html started\") time.sleep(4) print(\"get deatil html end\") def get_list(): print(\"get list html started\") time.sleep(2) print(\"get list html end\") if __name__ == \"__main__\": thread1 = threading.Thread(target=get_list) thread2 = threading.Thread(target=get_detail_html) start_time = time.time() thread1.start() thread2.start() thread1.join() thread2.join() print(f\"time = {time.time()-start_time}\") 运行上面的程序发现最后的时间输出已经达到了我们想要的结果，而且程序运行的时间就是我们线程的最大时间，而不是累加 get list html startedget detail html started get list html end get deatil html end time = 4.008591175079346 ","date":"2020-11-09","objectID":"/posts/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/:2:1","tags":["多线程","多进程"],"title":"Python多线程与多进程","uri":"/posts/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/"},{"categories":["Python"],"content":"通过Thread继承类 通过Thread类继承，需要重载的是run方法而不是start方法 import time import threading class GetList(threading.Thread): def run(self) -\u003e None: print(\"get list html started\") time.sleep(2) print(\"get list html end\") class GetDetail(threading.Thread): def run(self) -\u003e None: print(\"get detail html started\") time.sleep(4) print(\"get deatil html end\") if __name__ == \"__main__\": thread1 = GetList() thread2 = GetDetail() start_time = time.time() thread1.start() thread2.start() thread1.join() thread2.join() print(f\"time = {time.time()-start_time}\") ","date":"2020-11-09","objectID":"/posts/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/:2:2","tags":["多线程","多进程"],"title":"Python多线程与多进程","uri":"/posts/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/"},{"categories":["Python"],"content":"线程之间的通信 在上面的程序中只是简单的演示了多线程的写法，但是上面的程序中并没有业务逻辑，当我线程与线程之间需要合作完成业务功能的时候就需要在线程之间进行通信 ","date":"2020-11-09","objectID":"/posts/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/:3:0","tags":["多线程","多进程"],"title":"Python多线程与多进程","uri":"/posts/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/"},{"categories":["Python"],"content":"共享变量 import time import threading detail_url_list = [] def get_detail_html(detail_url_list): while True: # 防止detail_url_list为空 if len(detail_url_list): url = detail_url_list.pop() print(\"get detail html started\") time.sleep(4) print(\"get deatil html end\") def get_list(detail_url_list): print(\"get list html started\") time.sleep(2) for i in range(20): detail_url_list.append(f\"url{i}\") print(\"get list html end\") if __name__ == \"__main__\": thread1 = threading.Thread(target=get_list, args=(detail_url_list,)) for i in range(10): detail_thread = threading.Thread(target=get_detail_html, args=(detail_url_list,)) detail_thread.start() thread1.start() 但是这样维护起来比较麻烦，所以我们可以专门建立共享变量的包，方便管理，但是调取的时候不能直接调用到变量这一级上，如果调用到变量一级，当共享变量被其他线程改变的时候可能就不清楚变量的情况。 也就是说，如果我们使用的是共享变量，那么应该是import到模块一级from package-name import module-name，不应该直接将共享变量调出来 ","date":"2020-11-09","objectID":"/posts/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/:3:1","tags":["多线程","多进程"],"title":"Python多线程与多进程","uri":"/posts/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/"},{"categories":["Python"],"content":"Queue 在上面的多线程中通过共享变量的方式进行通信是很不安全的，尤其像list这样的数据结够很容易被修改。就像文章开头的第一个例子一样，本来想让程序修改total后最后还是为0，但是total最终的结果并不为0，这就是线程不安全的表现。可能为了达到我们预期的同步顺序我们可以给线程加一把锁，但是程序的性能以及编写程序的效率将会大幅下降 使用Queue能简化线程之间的通信，而且不需要为线程加锁 from queue import Queue 而在Python中，Queue并不是通过封装list实现的，而是通过封装deque实现的，因为deque在字节码层面就已经实现了线程安全，所以Queue是线程安全的 Queue有两个方法很常用 put向队列中添加数据 默认是阻塞的，如果队列已经满了则需要等待队列小于最大长度时才能添加 如果设为非阻塞，则如果队列满了会马上抛出异常 get向队列中取出数据 默认是阻塞的，只有队列中有数据时才能取出 如果设为非阻塞，则如果队列中没有数据则会马上抛出异常 from queue import Queue import time import threading def get_detail_html(queue:Queue): while True: url = queue.get() print(\"get detail html started\") time.sleep(4) print(\"get deatil html end\") def get_list(queue:Queue): print(\"get list html started\") time.sleep(2) for i in range(20): queue.put(f\"url{i}\") print(\"get list html end\") if __name__ == \"__main__\": detail_url_queue = Queue(maxsize=1000) thread1 = threading.Thread(target=get_list, args=(detail_url_queue,)) for i in range(10): detail_thread = threading.Thread(target=get_detail_html, args=(detail_url_queue,)) detail_thread.start() thread1.start() 这样就实现了通过Queue来通信 上面的程序中明显是主线程会更快的运行完，如果想让主线程等待，就需要在detail_thread线程上使用join方法，但是也可以通过Queue来告诉主线程是否阻塞 Queue也可以使用join方法，但是Queue使用join方法时会一直阻塞，只有使用了Queue的task_done方法时Queue才会退出 ","date":"2020-11-09","objectID":"/posts/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/:4:0","tags":["多线程","多进程"],"title":"Python多线程与多进程","uri":"/posts/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/"},{"categories":["Python"],"content":"线程同步 线程同步是多线程不得不面对的一个问题，在解决线程如何同步之前，应该知道线程为什么会不同步 import dis def add(a): a += 1 def desc(a): a -= 1 dis.dis(add) dis.dis(desc) 查看字节码 4 0 LOAD_FAST 0 (a) 2 LOAD_CONST 1 (1) 4 INPLACE_ADD 6 STORE_FAST 0 (a) 8 LOAD_CONST 0 (None) 10 RETURN_VALUE 7 0 LOAD_FAST 0 (a) 2 LOAD_CONST 1 (1) 4 INPLACE_SUBTRACT 6 STORE_FAST 0 (a) 8 LOAD_CONST 0 (None) 10 RETURN_VALUE 可以看到我们的代码虽然是一行一行的读的，但是解释器在将代码转换为字节码后运行的过程中并不是一下就将a增加或减小了1，在这个过程中如果线程在这个时间片上满了，或者字节码行数到了一定限度就会切换出去。 ","date":"2020-11-09","objectID":"/posts/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/:5:0","tags":["多线程","多进程"],"title":"Python多线程与多进程","uri":"/posts/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/"},{"categories":["Go"],"content":"Go原生Web学习 Handler请求 Go语言的Web Server在接收到http请求的时候时候会创建一个goroutine去处理请求，而处理http请求是个很复杂的工作，而Go语言的http.DefaultServeMax会将对应的请求交给我们指定的handler，我们只需要关心如何创建Web Server以及编写对应的handler ","date":"2020-10-31","objectID":"/posts/go%E5%8E%9F%E7%94%9Fweb-handler%E8%AF%B7%E6%B1%82/:0:0","tags":["http","Go","Web"],"title":"Go原生Web-Handler请求","uri":"/posts/go%E5%8E%9F%E7%94%9Fweb-handler%E8%AF%B7%E6%B1%82/"},{"categories":["Go"],"content":"http.Handle ","date":"2020-10-31","objectID":"/posts/go%E5%8E%9F%E7%94%9Fweb-handler%E8%AF%B7%E6%B1%82/:1:0","tags":["http","Go","Web"],"title":"Go原生Web-Handler请求","uri":"/posts/go%E5%8E%9F%E7%94%9Fweb-handler%E8%AF%B7%E6%B1%82/"},{"categories":["Go"],"content":"创建Web Server http.ListenAndServer() 第一个参数是网络地址 如果为\"\"，那么就是所有网络接口的80端口 第二个参数是handler 如果为nil，那么就是DefaultServeMax DefaultServerMax是一个multiplexer func main() { http.ListenAndServe(\"localhost:8080\", nil) } 这样就创建了一个简单的Web Server ListenAndServer源码如下 func ListenAndServe(addr string, handler Handler) error { server := \u0026Server{Addr: addr, Handler: handler} return server.ListenAndServe() } 可以看到它其实是调用的Server这个结构体中的ListenAndServe()方法 因此我们可以通过调用Server这个结构体中的ListenAndServe()方法创建http服务 func (srv *Server) ListenAndServe() error { if srv.shuttingDown() { return ErrServerClosed } addr := srv.Addr if addr == \"\" { addr = \":http\" } ln, err := net.Listen(\"tcp\", addr) if err != nil { return err } return srv.Serve(ln) } http.Server是一个struct Addr字段表示网络地址 如果为\"\"，那么就是所有网络接口的80端口 handler字段 如果为nil，那么就是DefaultServeMax 那么刚才的写法就可以更改 func main() { server := http.Server{ Addr: \"localhost:8080\", Handler: nil, } server.ListenAndServe() } ","date":"2020-10-31","objectID":"/posts/go%E5%8E%9F%E7%94%9Fweb-handler%E8%AF%B7%E6%B1%82/:1:1","tags":["http","Go","Web"],"title":"Go原生Web-Handler请求","uri":"/posts/go%E5%8E%9F%E7%94%9Fweb-handler%E8%AF%B7%E6%B1%82/"},{"categories":["Go"],"content":"DefaultServeMax 它是一个multiplexer（多路复用器） 它也是一个handler 在DefaultServeMax是ServerMax这个struct的指针变量，ServerMax中也是实现了ServeHTTP方法，所以它也是一个handler ","date":"2020-10-31","objectID":"/posts/go%E5%8E%9F%E7%94%9Fweb-handler%E8%AF%B7%E6%B1%82/:1:2","tags":["http","Go","Web"],"title":"Go原生Web-Handler请求","uri":"/posts/go%E5%8E%9F%E7%94%9Fweb-handler%E8%AF%B7%E6%B1%82/"},{"categories":["Go"],"content":"Handler Handler是一个接口 Handler定义了一个方法ServeHTTP() ResponseWriter 指向Request这个struct的指针 type Handler interface { ServeHTTP(ResponseWriter, *Request) } 所以只要任何一个类型有ServeHTTP这个方法那么就是一个Handler 自定义一个Handler type myHandler struct{} func (m *myHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"Freedom And Discipline\")) } func main() { mh := myHandler{} server := http.Server{ Addr: \"localhost:8080\", Handler: \u0026mh, } server.ListenAndServe() } 但是运行程序后所有请求都被myHandler处理了，所以请求哪个路径都是返回的Freedom And Discipline 有没有办法能让某个路由或者指定的路由是由myHandler处理的，我们可以将myHandler注册到DefaultServeMax上 我们可以使用http.Handle将某个Handler注册到DefaultServeMax上，ServerMux struct也有一个Handle方法，其实我们在调用http.Handle方法时就是在调用ServeMax的Handle方法 func Handle(pattern string, handler Handler) { DefaultServeMux.Handle(pattern, handler) } 至于Handle方法是怎么样将对应路由交给对应Handler的不需要知道，只需要知道使用Handle方法后DefaultServeMux会将对应路由与对应Handler绑定 type myHandler struct{} func (m *myHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"Freedom And Discipline\")) } func main() { mh := myHandler{} server := http.Server{ Addr: \"localhost:8080\", Handler: nil, } http.Handle(\"/hello\", \u0026mh) server.ListenAndServe() } 因此就可以通过http.Handle进行路由绑定，将对应路由交给对应的Handler处理 ","date":"2020-10-31","objectID":"/posts/go%E5%8E%9F%E7%94%9Fweb-handler%E8%AF%B7%E6%B1%82/:1:3","tags":["http","Go","Web"],"title":"Go原生Web-Handler请求","uri":"/posts/go%E5%8E%9F%E7%94%9Fweb-handler%E8%AF%B7%E6%B1%82/"},{"categories":["Go"],"content":"http.HandleFunc 在前面创建的服务都是通过Handler来处理的，但是如果每次都要定义struct和它的方法就会很麻烦，并不是所有场景都需要去定义struct，因此我们可以使用Handler函数，Handler函数的行为与Handler的行为类似，签名也是一样的，接收两个参数，一个是http.ResponseWriter，一个是http.Request的指针。 HandlerFunc是一个函数类型，它有ServeHTTP方法所以它也是一个Handler类型 type HandlerFunc func(ResponseWriter, *Request) // ServeHTTP calls f(w, r). func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) { f(w, r) } 通过HandleFunc可以更快的的编写处理逻辑，HandleFunc它调用的是DefaultServeMux上的HandleFunc方法 func HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { DefaultServeMux.HandleFunc(pattern, handler) } func (mux *ServeMux) HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { if handler == nil { panic(\"http: nil handler\") } mux.Handle(pattern, HandlerFunc(handler)) } 可以看到DefaultServeMux上的HandleFunc方法其实还是调用的自身的Handle方法，但是调用Handle方法之前先将handler参数传递给了HandlerFunc，所以HandlerFunc相当于一个适配器，将具有ResponseWriter、 *Request两个参数的函数转换为HandlerFunc，然后在ServeHTTP方法中中实现相应的逻辑，这里就相当于类型转换，因为函数传递进来之前这个函数并不是一个Handler类型，它只是一个函数，它没有ServeHTTP方法。 func List(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"list\")) } func main() { server := http.Server{ Addr: \"localhost:8080\", Handler: nil, } http.HandleFunc(\"/home\", func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"home\")) }) http.HandleFunc(\"/list\", List) server.ListenAndServe() } 那么之前定义的Handler类型就可以简化，将这个函数交给HandlerFunc即可，将会极大的简化http.Handle函数的写法，通过http.HandlerFunc将函数转换为Handler类型 func Map(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"map\")) } func main() { server := http.Server{ Addr: \"localhost:8080\", Handler: nil, } http.Handle(\"/map\", http.HandlerFunc(Map)) server.ListenAndServe() } ","date":"2020-10-31","objectID":"/posts/go%E5%8E%9F%E7%94%9Fweb-handler%E8%AF%B7%E6%B1%82/:2:0","tags":["http","Go","Web"],"title":"Go原生Web-Handler请求","uri":"/posts/go%E5%8E%9F%E7%94%9Fweb-handler%E8%AF%B7%E6%B1%82/"},{"categories":["Python","网络爬虫"],"content":"异步高并发爬取必应壁纸下载链接，写入数据库后再异步下载存储图片 URL：https://bing.ioliu.cn/?p=1 必应壁纸只有一个User-Agent反爬虫 首先看一下同步程序，逻辑很简单 import requests from lxml import etree class BingCrawler(): def __init__(self): self.BASE_URL = \"https://bing.ioliu.cn\" self.TOTAL_PAGE = 100 self.headers = { \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\" } def spider(self): session = requests.Session() for index in range(1 ,self.TOTAL_PAGE+1): url = self.BASE_URL + f\"/?p={index}\" response = session.get(url=url, headers=self.headers) yield response.content.decode(\"utf8\") @staticmethod def parser(html): selector = etree.HTML(html) url_list = selector.xpath(\"//div[@class='item']//div[@class='options']/a[2]/@href\") return url_list def save_picture(self, url_list): session = requests.Session() for url in url_list: file_name = url.split(\"/\")[2][:-15] + \".jpg\" picture_url = self.BASE_URL + url response = session.get(picture_url, headers=self.headers) with open(file_name, \"wb\") as file: file.write(response.content) def run(self): html_gen = self.spider() for html in html_gen: url_list = self.parser(html) self.save_picture(url_list) if __name__ == '__main__': bing_spider = BingCrawler() bing_spider.run() 这里并没有使用数据库存储下载链接 先定义两个常量，这里只下载一百页的图片，导入aiohttp import aiohttp from aiohttp import ClientSession TOTAL_PAGE = 100 BASE_URL = \"https://bing.ioliu.cn\" 首先是下载html async def fetch(url: str, session: ClientSession): async with session.get(url=url, headers=self.headers) as response: if response.status == 200: html = await response.read() 下载html的程序逻辑十分简单，发起这个请求是一个需要等待的过程，然后读取数据也是一个需要等待的过程，但是为什么不都是async呢？ 因为我们发起一个请求结束之后需要断掉连接，但是这里是异步的请求，协程在不停的切换，我们不知道什么时候该断掉这个连接，所以需要使用上下文管理器来创建这个协程，因为这个请求是一个协程，所以我们需要在阻塞的时候将这个请求挂起，所以我们需要加一个async关键字来标注这是一个协程，如果不标注那么解释器是不知道这是协程的，当我们需要等待的返回结果就是一个协程，那么就需要使用await关键字进行标注。 然后是解析数据，解析数据在Python中使用lxml速度还是很快的，而且现在也并没有异步解析库，那么只能使用同步解析 这里我设置了一个status_code async def parser(html: bytes): element = etree.HTML(html.decode(\"utf-8\")) href_list = element.xpath('//div[@class=\"item\"]//div[@class=\"options\"]/a[2]/@href') items = [{\"url\": BASE_URL + href, \"status_code\": 0, \"url_id\": md5(href)} for href in href_list] 下面是MD5编码的程序 import hashlib def md5(string: str) -\u003e str: m = hashlib.md5() m.update(string.encode()) return m.hexdigest() ","date":"2020-10-28","objectID":"/posts/%E5%BC%82%E6%AD%A5%E7%88%AC%E5%8F%96%E5%BF%85%E5%BA%94%E5%A3%81%E7%BA%B8/:0:0","tags":["协程","Python","网络爬虫"],"title":"异步爬取必应壁纸","uri":"/posts/%E5%BC%82%E6%AD%A5%E7%88%AC%E5%8F%96%E5%BF%85%E5%BA%94%E5%A3%81%E7%BA%B8/"},{"categories":["Python"],"content":"编写多线程的程序是比较困难的，而且在多线程中调度程序任何时候都能中断线程，必须要使用保留锁区保护主线程，防止多步操作在执行的过程中中断，防止数据处于无效的状态。 而协程默认会做好全方位的保护，以防止中断。 对于协程来说，无需保留锁，多个线程之间同步操作，协程自生就会同步，这是协程本身的特性决定的。 ","date":"2020-10-27","objectID":"/posts/python%E5%8D%8F%E7%A8%8B/:0:0","tags":["Python","进阶","协程","aiohttp"],"title":"Python协程","uri":"/posts/python%E5%8D%8F%E7%A8%8B/"},{"categories":["Python"],"content":"切换任务 def foo(): print(\"run foo\") time.sleep(1) def bar(): print(\"run bar\") time.sleep(1) while True: foo() bar() 现在有两个函数foo和bar让他们处于一直运行的状态 这时候有一个问题，在这个同步程序中它们都等待了1秒，在IO任务中等待的时间可能更长，为了节省时间我想让程序执行foo函数开始等待的时候马上切换到bar执行，print语句肯定也是需要花费一点点时间的，尽管这点时间看起来微不足道，但是在bar等待结束之前foo肯定等待结束了，这时候我想让程序马上切换到foo 显然多线程已经不能帮我们完成这样的任务了，因为多线程是开启多个线程来执行同一个任务，但是这里是需要在同一个线程之间切换两个任务。 同时我们也可以发现多线程对性能的开销是更大的，因为同时启动了很多个线程，但是绝大部分线程可能都处于等待的状态，这个等待当处于网络IO或者是文件IO的时候是很慢的，尽管做一个数据查询时能同时并发1000个甚至是10000个线程去访问数据库查询数据，但是数据库在查询的过程中线程会一直陷入等待的状态，不管对性能和内存的开销都是十分巨大的。 这时候我们就应该使用协程来并发执行任务，当任务出现阻塞的时候就切换任务。 import asyncio async def foo(): while True: print(\"run foo\") await asyncio.sleep(1) async def bar(): while True: print(\"run bar\") await asyncio.sleep(1) loop = asyncio.get_event_loop() loop.run_until_complete(asyncio.gather(foo(), bar())) loop.close() 通过使用asyncio创建时间循环之后运行程序发现程序在执行的时候在foo与bar之间进行了切换，并没有重复等待 在实际的应用中，如需要下载一个网页，那么在等待数据返回的时候程序就会陷入等待中，并且如果是多个URL，那么就需要创建多个任务，那么在等待的过程中就需要进行多个任务之间的切换 import requests urls = [ \"https://game.gtimg.cn/images/lol/act/img/skin/big81000.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81001.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81002.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81003.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81004.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81005.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81006.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81007.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81008.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81009.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81018.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81019.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81020.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81021.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81022.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81023.jpg\", ] def fetch(filename:str, url:str) -\u003e bytes: response = requests.get(url=url) if response.status_code == 200: print(f\"{filename}下载完成\") return response.content print(f\"{filename}的状态码是: {response.status_code}\") return b'' def save_to_file(filename:str, source:bytes): with open(file=filename, mode=\"wb\") as f: f.write(source) print(f\"{filename}存储完成\") def main(): for url in urls: filename=url.split(\"/\")[-1] source = fetch(filename=filename, url=url) save_to_file(filename=filename, source=source) if __name__ == \"__main__\": main() 上面是一个下载图片的同步任务，那么我们需要将他改为异步并发执行，但是因为requests并不是一个异步，所以需要使用aiohttp，存储文件也需要使用异步文件存储模块 pip install aiohttp pip install aiofiles 首先创建一个ClientSession对象，用来发送请求，因为这个创建过程也是一个协程，所以我们需要使用async关键字标注，然后就是讲任务挂载到事件循环中，因为在这里我想要使用回调函数的的方式执行程序，所以使用了ensure_future，最后就是等待任务执行结束。 async def main(): async with aiohttp.ClientSession() as session: task = [asyncio.ensure_future(fetch(url=url, session=session)) for url in urls] await asyncio.wait(task) 然后是修改fetch函数，我们需要通过session去做请求，这个请求过程也是一个协程，所以也需要async关键字标注 从请求返回的相应中读取内容需要我们等待，所以要使用await关键字标注，内容读取出来之后我们不需要讲内容返回，直接调用save_to_file就行，但是存储文件也是一个需要等待的过程，所以也需要在前面加上await关键字标注 async def save_to_file(filename:str, source:bytes): async with aiofiles.open(file=filename, mode=\"wb\") as picture: await picture.write(source) print(f\"{filename}存储完成\") 最后是写入文件，因为我们可能需要同时打开多个文件，所以我们也需要使用async关键字对打开文件的过程进行标注，写入文件需要等待所以需要加上await关键字进行标注 async def save_to_file(filename:str, source:bytes): async with aiofiles.open(file=filename, mode=\"wb\") as picture: await picture.write(source) print(f\"{filename}存储完成\") 下面是完整的程序 import aiohttp import asyncio from aiohttp.client import ClientSession import aiofiles urls = [ \"https://game.gtimg.cn/images/lol/act/img/skin/big81000.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81001.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81002.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81003.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81004.jpg\", \"https://game.gtimg.cn/images/lol/act/img/skin/big81005.j","date":"2020-10-27","objectID":"/posts/python%E5%8D%8F%E7%A8%8B/:1:0","tags":["Python","进阶","协程","aiohttp"],"title":"Python协程","uri":"/posts/python%E5%8D%8F%E7%A8%8B/"},{"categories":["Go"],"content":"程序做调试其实是比较麻烦的，但是得益于jet brains的IDE让我们调试起来十分方便，但是如果程序比较复杂，那么我们调试起来还是很麻烦，其实我做调试更多的是去看这个变量中存了哪些值是什么而不是为了因为程序有bug而去找bug，当程序出错的时候我更想快速的知道到底是哪里出错了，而不是慢慢的去调试 ","date":"2020-10-26","objectID":"/posts/python-go%E6%B5%8B%E8%AF%95/:0:0","tags":["Go","Python","测试"],"title":"Python and Go测试","uri":"/posts/python-go%E6%B5%8B%E8%AF%95/"},{"categories":["Go"],"content":"传统测试 传统的测试中测试数据和测试逻辑混在一起，出错信息也明确，下面有一段网上随便找的一个Python练习题的测试代码，这个题的意思是：传入一个字符串，如果长度小于3就直接返回。如果长度大于3并且是ing结尾那么就加上ly，如果结尾没有ing那么就加上ing def verbing(s): length = len(s) if length \u003e 3: if s[-3:] == \"ing\": return s + \"ly\" return s + \"ing\" return s def test(a, b: Any): if a == b: prefix = ' OK ' else: prefix = ' X ' print('%s got: %s expected: %s' % (prefix, repr(a), repr(b))) def main(): test(verbing('hail'), 'hailing') test(verbing('swiming'), 'swimingly') test(verbing('do'), 'do') 这个测试呢是传统测试方式，测试逻辑也是比较简单，但是这个测试程序不会随便挂掉 ","date":"2020-10-26","objectID":"/posts/python-go%E6%B5%8B%E8%AF%95/:1:0","tags":["Go","Python","测试"],"title":"Python and Go测试","uri":"/posts/python-go%E6%B5%8B%E8%AF%95/"},{"categories":["Go"],"content":"表格驱动测试 ","date":"2020-10-26","objectID":"/posts/python-go%E6%B5%8B%E8%AF%95/:2:0","tags":["Go","Python","测试"],"title":"Python and Go测试","uri":"/posts/python-go%E6%B5%8B%E8%AF%95/"},{"categories":["Go"],"content":"Python 使用表格驱动测试能让我们将测试与数据分开，并且专注于数据而不是测试逻辑，Python的表格驱动测试逻辑写还算简单，但是在判断上就戳中了一大痛点，海象运算符在这里使用只会让程序变得更复杂 from typing import Dict, Iterable def verbing(s: str) -\u003e str: length = len(s) if length \u003e 3: if s[-3:] == \"ing\": return s + \"ly\" return s + \"ing\" return s def test(test_data: Iterable[Dict]): for data in test_data: if verbing(data[\"a\"]) != data[\"b\"]: print(f\"X got: {data['a']} expected: {data['b']}\") if __name__ == \"__main__\": test_data = [ { \"a\": \"hail\", \"b\": \"hailing\", }, { \"a\": \"swiming\", \"b\": \"swimingly\" }, { \"a\": \"do\", \"b\": \"do\" } ] test(test_data=test_data) 上面的程序中使用了list嵌套dict的方式，然后通过数据迭代来完成整个测试，将我们需要测试的数据和我们的测试逻辑完成了分离，因此当测试不通过的时候我们可以直接查看到错误数据是哪，而不需要去一个一个找。 ","date":"2020-10-26","objectID":"/posts/python-go%E6%B5%8B%E8%AF%95/:2:1","tags":["Go","Python","测试"],"title":"Python and Go测试","uri":"/posts/python-go%E6%B5%8B%E8%AF%95/"},{"categories":["Go"],"content":"Go Go的表格驱动测试写起来就十分简单了，但是有一些格式上的要求，不过这些格式只会简化整个测试过程 测试程序和被测试程序需要在一个包下，而且包的声明都需要是main 创建的测试程序需要单独成文件，文件名称需要是_test结尾的 测试函数需要传递一个testing.T的指针 下面是一个计算三角形的第三边的程序这个程序的名称叫做triangle.go，需要使用calcTriangle来算出第三边的长度 package main import ( \"fmt\" \"math\" ) func Triangle() { var a, b int a, b = 3, 4 fmt.Println(calcTriangle(a, b)) } func calcTriangle(a, b int) int { var c int c = int(math.Sqrt(float64(a*a + b*b))) return c } 下面是测试程序，测试程序的名称叫做triangle_test.go package main import \"testing\" func TestTriangle(t *testing.T) { tests := []struct{ a, b, c int }{ {3, 4, 5}, {5, 12, 13}, {8, 15, 17}, {30000, 40000, 50000}, } for _, tt := range tests { if actual := calcTriangle(tt.a, tt.b); actual != tt.c { t.Errorf(\"calcTriangle(%d, %d); got %d; expected %d\", tt.a, tt.b, actual, tt.c) } } } 在Go中做表格驱动测试比Python还简单，只需要声明一个[]struct{}就行，切片中的每个元素就是我们需要测试的数据集合，我们在测试取值的时候就能很简单的拿到值去做验证，而且Go中的三元表达式进一步的让这个测试逻辑写起来变得简单。 那么如果我们没有使用IDE，程序是在服务器上开发的，那么Go语言在测试上的强大就更能体现出来了，我们只需要进入到程序目录，然后使用命令就行 $ go test . 只需要这样一个简单的命令，终端就会输出测试的结果以及测试的时间 ok pujichun.com/learngo/test/triangle 0.292s ","date":"2020-10-26","objectID":"/posts/python-go%E6%B5%8B%E8%AF%95/:2:2","tags":["Go","Python","测试"],"title":"Python and Go测试","uri":"/posts/python-go%E6%B5%8B%E8%AF%95/"},{"categories":["Go"],"content":"代码覆盖率 我们在IDE中写完测试文件之后在点击运行的时候有一个Run ……with Coverage，可以通过这样运行程序来看我们测试的代码覆盖率 运行之后红的就是没有覆盖到的，绿色的就是覆盖到了的 也可以通过命令行来完成 go test -coverprofile=t.out 然后这个覆盖结果就会输出到t.out这个文件中，但是这个文件看起来很麻烦，所以我们需要转换一下 go tool cover -html=t.out 然后就会将这个代码覆盖率转成html并打开 ","date":"2020-10-26","objectID":"/posts/python-go%E6%B5%8B%E8%AF%95/:2:3","tags":["Go","Python","测试"],"title":"Python and Go测试","uri":"/posts/python-go%E6%B5%8B%E8%AF%95/"},{"categories":["Go"],"content":"性能测试 ","date":"2020-10-26","objectID":"/posts/python-go%E6%B5%8B%E8%AF%95/:3:0","tags":["Go","Python","测试"],"title":"Python and Go测试","uri":"/posts/python-go%E6%B5%8B%E8%AF%95/"},{"categories":["Go"],"content":"Python Python的性能测试工具很多，我比较推荐的是cProfile因为模块是C语言写的，使用起来也很简单 我将上面的测试程序中测试部分封装到main函数中，然后只需要添加一行 cProfile.run(\"main()\") 运行这个程序就会输出性能测试结果了 另外一个模块就是line_profiler这个模块使用起来极其简单 pip install line_profiler 安装之后kernprof.py就会被添加到环境变量中 使用的时候只需要在需要测试的函数上加一个@profile装饰器就行 但是在运行的时候需要使用 kernprof -l -v testing.py 命令写起来虽然有一点麻烦，但是至少在我们不需要测试的时候也不要去改代码 当然了，Pycharm也是可以做的，而且看起来更美观一点，甚至直接把调用栈给可视化了出来，在调用栈中可以看到消耗时间的大小，只需要在运行的时候选择Profile 程序文件名即可 ","date":"2020-10-26","objectID":"/posts/python-go%E6%B5%8B%E8%AF%95/:3:1","tags":["Go","Python","测试"],"title":"Python and Go测试","uri":"/posts/python-go%E6%B5%8B%E8%AF%95/"},{"categories":["Go"],"content":"Go 那么在Go中只需要创建测试文件即可，就像上面的表格驱动测试一样，而且两个逻辑是可以同时存在里面的，需要使用哪个就用哪个就行，这里在triangle_test.go中添加这个性能测试函数 func BenchmarkTriangle(b *testing.B) { x, y, z := 30000, 40000, 50000 for i := 0; i \u003c b.N; i++ { if actual := calcTriangle(x, y); actual != z { b.Errorf(\"calcTriangle(%d, %d); got %d; expected %d\", x, y, actual, z) } } } 然后运行这个函数就能得到结果，上面的b.N就是Go语言自动分配的测试次数 命令行也可以执行 go test -bench . ","date":"2020-10-26","objectID":"/posts/python-go%E6%B5%8B%E8%AF%95/:3:2","tags":["Go","Python","测试"],"title":"Python and Go测试","uri":"/posts/python-go%E6%B5%8B%E8%AF%95/"},{"categories":["Go"],"content":"在计算机性能调试领域里，profiling 是指对应用程序的画像，画像就是应用程序使用 CPU 和内存的情况。 Go语言是一个对性能特别看重的语言，因此语言中自带了 profiling 的库，这篇文章就要讲解怎么在 golang 中做 profiling。 ","date":"2020-10-26","objectID":"/posts/go%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/:0:0","tags":["Go","性能调优"],"title":"Go性能调优","uri":"/posts/go%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"},{"categories":["Go"],"content":"常见的性能优化点 【CPU】去除不必要的序列化/反序列化 ：标准的json库非常耗性能，可以考虑使用其他库，更好的是使用grpc。 【CPU】线程泄露：goroutine飞出去以后忘了stop/close，尤其是time.ticker之类的定时器。 【CPU】盲目开启过多goroutine：goroutine能控制就控制一下，开几百个非阻塞的goroutine会导致调度消耗过多CPU。 【MEM】减少GC：举例来说，字符串(string)传递都是值拷贝，如果数据量大，会产生大量gc，此时完全可以用byte数组来代替。高阶一点就是使用sync.Pool来做一个池子，自己处理内存复用和回收的工作，避免GC。Java在这一块做的很成熟，有新生代和老年代的概念，golang还没有那么完善。 【MEM】内存预分配：golang的slice很好用，但是底层是数组，如果空间能确定就先确定下来，以免append的时候空间不够导致不停的copy数据来扩大数组大小。 【DISK】减少磁盘随机读写IO：SATA盘的理论最大读写在300MB/s，磁盘寻道就要10ms，如果大量随机读写，哪怕1kb数据读写，也要10ms，时间就主要花在寻道上。 【NET】新服务的话，尽量使用grpc代替http。 ","date":"2020-10-26","objectID":"/posts/go%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/:1:0","tags":["Go","性能调优"],"title":"Go性能调优","uri":"/posts/go%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"},{"categories":["Go"],"content":"Go语言项目中的性能优化主要有以下几个方面： CPU profile：报告程序的 CPU 使用情况，按照一定频率去采集应用程序在 CPU 和寄存器上面的数据 Memory Profile（Heap Profile）：报告程序的内存使用情况 Block Profiling：报告 goroutines 不在运行状态的情况，可以用来分析和查找死锁等性能瓶颈 Goroutine Profiling：报告 goroutines 的使用情况，有哪些 goroutine，它们的调用关系是怎样的 ","date":"2020-10-26","objectID":"/posts/go%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/:2:0","tags":["Go","性能调优"],"title":"Go性能调优","uri":"/posts/go%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"},{"categories":["Go"],"content":"cpuprofile 在性能测试的程序中，只需要在命令行中输入命令即可 go test -bench . -cpuprofile cpu.out 当程序性能测试完成之后就会在当前路径下生成一个cpu.out的二进制文件 那么我们需要在查看这个文件中的信息，需要使用到Go中的pprof go tool pprof cpu.out 这时候会进入一个交互式的命令环境中 使用help命令能够查看在这个环境中能做的事情，可以直接输入web就能看到性能消耗的可视化图像 我输入web以后报错了failed to execute dot. Is Graphviz installed? Error: exec: \"dot\": executable file not found in %PATH% 这是因为电脑没有安装gvedit导致的 https://graphviz.gitlab.io/_pages/Download/windows/graphviz-2.38.msi 下载后安装，将bin目录添加到环境变量即可 进入pprof的交互环境后执行web命令就能弹出一个程序执行时间消耗图，然后就能根据占用时间对程序进行优化了。 ","date":"2020-10-26","objectID":"/posts/go%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/:3:0","tags":["Go","性能调优"],"title":"Go性能调优","uri":"/posts/go%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"},{"categories":["Go"],"content":"Viper是适用于Go应用程序的完整配置解决方案。它被设计用于在应用程序中工作，并且可以处理所有类型的配置需求和格式 它支持： 设置默认值 从JSON，TOML，YAML，HCL，envfile和Java属性配置文件中读取 实时观看和重新读取配置文件（可选） 从环境变量中读取 从远程配置系统（etcd或Consul）中读取，并观察更改 从命令行标志读取 从缓冲区读取 设置显式值 可以将Viper视为满足我们所有应用程序配置需求的注册表。 关于Viper详细的内容可以去看这个项目的README ","date":"2020-10-25","objectID":"/posts/go%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86--viper/:0:0","tags":["Go"],"title":"Go项目配置管理Viper","uri":"/posts/go%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86--viper/"},{"categories":["Go"],"content":"安装 go get -u -v github.com/spf13/viper ","date":"2020-10-25","objectID":"/posts/go%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86--viper/:1:0","tags":["Go"],"title":"Go项目配置管理Viper","uri":"/posts/go%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86--viper/"},{"categories":["Go"],"content":"指定配置文件 可以直接指定文件路径 viper.SetConfigFile(\"./conf/config.yaml\") 指定文件名，然后指定路径 viper.SetConfigFile(\"config.yaml\") viper.AddConfigPath(\"./conf\") ","date":"2020-10-25","objectID":"/posts/go%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86--viper/:2:0","tags":["Go"],"title":"Go项目配置管理Viper","uri":"/posts/go%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86--viper/"},{"categories":["Go"],"content":"查找并读取配置文件 直接读取 err := viper.ReadInConfig() if err != nil { panic(err) } 读取io.Reader func ReadConfig(in io.Reader) error { return v.ReadConfig(in) } ","date":"2020-10-25","objectID":"/posts/go%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86--viper/:3:0","tags":["Go"],"title":"Go项目配置管理Viper","uri":"/posts/go%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86--viper/"},{"categories":["Go"],"content":"获取值 Viper获取值的方法有下面几种 Get(key string) : interface{} GetBool(key string) : bool GetFloat64(key string) : float64 GetInt(key string) : int GetIntSlice(key string) : []int GetString(key string) : string GetStringMap(key string) : map[string]interface{} GetStringMapString(key string) : map[string]string GetStringSlice(key string) : []string GetTime(key string) : time.Time GetDuration(key string) : time.Duration IsSet(key string) : bool AllSettings() : map[string]interface{} ","date":"2020-10-25","objectID":"/posts/go%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86--viper/:4:0","tags":["Go"],"title":"Go项目配置管理Viper","uri":"/posts/go%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86--viper/"},{"categories":["Go"],"content":"嵌套访问值 Viper可以通过传入.分隔的路径来访问嵌套的字段，例如，配置文件的内容是下面的一段JSON { \"host\": { \"address\": \"localhost\", \"port\": 5799 }, \"datastore\": { \"metric\": { \"host\": \"127.0.0.1\", \"port\": 3099 }, \"warehouse\": { \"host\": \"198.0.0.1\", \"port\": 2112 } } } viper.GetInt(\"datastore.warehouse.port\") //返回2112 ","date":"2020-10-25","objectID":"/posts/go%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86--viper/:4:1","tags":["Go"],"title":"Go项目配置管理Viper","uri":"/posts/go%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86--viper/"},{"categories":["Go"],"content":"反序列化 可以将所有或特定的值解析到结构体、map等 例如下面的一段yaml格式的配置内容 database: school port: 3306 user: root password: 123456 需要先定义好结构体 type Config struct { User string Password string Database string Port int } 然后直接反序列化绑定 viper.SetConfigFile(\"./conf/config.yaml\") if err := viper.ReadInConfig(); err != nil { panic(err) } var C Config if err := viper.Unmarshal(\u0026C); err != nil { panic(err) } fmt.Println(C.User) 如果是上面的那段JSON那种嵌套字段就需要定义嵌套结构体 首先还是需要先定义好结构体 type Config struct { Host DataStore WareHouse } type Host struct { Address string Port int } type DataStore struct { Host string Metric } type Metric struct { Host string Port int } type WareHouse struct { Host string Port int } 然后反序列化绑定 viper.SetConfigFile(\"./conf/config.json\") if err := viper.ReadInConfig(); err != nil { panic(err) } var C Config if err := viper.Unmarshal(\u0026C); err != nil { panic(err) } fmt.Println(C.DataStore.Port) ","date":"2020-10-25","objectID":"/posts/go%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86--viper/:5:0","tags":["Go"],"title":"Go项目配置管理Viper","uri":"/posts/go%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86--viper/"},{"categories":["Go"],"content":"全局化 上面的读取配置，访问字段，以及反序列化为的都是我们在应用程序中能够直接使用，一个程序我们不可能只写一个文件，就算只写一个文件，我们怎么也会将文件中的程序模块化，这时候我们就该让配置全局化 不将配置反序列化，读取配置后直接在别的模块中获取字段，如果文件拆分为了不同的文件模块，那么只需要在程序开始时读入配置，然后直接使用viper.GetString(\"字段名称\")即可，还是刚才的那段JSON配置，新建了一个utils包，然后创建一个Go文件写入下面的程序 package utils import ( \"fmt\" \"github.com/spf13/viper\" ) func PrintConfig() { fmt.Printf(\"host: %s, port: %d\", viper.GetString(\"datastore.metric.host\"), viper.GetInt(\"datastore.metric.port\")) } 然后在main包中调用 func main() { viper.SetConfigFile(\"./conf/config.json\") if err := viper.ReadInConfig(); err != nil { panic(err) } utils.PrintConfig() } 编译后运行程序就能看到输出了结果，这样是比较方便的 如果将读取的配置文件反序列化了，那么则需要将这个结构体名称首字母大写变为可访问的，在conf包中新建一个Go文件，将刚才的程序做一下修改 package conf import \"github.com/spf13/viper\" type Config struct { Host DataStore WareHouse } type Host struct { Address string Port int } type DataStore struct { Host string Metric } type Metric struct { Host string Port int } type WareHouse struct { Host string Port int } var C Config func ReadConfig() { viper.SetConfigFile(\"./conf/config.json\") if err := viper.ReadInConfig(); err != nil { panic(err) } if err := viper.Unmarshal(\u0026C); err != nil { panic(err) } } 然后读取配置 func main() { conf.ReadConfig() fmt.Printf(\"host: %s, port: %d\", conf.C.DataStore.Host, conf.C.DataStore.Port) } 但是在这里使用viper.GetString(\"datastore.metric.host\")依旧能访问到字段，说明绑定以后读取的配置内容并没有被销毁，所以最好使用第一种方法，这样性能可能会稍微提高一点 ","date":"2020-10-25","objectID":"/posts/go%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86--viper/:6:0","tags":["Go"],"title":"Go项目配置管理Viper","uri":"/posts/go%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86--viper/"},{"categories":["Go"],"content":"在Go中服务器异常统一处理方式 首先创建一个Server来查看本地文件 func main() { http.HandleFunc(\"/list/\", func(writer http.ResponseWriter, request *http.Request) { path := request.URL.Path[len(\"/list/\"):] file, err := os.Open(path) if err != nil { panic(err) } all, err := ioutil.ReadAll(file) if err != nil { panic(err) } writer.Write(all) }) err := http.ListenAndServe(\"9090\", nil) if err != nil { panic(err) } } 这时候访问localhost:9090/list/{file-path}这个地址，如果这个文件是普通文件，而且存在，那么在浏览器上就能看到文件内容了 但是如果文件不存在，那么这时候浏览器就不能正常访问，而且本地将会报错，Go语言不像其他语言一样拥有try-catch或者try-except语句，而且这样的错误处理语句多了以后程序的健壮性将会下降，那么我们就需要将错误进行封装处理 这里先将/list/的处理函数拆分出去 func handlerList(writer http.ResponseWriter, request *http.Request) { path := request.URL.Path[len(\"/list/\"):] file, err := os.Open(path) if err != nil { panic(err) } all, err := ioutil.ReadAll(file) if err != nil { panic(err) } writer.Write(all) } func main() { http.HandleFunc(\"/list/\", handlerList) err := http.ListenAndServe(\"9090\", nil) if err != nil { panic(err) } } 虽然这时候程序并没有很大的改变，但是现在我们可以将error交给专门处理异常的函数进行处理 首先定义一个类型appHandler，这个类型的函数接收两个参数http.ResponseWriter, *http.Request返回一个error type appHandler func(writer http.ResponseWriter, request *http.Request) error 然后利用Go中闭包的特性将错误逻辑进行封装，定义一个errWrapper函数，将业务处理的逻辑函数作为参数传递进去，返回一个能处理异常的函数，将http.ResponseWriter, *http.Request两个对象先交给appHandler类型的函数去处理，然后得到一个error对象，如果这个error对象不是nil，那么就需要进行异常处理了 func errWrapper(handler appHandler) func(writer http.ResponseWriter, request *http.Request) { return func(writer http.ResponseWriter, request *http.Request) { err := handler(writer, request) code := http.StatusOK if err != nil { log.Printf(\"Error handling request: %s\", err.Error()) switch { case os.IsNotExist(err): code = http.StatusNotFound default: code = http.StatusInternalServerError } http.Error(writer, http.StatusText(code), code) } } } 这时候需要更改一下handler函数，将error进行返回，然后http.HandleFunc的处理函数就要改为errWrapper(handlerList) func handlerList(writer http.ResponseWriter, request *http.Request) { path := request.URL.Path[len(\"/list/\"):] file, err := os.Open(path) if err != nil { return err } all, err := ioutil.ReadAll(file) if err != nil { return err } writer.Write(all) return nil } type appHandler func(writer http.ResponseWriter, request *http.Request) error func errWrapper(handler appHandler) func(writer http.ResponseWriter, request *http.Request) { return func(writer http.ResponseWriter, request *http.Request) { err := handler(writer, request) code := http.StatusOK if err != nil { log.Printf(\"Error handling request: %s\", err.Error()) switch { case os.IsNotExist(err): code = http.StatusNotFound default: code = http.StatusInternalServerError } http.Error(writer, http.StatusText(code), code) } } } func main() { http.HandleFunc(\"/list/\", errWrapper(handlerList)) err := http.ListenAndServe(\"9090\", nil) if err != nil { panic(err) } } ","date":"2020-08-19","objectID":"/posts/go%E4%B8%ADweb%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F/:0:0","tags":["Go","Web","error"],"title":"Go中Web异常处理方式","uri":"/posts/go%E4%B8%ADweb%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F/"},{"categories":["网络爬虫"],"content":"网站url: http://fanyi.youdao.com/ 请求这个网站的api: http://fanyi.youdao.com/translate_o?smartresult=dict\u0026smartresult=rule 需要的参数很多，如果将translate_o中的_o去掉，那么请求需要的验证性参数都不会去验证，去掉后的api似乎是老的api ","date":"2020-08-01","objectID":"/posts/%E6%9C%89%E9%81%93%E7%BF%BB%E8%AF%91web%E7%AB%AFjs%E9%80%86%E5%90%91/:0:0","tags":["网络爬虫","Python","js逆向","参数混淆"],"title":"有道翻译web端js逆向","uri":"/posts/%E6%9C%89%E9%81%93%E7%BF%BB%E8%AF%91web%E7%AB%AFjs%E9%80%86%E5%90%91/"},{"categories":["网络爬虫"],"content":"Cookie 通过多次请求发现请求中的Cookie是变化的 并且多次请求之后还产生了一个JSESSIOND的字段，在Application面板中将这个字段删除后也能成功请求，但是将___rl__test__cookies删除后又会重新生成，全局搜索这个字段发现是时间戳 那么Cookie就解决了 ","date":"2020-08-01","objectID":"/posts/%E6%9C%89%E9%81%93%E7%BF%BB%E8%AF%91web%E7%AB%AFjs%E9%80%86%E5%90%91/:0:1","tags":["网络爬虫","Python","js逆向","参数混淆"],"title":"有道翻译web端js逆向","uri":"/posts/%E6%9C%89%E9%81%93%E7%BF%BB%E8%AF%91web%E7%AB%AFjs%E9%80%86%E5%90%91/"},{"categories":["网络爬虫"],"content":"Form Data 多次请求发现发生变化的字段只有i、salt、sign、ts i是需要翻译的内容 salt和ts都很像时间戳，salt比ts少一位 sign应该是一个混淆编码后的参数 全局搜索sign 通过断点调试就能发现ts是时间戳，salt是ts字符串加一个随机数字符组成的新字符串，bv是浏览器版本信息通过MD5加密生成的，sign是一个复合字符串通过MD5加密生成的 那么Form Data也就解决了 ","date":"2020-08-01","objectID":"/posts/%E6%9C%89%E9%81%93%E7%BF%BB%E8%AF%91web%E7%AB%AFjs%E9%80%86%E5%90%91/:0:2","tags":["网络爬虫","Python","js逆向","参数混淆"],"title":"有道翻译web端js逆向","uri":"/posts/%E6%9C%89%E9%81%93%E7%BF%BB%E8%AF%91web%E7%AB%AFjs%E9%80%86%E5%90%91/"},{"categories":["网络爬虫"],"content":"代码 https://gitee.com/pujichun/spider/tree/master/fanyi.youdao ","date":"2020-08-01","objectID":"/posts/%E6%9C%89%E9%81%93%E7%BF%BB%E8%AF%91web%E7%AB%AFjs%E9%80%86%E5%90%91/:0:3","tags":["网络爬虫","Python","js逆向","参数混淆"],"title":"有道翻译web端js逆向","uri":"/posts/%E6%9C%89%E9%81%93%E7%BF%BB%E8%AF%91web%E7%AB%AFjs%E9%80%86%E5%90%91/"},{"categories":["网络爬虫"],"content":"网站url: https://piaofang.maoyan.com/rankings/year?year=2020\u0026limit=100\u0026tab=1 初看网站的html发现有字体反爬虫，但是经过多次抓包之后发现除了第一次请求页面有字体反爬虫之外，其它几个年份并没有 而且第一次请求的年分的数据也可以通过api请求到 简单分析字体文件后发现字体和后面请求到的页面对不上，而且每次请求字体的url都不一样，因此判断字体，全局搜索字体url发现url在请求得到的html中那么字体对应问题就解决了 请求数据的时候发现有一个Uid参数比较特殊 网页进行异步请求的时候这个Uid并不会变，但是刷新网页之后这个Uid就改变了 全局搜索Uid，在nAjax.js中找到了这个Uid 通过上面的程序可以发现Uid是通过解析html中的meat标签中name属性为csrf的那个标签，然后获取到他的content属性值 那么Uid也解决了 最后只需要解析数据即可 ","date":"2020-07-31","objectID":"/posts/%E7%88%AC%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1%E7%A5%A8%E6%88%BF/:0:0","tags":["Go","网络爬虫","字体反爬"],"title":"爬取猫眼电影票房","uri":"/posts/%E7%88%AC%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1%E7%A5%A8%E6%88%BF/"},{"categories":["网络爬虫"],"content":"完整代码 https://gitee.com/pujichun/spider/tree/master/piaofang.maoyan ","date":"2020-07-31","objectID":"/posts/%E7%88%AC%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1%E7%A5%A8%E6%88%BF/:0:1","tags":["Go","网络爬虫","字体反爬"],"title":"爬取猫眼电影票房","uri":"/posts/%E7%88%AC%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1%E7%A5%A8%E6%88%BF/"},{"categories":["网络爬虫"],"content":"AES解密 Python在Windows环境下使用AES可以使用第三方库pycryptodome pip install pycryptodome Python在Linux环境下使用AES可以使用第三方库pycrypto pip install pycrtpto 安装好之后就可以直接调用Crypto.Cipher下的AES模块了 AES加密 AES会对需要加密的明文进行分组，将全量明文分组为一个个16个字节（128bit）的明文块，所以需要加密的明文字节数量必须是16个字节的倍数，但是网络传输的文本内容长度是变化无常的，长度很难严格控制在16个字节的倍数，因此往往会在字节类型的文本最后空格来补足，因为空格容易处理 ","date":"2020-07-22","objectID":"/posts/%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86-aes%E5%8A%A0%E5%AF%86/:0:0","tags":["js逆向","网络爬虫"],"title":"对称加密-AES加密","uri":"/posts/%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86-aes%E5%8A%A0%E5%AF%86/"},{"categories":["网络爬虫"],"content":"CBC加密 CBC加密需要一个16个字节的key（密钥）和一个16位的iv（偏移量） 使用AES加密需要调用AES模块中的new方法来生成一个AES对象，这个方法主要需要三个参数 AES.new(key: bytes, mode: AESMode, iv: bytes) 然后使用AES对象来将明文转换为密文（加密的明文需要是字节类型） from Crypto.Cipher import AES def add_space(text:str) -\u003e bytes: if length := len(encode_text := text.encode(\"utf8\")) % 16: encode_text += b\" \" * (16 - length) return encode_text def encrypt(text:str) -\u003e bytes: key = b\"1234567812345678\" iv = b\"abcdefghabcdefgh\" mode = AES.MODE_CBC # 生成加密器 cryptor = AES.new(key=key, mode=mode, iv=iv) encode_text = add_space(text) # 对明文进行加密 cipher = cryptor.encrypt(encode_text) return cipher ","date":"2020-07-22","objectID":"/posts/%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86-aes%E5%8A%A0%E5%AF%86/:1:0","tags":["js逆向","网络爬虫"],"title":"对称加密-AES加密","uri":"/posts/%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86-aes%E5%8A%A0%E5%AF%86/"},{"categories":["网络爬虫"],"content":"CBC解密 解密的时候也需要创建一个AES对象，这个对象和加密时的对象一样，但是调用的是decrypt方法。 def decrypt(cipher: bytes) -\u003e str: key = b\"1234567812345678\" iv = b\"abcdefghabcdefgh\" mode = AES.MODE_CBC cryptor = AES.new(key=key, mode=mode, iv=iv) text = cryptor.decrypt(cipher) return text.decode(\"utf8\").strip() 其他的加密和解密方法也都类似，只是参数不同，大多数情况都是使用CBC加密，因为有一个偏移量能让程序密文更加不容易被破解 ","date":"2020-07-22","objectID":"/posts/%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86-aes%E5%8A%A0%E5%AF%86/:2:0","tags":["js逆向","网络爬虫"],"title":"对称加密-AES加密","uri":"/posts/%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86-aes%E5%8A%A0%E5%AF%86/"},{"categories":["网络爬虫"],"content":"案例 URL:https://www.in985.com/dataCenter/explan?tdsourcetag=s_pcqq_aiomsg 这个页面的数据采用的Ajax请求，需要登陆 这里的数据是加密的，而且有一个encry:true的字段 这里虽然没指出是AES加密，但是可以猜到，可以直接全局搜索decrypt，当然如果不确定也可以全局搜索JSON.parse，因为这是个字符串，解密后肯定是要转为json数据的，在相关的地方打上断点，然后点击下一页进行数据请求 这里就是整个AES的CBC解密函数，通过调试可以知道这个t就是请求得到的加密后的字符串，可以看到这里其实还对数据进行了Hex解码，然后进行base64编码，但是这里JavaScript用的AES库解码时需要传base54编码，所以用Python来解码的时候不需要进行base64编码 加密方式时CBC，密钥key就是e，偏移量iv就是n ","date":"2020-07-22","objectID":"/posts/%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86-aes%E5%8A%A0%E5%AF%86/:3:0","tags":["js逆向","网络爬虫"],"title":"对称加密-AES加密","uri":"/posts/%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86-aes%E5%8A%A0%E5%AF%86/"},{"categories":["网络爬虫"],"content":"Python解密程序 from Crypto.Cipher import AES from binascii import a2b_hex def decrypt(text: str) -\u003e str: cipher = a2b_hex(text) key = \"m1x2!3p4$567890000000123456789@$\".encode(\"utf8\") iv = b\"1234567890000000\" mode = AES.MODE_CBC cryptor = AES.new(key=key, mode=mode, iv=iv) original_text = cryptor.decrypt(cipher) return original_text.decode(\"utf8\") 将请求得到的数据直接用这个方法进行解码就能得到json字符串了，但是还需要去除字符串末尾的空格和json序列化 ","date":"2020-07-22","objectID":"/posts/%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86-aes%E5%8A%A0%E5%AF%86/:3:1","tags":["js逆向","网络爬虫"],"title":"对称加密-AES加密","uri":"/posts/%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86-aes%E5%8A%A0%E5%AF%86/"},{"categories":["Go"],"content":"在Go语言中我们一般使用切片而不是数组 ","date":"2020-07-19","objectID":"/posts/go-slice%E5%8E%9F%E7%90%86/:0:0","tags":["Slice","Go"],"title":"Go Slice原理","uri":"/posts/go-slice%E5%8E%9F%E7%90%86/"},{"categories":["Go"],"content":"Slice扩展 Go语言中Slice是数组的一段view。一个切片中记录了一个ptr（指针），用于保存这个Slice中第0个元素指向数组的中的位置，有一个值记录了这个Slice的len（长度），还有有一个值记录了这个Slice在这个数组中及以后的长度cap（容量） arr := [...]int{0, 1, 2, 3, 4, 5, 6, 7} s1 := arr[2:6] s2 := s1[3:5] 在上面的程序中可以很容易得出s1的值为[2 3 4 5]，但是s1的长度仅为4，而s2是从s1中索引为3，也就是第4个元素开始取，取到索引为4（第五个）的元素，这样的话s1的长度并不够，但是运行这个程序s2的值却为[5 6]，但是运行s1[4]程序却会报错。 这个过程图解如下 因为Slice在内部实现的时候有一个长度，所以我们无法通过索引越界取到arr中的值，但是通过对s1进行切片操作的时候这个只要在cap内那么就能通过映射取到arr中的值。 Slice的结构决定了它能向后扩展，但是不能向前扩展，向后扩展的时候不能超过cap的长度 fmt.Printf(\"%v, %d, %d\", s1, len(s1), cap(s1)) ","date":"2020-07-19","objectID":"/posts/go-slice%E5%8E%9F%E7%90%86/:1:0","tags":["Slice","Go"],"title":"Go Slice原理","uri":"/posts/go-slice%E5%8E%9F%E7%90%86/"},{"categories":["Go"],"content":"Slice扩容 上面的内容说的都是Slice扩展的时候，当然了，如果我们使用Slice存储数据的时候如果Slice的长度不够Slice是会动态扩容的，但是Slice动态扩容对性能消耗还是不小，所以最好是开始的时候就指定好cap ","date":"2020-07-19","objectID":"/posts/go-slice%E5%8E%9F%E7%90%86/:2:0","tags":["Slice","Go"],"title":"Go Slice原理","uri":"/posts/go-slice%E5%8E%9F%E7%90%86/"},{"categories":["Python"],"content":"装饰器的原理和执行逻辑 装饰器是可调用的对象，装饰器的参数是函数，函数在Python中也是对象。简单理解，装饰器就是一个特殊的函数，它的参数是函数，我们在执行装饰器时可以使用Python中的语法糖@decorate，也可以使用函数执行的方式运行decorate(func) # 假设现在有一个名为decorator的装饰器 @decorator def target(): print(\"this target()\") target = decorator(target) 上面程序中两种执行方式不同，但是结果是相同的 ","date":"2020-07-11","objectID":"/posts/python%E8%A3%85%E9%A5%B0%E5%99%A8/:0:0","tags":["装饰器","Python"],"title":"Python装饰器","uri":"/posts/python%E8%A3%85%E9%A5%B0%E5%99%A8/"},{"categories":["Python"],"content":"Python何时执行装饰器 Python中装饰器的一个关键特性是，它们在被装饰的函数定义之后立即执行 Python何时执行装饰器是一个很重要的点，并不是说它有多复杂，而是决定了我们如何编写装饰器 当运行下面程序时将会报错。 import time def time_diff(func): start_time = time.time() func() print(\"程序运行时间为: \", time.time() - start_time) @time_diff def target(): for i in range(10000): print(i) target() 因为target函数中的程序在被定义之后就执行了，所以我们手动调用target将会是一个NoneType，那么这样我们就不能在想执行的时候执行它 因此我们想要能够像函数一样执行就需要给返回一个函数，在函数中定义一个函数，然后将这个函数作为对象返回 import time def time_diff(func): def inner(): start_time = time.time() func() print(time.time() - start_time) return inner @time_diff def target(): for i in range(10000): print(i) target() 此时我们就可以像函数一样调用这个被装饰的函数，这个过程其实我们可以通过装饰器的执行特性想到，当我们用装饰器装饰这个函数 的时候其实就是将这个函数当作参数传给了这个装饰器背后的函数，如果装饰器函数没有返回值，那么这个被装饰的函数将是NoneType，其实也就是上面第一个程序实例中的过程，只是上面的程序中并没有将装饰器函数写出来 import time def time_diff(func): def inner(): start_time = time.time() func() print(time.time() - start_time) return inner @time_diff def target(): for i in range(10000): print(i) target = time_diff(target) 上面的程序将装饰器的装饰过程还原为了函数闭包的写法 ","date":"2020-07-11","objectID":"/posts/python%E8%A3%85%E9%A5%B0%E5%99%A8/:1:0","tags":["装饰器","Python"],"title":"Python装饰器","uri":"/posts/python%E8%A3%85%E9%A5%B0%E5%99%A8/"},{"categories":["Python"],"content":"维持原函数属性 我们使用装饰器是有一些隐患的，我们通过程序的执行过程我们可以看到target这个函数最终是被指向inner这个函数的，我们可以通过查看函数的属性验证 import time def time_diff(func): def inner(): start_time = time.time() func() print(time.time() - start_time) return inner @time_diff def target(): for i in range(10000): print(i) print(target.__name__) 此时的target指向的就是inner这个函数，编写装饰器时大多数时候都是重新返回一个函数，我们可以通过函数数学进行查看 print(target.__name__) 输出结果将会是inner 此时如果我们再添加一个函数 import time def time_diff(func): def inner(): start_time = time.time() func() print(time.time() - start_time) return inner @time_diff def target(): for i in range(10000): print(i) @time_diff def foo(): for i in range(100): print(i) print(target.__name__) print(foo.__name__) 此时我们会看到输出结果都是inner。注意：print函数中调用的是函数属性，并没有执行函数。 虽然函数的真实名字对我们目前的程序没有造成影响，但是一些特殊情况下函数的名称可能会对程序产生影响，为了消除装饰器带来的负面影响，我们可以使用functool中的wraps装饰器，这个装饰器能保留函数原来的名称和docstring import functools import time def time_diff(func): @functools.wraps(func) def inner(*args, **kwargs): start_time = time.time() func(*args, **kwargs) print(time.time() - start_time) return inner @time_diff def target(): for i in range(10000): print(i) @time_diff def foo(): for i in range(100): print(i) print(target.__name__) print(foo.__name__) 当然如果只在意函数的名字和docstring，我们也可以手动赋值 import functools import time def time_diff(func): # @functools.wraps(func) def inner(*args, **kwargs): \"\"\"inner函数\"\"\" start_time = time.time() func(*args, **kwargs) print(time.time() - start_time) inner.__name__ = func.__name__ inner.__doc__ = func.__doc__ return inner @time_diff def target(): \"\"\"target函数\"\"\" for i in range(10000): print(i) @time_diff def foo(): \"\"\"foo函数\"\"\" for i in range(100): print(i) print(target.__name__) print(foo.__name__) print(target.__doc__) print(foo.__doc__) 上面的程序中笔者已经将functools.wraps()这个装饰器注释掉了，读者可以自行注释inner.__name__ = func.__name__和inner.__doc__ = func.__doc__查看输出结果变化情况 ","date":"2020-07-11","objectID":"/posts/python%E8%A3%85%E9%A5%B0%E5%99%A8/:2:0","tags":["装饰器","Python"],"title":"Python装饰器","uri":"/posts/python%E8%A3%85%E9%A5%B0%E5%99%A8/"},{"categories":["Python"],"content":"装饰带参的函数 在大部分应用场景中我们几乎都需要给函数传递参数，原函数的写法我们肯定是固定不变的，那么要如何在装饰器函数中给被装饰的函数传入参数呢？ 以上面的装饰器为例，当装饰执行之后target指向的是inner这个函数，当我们在调用target函数时其实调用的是inner函数，因此这个参数需要传给inner函数，然后将接收到的参数传递给原函数就行了。 import time def time_diff(func): def inner(*args, **kwargs): start_time = time.time() func(*args, **kwargs) print(time.time() - start_time) return inner @time_diff def target(n): for i in range(n): print(i) target(20) 通过用不定长参数的接收方法达到让装饰器更通用的效果 ","date":"2020-07-11","objectID":"/posts/python%E8%A3%85%E9%A5%B0%E5%99%A8/:3:0","tags":["装饰器","Python"],"title":"Python装饰器","uri":"/posts/python%E8%A3%85%E9%A5%B0%E5%99%A8/"},{"categories":["Python"],"content":"给装饰器传入参数 在Flask、FastApi中路由函数都是被装饰器装饰的，在被装饰的时候会给装饰器传入一个路由参数 from flask import Flask app = Flask(__name__) @app.route(\"/index\") def index(): return \"\u003ch1\u003eHello Decorate!\u003c/h1\u003e\" if __name__ == \"__main__\": app.run(debug=True) 不妨看看route这个装饰器的源码 def route(self, rule, **options): \"\"\"A decorator that is used to register a view function for a given URL rule. This does the same thing as :meth:`add_url_rule` but is intended for decorator usage:: @app.route('/') def index(): return 'Hello World' For more information refer to :ref:`url-route-registrations`. :param rule: the URL rule as string :param endpoint: the endpoint for the registered URL rule. Flask itself assumes the name of the view function as endpoint :param options: the options to be forwarded to the underlying :class:`~werkzeug.routing.Rule` object. A change to Werkzeug is handling of method options. methods is a list of methods this rule should be limited to (``GET``, ``POST`` etc.). By default a rule just listens for ``GET`` (and implicitly ``HEAD``). Starting with Flask 0.6, ``OPTIONS`` is implicitly added and handled by the standard request handling. \"\"\" def decorator(f): endpoint = options.pop(\"endpoint\", None) self.add_url_rule(rule, endpoint, f, **options) return f return decorator 从上面的源码中可以看出，如果我们要给装饰器函数传递参数，那么被装饰的函数作为参数被接收的逻辑就放到了内部真正要去修饰的函数参数当中，因此想要给装饰器函数传入参数是需要将函数传入的位置写到真正的装饰逻辑当中，二参数写到最外层的函数中。 import time def time_diff(*args, **kwargs): print(args, kwargs) def inner(func): start_time = time.time() func() print(time.time() - start_time) return inner @time_diff(\"Hello Decorate\") def target(): for i in range(10): print(i) target() 运行程序之后将会发生NoneType的error，这是为什么呢？我们明明已经返回了函数，但是为什么在我们调用这个函数之前函数又被提前调用了。这时候需要从装饰器的执行开始分析，**Python中装饰器的一个关键特性是，它们在被装饰的函数定义之后立即执行。**当使用time_diff(\"Hello Decorate\")的时候它是被作为一个普通的闭包函数，而不是一个装饰器，真正的装饰器在内部，而time_diff返回的inner函数才是装饰器函数，返回的inner函数将target函数作为参数立刻形成装饰器的语法糖，然后装饰器被执行了，但是inner函数中是直接执行的func函数，所以在外部调用target函数的时候会发生NoneType的error。 要解决这个问题，我们继续使用函数的闭包特性就可以 import time def time_diff(*args, **kwargs): def wrapper(func): def inner(): print(args, kwargs) start_time = time.time() func() print(time.time() - start_time) return inner return wrapper @time_diff(\"Hello Decorate\") def target(): for i in range(10): print(i) target() 此时一个复杂的装饰器就快实现了，装饰器难点就在于何时装饰，因此我们要给被装饰的函数传递参数的时候，这个参数写在inner函数当中就行。 import time def time_diff(msg): print(msg) def wrapper(func): def inner(*args, **kwargs): start_time = time.time() func(*args, **kwargs) print(time.time() - start_time) return inner return wrapper @time_diff(\"Hello Decorate\") def target(n): for i in range(n): print(i) target(11) ","date":"2020-07-11","objectID":"/posts/python%E8%A3%85%E9%A5%B0%E5%99%A8/:4:0","tags":["装饰器","Python"],"title":"Python装饰器","uri":"/posts/python%E8%A3%85%E9%A5%B0%E5%99%A8/"},{"categories":["Python"],"content":"控制装饰器是否装饰函数 当我们不想这个函数被装饰器装饰的时候我们需要将装饰器的语法糖注释掉，但是如果被装饰的函数很多，那么就需要一个一个的去注释，因此我们需要一个开关控制装饰器是否被装饰 import time ok = True def time_diff(switch): def wrapper(func): def inner(*args, **kwargs): if switch: start_time = time.time() func(*args, **kwargs) print(time.time() - start_time) else: func(*args, **kwargs) return inner return wrapper @time_diff(switch=ok) def target(n): for i in range(n): print(i) target(11) 这样我们就能全局决定是否启用装饰器 ","date":"2020-07-11","objectID":"/posts/python%E8%A3%85%E9%A5%B0%E5%99%A8/:5:0","tags":["装饰器","Python"],"title":"Python装饰器","uri":"/posts/python%E8%A3%85%E9%A5%B0%E5%99%A8/"},{"categories":["Python"],"content":"多个装饰器装饰函数 在实际的开发过程中如果框架的功能完备，那么肯定会有log功能。但是如果轮子需要我们自己造，那么需要打log的地方可能稍微会有点多。曾经写过功能相对较为完善的一个简单的小脚本中，我实例化了一个logger对象，然后在多个文件间调用，整个log调用过程简直是我的噩梦，导致最后这个logger只是一个锦上添花的东西，其实我仅仅是想通过这个log监控我的程序调用栈。后来的优化方案就是使用装饰器，但是在使用装饰器的时候我发现了一个我不得不面对的问题，就是原有的程序的某些方法上已经有一个装饰器了，如果我要在添加一个装饰器该怎么加？ 其实这个问题还是需要回到函数的调用，需要装饰的函数将被作为参数传递给装饰器，然后对这个函数进行加工，如果有第二个装饰器，那么第二个装饰器装饰的就是加工后返回的函数，这个过程可以拆解为函数的参数的传递 def decorator1(f): def wrapper1(): print(\"wrapper1\") f() return wrapper1 def decorator2(f): def wrapper2(): print(\"wrapper2\") f() return wrapper2 def a(): print(\"hello\") a = decorator1(a) a = decorator2(a) 将最后两句代码简化 a = decorator2(decorator1(a)) 如果写为装饰器的语法糖 def decorator1(f): def wrapper1(): print(\"wrapper1\") f() return wrapper1 def decorator2(f): def wrapper2(): print(\"wrapper2\") f() return wrapper2 @decorator2 @decorator1 def a(): print(\"hello\") 装饰器语法糖下方的程序都将是该装饰器的函数参数，当然前提需要下方的结果是一个函数，而@decorator2下方还是一个装饰器语法糖，那么就这样一层一层的套下去。其实我们也可以简单的理解为哪个装饰器先碰到需要被装饰的函数，那么就先执行哪个装饰器。 ","date":"2020-07-11","objectID":"/posts/python%E8%A3%85%E9%A5%B0%E5%99%A8/:6:0","tags":["装饰器","Python"],"title":"Python装饰器","uri":"/posts/python%E8%A3%85%E9%A5%B0%E5%99%A8/"},{"categories":["Python"],"content":"总结 要在面试的时候破装饰器的题，或者想写好装饰器，笔者认为最关键的就是两点 装饰器的参数是函数 装饰器在被装饰的函数定义之后立即执行 这两点在笔者学习装饰器的时候几乎解决了所有问题 ","date":"2020-07-11","objectID":"/posts/python%E8%A3%85%E9%A5%B0%E5%99%A8/:7:0","tags":["装饰器","Python"],"title":"Python装饰器","uri":"/posts/python%E8%A3%85%E9%A5%B0%E5%99%A8/"},{"categories":["Go"],"content":" Gin 是一个用 Go (Golang) 编写的 web 框架。 它是一个类似于 martini 但拥有更好性能的 API 框架, 由于 httprouter，速度提高了近 40 倍。 如果你是性能和高效的追求者, 你会爱上 Gin Gin框架中文文档 拉取仓库，进入到github.com目录下，创建gin-gonic文件夹，然后执行拉取命令 git clone https://github.com/gin-gonic/gin.git 下载仓库源码，下载仓库源码和使用克隆命令一样，都需要创建一个gin-gonic目录 使用go get命令安装 go get -u github.com/gin-gonic/gin ","date":"2020-06-12","objectID":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/:0:0","tags":["gin","Go"],"title":"初识Gin框架","uri":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/"},{"categories":["Go"],"content":"创建一个简单的Web服务 使用Gin框架可以很快的构建一个web服务 package main import ( \"github.com/gin-gonic/gin\" \"net/http\" ) func hello(c *gin.Context) { c.String(http.StatusOK, \"hello, world\") } func main() { r := gin.Default() r.GET(\"/hello\", hello) r.Run(\":9090\") } 使用gin.Default()方法创建的是一个默认的http后端引擎 注册路由的时候可以直接绑定该路由的请求方法 路由处理函数中需要传递一个*gin.Context指针，所有本次请求相关的方法都在这个Context中 响应需要一个状态码，Go语言的http库中已经定义好了全部的响应状态码常量，直接调用即可 引擎启动的时候如果不绑定端口，那么默认就是8080端口 注册路由的方法还提供了一种Any方法，通过GET, POST, PUT, PATCH, HEAD, OPTIONS, DELETE, CONNECT, TRACE其中任何一种方法都能请求到 func (group *RouterGroup) Any(relativePath string, handlers ...HandlerFunc) IRoutes { group.handle(http.MethodGet, relativePath, handlers) group.handle(http.MethodPost, relativePath, handlers) group.handle(http.MethodPut, relativePath, handlers) group.handle(http.MethodPatch, relativePath, handlers) group.handle(http.MethodHead, relativePath, handlers) group.handle(http.MethodOptions, relativePath, handlers) group.handle(http.MethodDelete, relativePath, handlers) group.handle(http.MethodConnect, relativePath, handlers) group.handle(http.MethodTrace, relativePath, handlers) return group.returnObj() } ","date":"2020-06-12","objectID":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/:1:0","tags":["gin","Go"],"title":"初识Gin框架","uri":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/"},{"categories":["Go"],"content":"Restful Api 因为Gin框架的易用性可以很简单的定义Restful Api分风格的路由，并不需要为每种方法都注册一个不同的路由，只需要使用同一个路由来绑定不同的请求方法就可以执行不同的功能，比如约定： GET用来获取资源 POST用来新建资源 PUT用来更新资源 DELETE用来删除资源 这样注册路由会使程序整体更加的清晰，可读性和维护性更高 func main() { r := gin.Default() r.GET(\"/user\", getting) r.POST(\"/user\", posting) r.PUT(\"/user\", putting) r.DELETE(\"/user\", deleting) r.PATCH(\"/user\", patching) r.HEAD(\"/user\", head) r.OPTIONS(\"/user\", options) r.Run() } ","date":"2020-06-12","objectID":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/:1:1","tags":["gin","Go"],"title":"初识Gin框架","uri":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/"},{"categories":["Go"],"content":"获取参数 ","date":"2020-06-12","objectID":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/:2:0","tags":["gin","Go"],"title":"初识Gin框架","uri":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/"},{"categories":["Go"],"content":"获取Querystring参数 querystring指的是URL中?后面携带的参数，例如https://www.google.com/search?q=golang\u0026oq=golang，不同参数之间用\u0026符号连接 func main() { r := gin.Default() r.GET(\"/user\", func(c *gin.Context){ name := c.DefaultQuery(\"name\", \"user\") age := c.Query(\"age\") name2, ok := c.GetQuery(\"name\") c.String(http.StatusOK, \"Hello! %s %s\", name, age) }) r.Run(\":9090\") } DefaultQuery第一个参数是要解析的字段的名称，第二个参数是一个默认值，也就是在找不到name这个参数的时候会使用后面的默认值 Query就是比较常规的解析，如果没有这个字段，那么解析结果就是空字符串 GetQuery是尝试去取值，如果取到那么就会返回该字段的值和布尔值true，如果取不到那么还是空字符串，但是会返回false ","date":"2020-06-12","objectID":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/:2:1","tags":["gin","Go"],"title":"初识Gin框架","uri":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/"},{"categories":["Go"],"content":"获取Data Form参数 func main() { r := gin.Default() r.POST(\"post_form\", func(c *gin.Context) { name := c.DefaultPostForm(\"name\", \"user\") age := c.PostForm(\"age\") name2, ok := GetPostForm(\"name\") c.JSON(http.StatusOK, gin.H{ \"name\": name, \"age\": age, }) r.Run(\":9090\") } 这里解析方法和Querystring差不多 ","date":"2020-06-12","objectID":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/:2:2","tags":["gin","Go"],"title":"初识Gin框架","uri":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/"},{"categories":["Go"],"content":"返回Json数据 ","date":"2020-06-12","objectID":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/:3:0","tags":["gin","Go"],"title":"初识Gin框架","uri":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/"},{"categories":["Go"],"content":"使用map func main() { r := gin.Default() r.GET(\"/json\", func(c *gin.Context) { //使用map data := map[string]interface{}{ \"name\": \"pujic\", \"age\":18, } c.JSON(http.StatusOK, data) }) r.Run(\":9090\") } ","date":"2020-06-12","objectID":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/:3:1","tags":["gin","Go"],"title":"初识Gin框架","uri":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/"},{"categories":["Go"],"content":"使用gin.H gin.H是Gin框架中定义好的map[string]interface{}的类型别名，准确的说应该是这个H，但是调用的时候要使用gin.H func main() { r := gin.Default() r.GET(\"/json\", func(c *gin.Context) { data := gin.H{\"name\": \"pujic\", \"age\": 18} c.JSON(http.StatusOK, data) }) r.Run(\":9090\") } ","date":"2020-06-12","objectID":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/:3:2","tags":["gin","Go"],"title":"初识Gin框架","uri":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/"},{"categories":["Go"],"content":"使用结构体 func main() { r := gin.Default() r.GET(\"/json\", func(c *gin.Context) { var data struct { Name string Age uint8 } data.Name = \"pujic\" data.Age = 18 c.JSON(http.StatusOK, data) }) r.Run(\":9090\") } c.JSON()函数在拿到数据之后会对数据进行一个Json.Marshal，Go语言中Json序列化是通过反射取值的，如果结构体的首字母不是大写，那么就取不到这个字段 但是如果前端请求的Json数据字段需要是首字母小写的，那么就需要给结构体添加Tag，指定Json Marshal的时候该字段的key func main() { r := gin.Default() r.GET(\"/json\", func(c *gin.Context) { var data struct { Name string `json:\"name\"` Age uint8 `json:\"age\"` } data.Name = \"pujic\" data.Age = 18 c.JSON(http.StatusOK, data) }) r.Run(\":9090\") } ","date":"2020-06-12","objectID":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/:3:3","tags":["gin","Go"],"title":"初识Gin框架","uri":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/"},{"categories":["Go"],"content":"获取URL路径参数 想要获取到路径参数需要进行规则匹配 func main() { r := gin.Default() r.GET(\"/:name/:age\", func(c *gin.Context) { name :=c.Param(\"name\") age := c.Param(\"age\") c.JSON(http.StatusOK, gin.H{ \"name\": name, \"age\" : age, }) }) r.Run(\":9090\") } 这时候访问本机9090端口后的任意符合/name/age这样的路径，那么参数都将被获取到 但是这时候如果访问任何路径都将被匹配，因为匹配是从头开始的，但是如果是不想访问这个层级的路径，那么就可能会被错误匹配到 这时候就需要重写匹配规则 func main() { r := gin.Default() r.GET(\"/user/:name/:age\", func(c *gin.Context) { name :=c.Param(\"name\") age := c.Param(\"age\") c.JSON(http.StatusOK, gin.H{ \"name\": name, \"age\" : age, }) }) r.GET(\"/province/:city/:area\", func(c *gin.Context) { city:=c.Param(\"city\") area := c.Param(\"area\") c.JSON(http.StatusOK, gin.H{ \"city\": city, \"area\": area, }) }) r.Run(\":9090\") } ","date":"2020-06-12","objectID":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/:4:0","tags":["gin","Go"],"title":"初识Gin框架","uri":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/"},{"categories":["Go"],"content":"参数绑定 有时候我们从前端获取到的数据通常是多条，并且需要在程序中使用到这些数据，比如存储到数据库中，那么通常会使用结构体来操作数据，但是如果要获取的字段很多的话，从获取到赋值都是一个很繁琐的过程，使用Gin框架进行参数绑定就会方便很多 json 参数 type UserInfo struct { Name string `json:\"name\"` Age int `json:\"age\"` Sex string `json:\"sex\"` } func main() { r := gin.Default() r.POST(\"/\", func(c *gin.Context) { var userInfo UserInfo err := c.ShouldBindJSON(\u0026userInfo) if err != nil { c.JSON(200, gin.H{\"msg\": err}) return } c.JSON(200, userInfo) }) r.Run(\":9090\") } URL查询参数 type UserInfo struct { Name string `json:\"name\" form:\"name\"` Age int `json:\"age\" form:\"age\"` Sex string `json:\"sex\" form:\"sex\"` } func main() { r := gin.Default() r.POST(\"/query\", func(c *gin.Context) { var userInfo UserInfo err := c.ShouldBindQuery(\u0026userInfo) if err != nil { fmt.Println(err) c.JSON(200, gin.H{\"msg\": err}) return } c.JSON(200, userInfo) }) r.Run(\":9090\") } form 参数 type UserInfo struct { Name string `form:\"name\"` Age int `form:\"age\"` Sex string `form:\"sex\"` } func main() { r := gin.Default() r.POST(\"/form\", func(c *gin.Context) { var userInfo UserInfo err := c.ShouldBind(\u0026userInfo) if err != nil { fmt.Println(err) c.JSON(200, gin.H{\"msg\": \"你错了\"}) return } c.JSON(200, userInfo) }) r.Run(\":9090\") } ","date":"2020-06-12","objectID":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/:5:0","tags":["gin","Go"],"title":"初识Gin框架","uri":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/"},{"categories":["Go"],"content":"路由组 一些情况下可能需要统一前缀的URL需求，比如视频网站的喜剧频道等等，通常涉及到分类的，某一个类别下基本都会有统一的URL前缀，这种情况下使用路由组将会方便很多，Gin 可以使用 Group 方法统一归类到路由组中 func main() { r := gin.Default() group1 := r.Group(\"/user\") { group1.GET(\"/name\", func(c *gin.Context) { c.String(http.StatusOK, \"pujic\") }) group1.GET(\"/age\", func(c *gin.Context) { c.String(http.StatusOK, \"18\") }) } group2 := r.Group(\"/product\") { group2.GET(\"/datetime\", func(c *gin.Context) { c.String(http.StatusOK, \"2020\") }) group2.GET(\"/weight\", func(c *gin.Context) { c.String(http.StatusOK, \"1kg\") }) } r.Run(\":9090\") } ","date":"2020-06-12","objectID":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/:6:0","tags":["gin","Go"],"title":"初识Gin框架","uri":"/posts/%E5%88%9D%E8%AF%86gin%E6%A1%86%E6%9E%B6/"},{"categories":["Go"],"content":"在使用Go语言实现客服端和服务端的时候基本不需要第三方库来完成，使用标准库中的net/http就能够胜任绝大多数的需求，基本不需要自己造轮子，Go语言的标准库够强大，且够灵活。 ","date":"2020-06-11","objectID":"/posts/go%E6%A0%87%E5%87%86%E5%BA%93-net-http/:0:0","tags":["http","Go"],"title":"Go标准库-net/http","uri":"/posts/go%E6%A0%87%E5%87%86%E5%BA%93-net-http/"},{"categories":["Go"],"content":"默认Server 使用Go语言的标准库net/http进行服务端的开发 package main import ( \"fmt\" \"net/http\" ) func sayHello(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"Hello\")) } func main() { http.HandleFunc(\"/hello\", sayHello) err := http.ListenAndServe(\":9090\", nil) if err != nil { fmt.Println(\"http serve failed, error:\", err) } } 首先调用http.HandleFunc，按顺序做了几件事： 调用了DefaultServerMux的HandleFunc 调用了DefaultServerMux的Handle 往DefaultServeMux的map[string]muxEntry中增加对应的handler和路由规则 其次调用http.ListenAndServe(“:9090”, nil)，按顺序做了几件事情： 实例化Server 调用Server的ListenAndServe() 调用net.Listen(“tcp”, addr)监听端口 启动一个for循环，在循环体中Accept请求 对每个请求实例化一个Conn，并且开启一个goroutine为这个请求进行服务go c.serve() 读取每个请求的内容w, err := c.readRequest() 判断header是否为空，如果没有设置handler（这个例子就没有设置handler），handler就设置为DefaultServeMux 调用handler的ServeHttp 在这个例子中，下面就进入到DefaultServerMux.ServeHttp 根据request选择handler，并且进入到这个handler的ServeHTTP mux.handler(r).ServeHTTP(w, r) 选择handler： A 判断是否有路由能满足这个request（循环遍历ServerMux的muxEntry） B 如果有路由满足，调用这个路由handler的ServeHttp C 如果没有路由满足，调用NotFoundHandler的ServeHttp 指定路由执行的方法中需要传递两个参数，w和r，w是http.ResponseWriter类型，需要传递回去的数据需要使用往w中写入。r是http.Request类型的指针，从前端传递过来的数据需要通过r来读取。 ","date":"2020-06-11","objectID":"/posts/go%E6%A0%87%E5%87%86%E5%BA%93-net-http/:1:0","tags":["http","Go"],"title":"Go标准库-net/http","uri":"/posts/go%E6%A0%87%E5%87%86%E5%BA%93-net-http/"},{"categories":["Go"],"content":"自定义Server 要管理服务端的行为，可以创建一个自定义的Server：、 s := \u0026http.Server{ Addr: \":9090\", Handler: myHandler, ReadTimeout: 10 * time.Second, WriteTimeout: 10 * time.Second, MaxHeaderBytes: 1 \u003c\u003c 20, } log.Fatal(s.ListenAndServe()) ","date":"2020-06-11","objectID":"/posts/go%E6%A0%87%E5%87%86%E5%BA%93-net-http/:2:0","tags":["http","Go"],"title":"Go标准库-net/http","uri":"/posts/go%E6%A0%87%E5%87%86%E5%BA%93-net-http/"},{"categories":["Go"],"content":"客户端 客户端也就是http请求 ","date":"2020-06-11","objectID":"/posts/go%E6%A0%87%E5%87%86%E5%BA%93-net-http/:3:0","tags":["http","Go"],"title":"Go标准库-net/http","uri":"/posts/go%E6%A0%87%E5%87%86%E5%BA%93-net-http/"},{"categories":["Go"],"content":"GET请求 package main import ( \"fmt\" \"io/ioutil\" \"net/http\" ) func main() { response, err := http.Get(\"http://www.baidu.com\") if err != nil { } defer response.Body.Close() body, _ := ioutil.ReadAll(response.Body) fmt.Println(string(body)) } response是一个http.Response对象，使用Body方法可以将响应内容读取出来，但是读取出来的数据是byte类型，需要转换。 ","date":"2020-06-11","objectID":"/posts/go%E6%A0%87%E5%87%86%E5%BA%93-net-http/:3:1","tags":["http","Go"],"title":"Go标准库-net/http","uri":"/posts/go%E6%A0%87%E5%87%86%E5%BA%93-net-http/"},{"categories":["Go"],"content":"POST请求 package main import ( \"bytes\" \"encoding/json\" \"fmt\" \"io/ioutil\" \"net/http\" ) func main() { formData := make(map[string]string) formData[\"username\"] = \"pujic\" formData[\"password\"] = \"123456\" bytesData, _ := json.Marshal(formData) response, err := http.Post(\"http://xxxxxx.com\", \"application/json;charset=utf-8\", bytes.NewBuffer([]byte(bytesData))) if err != nil { fmt.Println(\"http error \", err) } defer response.Body.Close() content, err := ioutil.ReadAll(response.Body) if err != nil { fmt.Println(\"read response error \", err) } str := (*string)(unsafe.Pointer(\u0026content)) fmt.Println(*str) } 上面的请求中添加了请求头，是为了告诉服务端我们传过去的数据是什么数据，这个是必须要设置的，这样POST请求的参数才能正常的提交。 ","date":"2020-06-11","objectID":"/posts/go%E6%A0%87%E5%87%86%E5%BA%93-net-http/:3:2","tags":["http","Go"],"title":"Go标准库-net/http","uri":"/posts/go%E6%A0%87%E5%87%86%E5%BA%93-net-http/"},{"categories":["Go"],"content":"使用http.NewRequest发起请求 使用NewRequest发送请求需要先实例化一个请求对象 package main import ( \"net/http\" ) func main() { request, err := http.NewRequest(\"GET\", \"http://www.baidu.com\", nil) request.Header.Set(\"User-agent\", \"Mozilla/5.0\") response, err := http.DefaultClient.Do(request) } 使用NewRequest实例化得到的是一个Request结构体对象，可以给这个Request对象添加请求头等信息，要发送这个请求需要使用http.DefaultClient.Do去完成，也可以使用http.client.Do完成 如果是POST请求那么要提交的表单可以在构造请求对象的时候传递，或者使用请求对象的Body字段来添加（详细方法看源码） req := \u0026Request{ ctx: ctx, Method: method, URL: u, Proto: \"HTTP/1.1\", ProtoMajor: 1, ProtoMinor: 1, Header: make(Header), Body: rc, Host: u.Host, } ","date":"2020-06-11","objectID":"/posts/go%E6%A0%87%E5%87%86%E5%BA%93-net-http/:3:3","tags":["http","Go"],"title":"Go标准库-net/http","uri":"/posts/go%E6%A0%87%E5%87%86%E5%BA%93-net-http/"},{"categories":["Go"],"content":"使用http.Client来请求 import \"net/http\" func main(){ client := http.Client{} response, err := client.Get(\"http://wwww.baidu.com\") } http.Client其实也就是在http.NewRequest上进行封装 func (c *Client) Get(url string) (resp *Response, err error) { req, err := NewRequest(\"GET\", url, nil) if err != nil { return nil, err } return c.Do(req) } ","date":"2020-06-11","objectID":"/posts/go%E6%A0%87%E5%87%86%E5%BA%93-net-http/:3:4","tags":["http","Go"],"title":"Go标准库-net/http","uri":"/posts/go%E6%A0%87%E5%87%86%E5%BA%93-net-http/"},{"categories":["网络爬虫"],"content":"XPath即为XML路径语言（XML Path Language），它是一种用来确定XML文档中某部分位置的语言。 XPath基于XML的树状结构，提供在数据结构树中找寻节点的能力。 ","date":"2020-03-26","objectID":"/posts/xpath%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:0","tags":["xpath","网络爬虫"],"title":"xpath基本语法","uri":"/posts/xpath%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/"},{"categories":["网络爬虫"],"content":"选择结点 语法 描述 示例 示例解释 / 从根节点开始选取 /html 只会从头开始匹配，选择到一个结果，为html节点 // 不考虑位置，选取所有符合条件的结点 //div 不用从头开始匹配，选择所有div结点 . 选取当前结点 ./div 选择当前结点下的div结点（子节点中的div结点） .. 选取当前结点的父节点 ../div 选择当前结点的父节点 ","date":"2020-03-26","objectID":"/posts/xpath%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:1","tags":["xpath","网络爬虫"],"title":"xpath基本语法","uri":"/posts/xpath%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/"},{"categories":["网络爬虫"],"content":"选择属性 使用@符号来选择属性 示例 示例解释 //div[@id] 选择所有属性为id的所有div结点 //@class 选择属性为class的所有结点 //div[@class=“name”] 选择class属性为name的所有div结点 ","date":"2020-03-26","objectID":"/posts/xpath%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:2","tags":["xpath","网络爬虫"],"title":"xpath基本语法","uri":"/posts/xpath%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/"},{"categories":["网络爬虫"],"content":"指定结点 使用[]来指定是第几个结点 示例 示例解释 //tbody/tr[3] 选择所有tbody结点下的第3个tr结点 //tbody/tr[last()] 选择所有tbody结点下的最后一个tr结点 //tbody/tr[position()\u003c3] 选择所有tbody结点下位置小于3的tr结点 ","date":"2020-03-26","objectID":"/posts/xpath%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:3","tags":["xpath","网络爬虫"],"title":"xpath基本语法","uri":"/posts/xpath%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/"},{"categories":["网络爬虫"],"content":"内容选取 示例 示例解释 //img/@src 取出所有img标签的src属性值 //p/text() 取出所有p标签包裹的所有文本内容 ","date":"2020-03-26","objectID":"/posts/xpath%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:4","tags":["xpath","网络爬虫"],"title":"xpath基本语法","uri":"/posts/xpath%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/"},{"categories":["环境配置"],"content":"采用解压压缩文件的安装方式安装Windows版本的MySQL！ ","date":"2020-03-14","objectID":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/:0:0","tags":["MySQL","install","navicat"],"title":"MySQL-8.0.19安装配置","uri":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"下载MySQL ","date":"2020-03-14","objectID":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/:1:0","tags":["MySQL","install","navicat"],"title":"MySQL-8.0.19安装配置","uri":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"官网下载 MySQL官方网站主页：https://www.mysql.com/ 将页面拉到最下方，选择**DOWNLOADS**这一栏下的MySQL Community Server 就进入到下载页面 选择箭头所指的一览下载的是ZIP文件，选择MySQL logo下的Go to Download Page下载的是exe的安装包，点击后会跳转到Oracle账号登录/注册页面，也可以点击No thanks,just start my download跳过登录直接下载。 在官网下载MySQL的速度不是很理想，建议使用国内镜像。 ","date":"2020-03-14","objectID":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/:1:1","tags":["MySQL","install","navicat"],"title":"MySQL-8.0.19安装配置","uri":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"清华大学开源软件镜像站 MySQL-8.0版本目录链接：https://mirrors.tuna.tsinghua.edu.cn/mysql/downloads/MySQL-8.0/ 可以直接点击上面的链接进行下载，也可以一步一步来 进入清华大学开源镜像站首页https://mirrors.tuna.tsinghua.edu.cn/，在镜像列表中找到mysql这一项，点击进入 然后选择downloads这一级 进入目录后选择MySQL-8.0，有更新的也可以选更新的，安装方式应该相同 进入后，8.0版本的项目超级多，Ctrl+F搜索winx64.zip就能快速定位 如果搜索winx64.msi下载的就是exe安装包 下载的文件解压后有5个文件夹和２个文件 ","date":"2020-03-14","objectID":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/:1:2","tags":["MySQL","install","navicat"],"title":"MySQL-8.0.19安装配置","uri":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"配置MySQL 在配置之前，首先要将文件夹移动到要存放这个程序的位置，这一步很重要！ 然后可以给这个文件夹重命名一下，比如我这里叫mysql，当然也可以不重命名！ 将bin目录添加到环境变量中 ","date":"2020-03-14","objectID":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/:2:0","tags":["MySQL","install","navicat"],"title":"MySQL-8.0.19安装配置","uri":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"创建配置文件 在解压后的目录下创建一个配置文件my.ini 然后将一下信息写入配置文件中 [mysql] # 设置mysql客户端默认字符集 default-character-set=utf8 [mysqld] #设置3306端口 port = 3306 # 设置mysql的安装目录 basedir=Z:/soft/mysql # 设置mysql数据库的数据的存放目录 datadir=Z:/soft/mysql/data # 允许最大连接数 max_connections=200 # 服务端使用的字符集默认为UTF8 character-set-server=utf8 # 创建新表时将使用的默认存储引擎 default-storage-engine=INNODB # 允许连接失败的次数 max_connect_errors=10 # 默认使用“mysql_native_password”插件认证 default_authentication_plugin=mysql_native_password ","date":"2020-03-14","objectID":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/:2:1","tags":["MySQL","install","navicat"],"title":"MySQL-8.0.19安装配置","uri":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"安装MySQL服务 以管理员的身份运行cmd，进入到MySQL程序文件夹中的bin文件夹下面，然后执行命令 mysqld install 如果这时候弹出 那么需要到微软官网下载 Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019，链接：https://support.microsoft.com/en-gb/help/2977003/the-latest-supported-visual-c-downloads 电脑是32位就选x86，是64位就选x64，然后点击Next开始下载，下载完成后双击安装即可。 因为不同的版本的依赖程序的版本可能不同，所以推荐直接下载这个版本，如果还是无法解决，可以尝试安装2013版本。 安装成功后会提示Service successfully installed 执行下面的命令 mysqld --initialize --console 执行这个命令是使MySQL程序按照配置文件进行配置，命令执行完成之后会在程序文件夹中生成一个data文件夹，data文件夹的位置就在配置文件中的datadir后面的路径下。并且会随机生成登录密码！，这个密码在最后一行冒号后面，不包含空格 这里的随机密码就是aFV4%y2.3B+5 如果窗口不小心被关闭，密码丢失了，可以将data文件夹删除，然后重新使用mysqld --initialize --console命令来生成。 这些都做好后，就可以启动MySQL服务了 net start MySQL ","date":"2020-03-14","objectID":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/:2:2","tags":["MySQL","install","navicat"],"title":"MySQL-8.0.19安装配置","uri":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"修改密码 MySQL服务启动后，就可以使用MySQL服务了 使用命令登录MySQL系统 mysql -u root -p 然后会提示Enter password输入密码，将刚才临时生成的密码输入进去 然后输入SQL命令 ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY '你的密码'; 结尾一定要是分号，如果没有输入分号就回车也没关系，下一行输入一个分号就好了 我这里就是将密码设置为空，就是不设置密码，直接登录，登录时就不用输入-p，有密码就需要输入-p 然后输入exit命令就可以退出MySQL系统。 ","date":"2020-03-14","objectID":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/:3:0","tags":["MySQL","install","navicat"],"title":"MySQL-8.0.19安装配置","uri":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"可视化工具 ","date":"2020-03-14","objectID":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/:4:0","tags":["MySQL","install","navicat"],"title":"MySQL-8.0.19安装配置","uri":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"Navicat Premium Navicat Premium 功能十分强大，支持的数据库类型非常多，不过是一款收费软件，可以试用14天，既然可以试用，那么就可以无限试用。 下载链接：https://www.navicat.com.cn/products#navicat ","date":"2020-03-14","objectID":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/:4:1","tags":["MySQL","install","navicat"],"title":"MySQL-8.0.19安装配置","uri":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"HeidiSQL HeidiSQL是一款比较好用的数据库可视化软件，易用，而且免费，可以在微软应用商店下载，下载速度很快，对！就是Windows系统自带的应用商店！也可以到官网下载。 下载地址：https://www.heidisql.com/download.php ","date":"2020-03-14","objectID":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/:4:2","tags":["MySQL","install","navicat"],"title":"MySQL-8.0.19安装配置","uri":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"Jet Brains全家桶 只要是Jet Brains的IDE那么就可以连接MySQL，而且主流的数据库基本都支持，使用起来确实很方便，在写程序的时候就不用为了查看数据库单独开一个可视化软件了。 首次连接需要下载驱动，在连接配置页面的左下角Download missing driver files 如果连接失败并提示：Server returns invalid timezone. Go to 'Advanced' tab and set 'serverTimezone' property manually. 这是时区错误，MySQL默认的时区是UTC时区，比北京时间晚8个小时。 所以要修改mysql的时长 在mysql的命令模式下，输入： set global time_zone='+8:00'; 再次连接应该就成功了。 ","date":"2020-03-14","objectID":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/:4:3","tags":["MySQL","install","navicat"],"title":"MySQL-8.0.19安装配置","uri":"/posts/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"采用解压压缩文件的方式安装Windows版本的MongoDB！ 在MongoDB的官网有两种安装MongoDB的安装包可供下载，一种是MSI文件，一种是ZIP，MIS文件简单说就是Windows的安装包文件。如果使用MIS文件安装，就按步骤在图形界面中往下配置即可，安装时间比较长，而且也需要进行一些文件的配置，显得不是很方便；如果使用ZIP格式文件安装，那么只需要将安装包下载后解压至指定的目录，然后进行文件配置即可，这就省掉了MSI文件安装的释放文件的时间，而且相对而言更加灵活一些。 ","date":"2020-03-14","objectID":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/:0:0","tags":["MongoDB","Robo 3T","install"],"title":"MongoDB的安装以及配置","uri":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"下载安装包 进入MongoDB官网 社区版下载地址链接：https://www.mongodb.com/download-center/community ","date":"2020-03-14","objectID":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/:1:0","tags":["MongoDB","Robo 3T","install"],"title":"MongoDB的安装以及配置","uri":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"版本选择 Version(版本)：current release(当前版本)，development release(开发版本)，previous release(先前版本) 版本选择当前版本，有特殊需要可以选择先前版本，不建议选开发版本 ","date":"2020-03-14","objectID":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/:1:1","tags":["MongoDB","Robo 3T","install"],"title":"MongoDB的安装以及配置","uri":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"系统选择 系统有很多种，但只有三类，Windows、Mac OS、Linux，这里选择Windows ","date":"2020-03-14","objectID":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/:1:2","tags":["MongoDB","Robo 3T","install"],"title":"MongoDB的安装以及配置","uri":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"安装包选择 Package：MSI、ZIP 安装包选择ZIP 上面三个选项选择好之后，点击Download就可以下载了，至于跳转出来的网页可以不必理会。下载速度还是比较快的。 ","date":"2020-03-14","objectID":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/:1:3","tags":["MongoDB","Robo 3T","install"],"title":"MongoDB的安装以及配置","uri":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"安装配置 ","date":"2020-03-14","objectID":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/:2:0","tags":["MongoDB","Robo 3T","install"],"title":"MongoDB的安装以及配置","uri":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"解压文件 将下载后的文件解出来，放到一个文件夹中，一共有6个文件 这个文件夹名称最好修改为mongondb，然后放到想存放的位置，当然他们是要放在一起的。 ","date":"2020-03-14","objectID":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/:2:1","tags":["MongoDB","Robo 3T","install"],"title":"MongoDB的安装以及配置","uri":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"配置 创建一个data文件夹，然后在data文件夹下面创建一个db文件夹，还需要创建一个log文件夹，然后在log文件夹中创建一个mongo.log的文件。data文件夹是数据库存放数据的文件夹，log文件夹是数据库存放日志的文件夹，这两个文件夹放哪看个人习惯，他们的目录结构是这样的 将bin文件夹所在路径添加到环境变量中 在终端中输入 mongod --dbpath C:\\mongodb\\data\\db 然后在命令提示符窗口会出现很多日志信息 打开浏览器，输入本地端口 localhost:27017 然后浏览器跳转页面，就会出现下面的提示 It looks like you trying to access MongoDB over HTTP on the native driver port ","date":"2020-03-14","objectID":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/:2:2","tags":["MongoDB","Robo 3T","install"],"title":"MongoDB的安装以及配置","uri":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"注册服务 因为每次启动都指定db的路径比较麻烦，所以可以直接将配置写在文件中 在bin文件夹的同级目录下创建配置文件mongo.config，然后在配置文件中写入以下信息 不需要更详细的配置只写入这两项即可，如果想要更详细的配置MongoDB可以参照官方文档 systemLog: destination: file path: \"C:\\mongodb\\log\\mongo.log\" logAppend: true storage: journal: enabled: true dbPath: \"C:\\mongodb\\data\\db\" processManagement: fork: true net: bindIp: 127.0.0.1 port: 27017 setParameter: enableLocalhostAuthBypass: false 如果要让别人在局域网内能访问到数据库需要将bindIp设为0.0.0.0或者是允许的ip 配置文件修改完成后，以管理员的身份运行命令提示符 在命令提示符中输入注册服务的命令 mongod --config C:\\mongodb\\mongo.config --install --serviceName \"MongoDB\" 然后输入启动服务的命令 net start MongoDB 这样MongoDB本地服务就启动了 在浏览器输入localhost:27017上一个步骤中的提示页面就说明没有问题了。 ","date":"2020-03-14","objectID":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/:3:0","tags":["MongoDB","Robo 3T","install"],"title":"MongoDB的安装以及配置","uri":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"移除服务 如果注册服务之后启动不了MongoDB，也没有关系，先将MongoDB的服务移除，然后再重新注册服务，移除服务需要输入以下命令 mongod --dbpath C:\\mongodb\\data\\db --logpath c:\\mongodb\\log\\mongo.log --logappend --serviceNameDisplayName \"MongoDB\" --remove 移除之后再使用上面的注册命令注册就基本没问题了。 ","date":"2020-03-14","objectID":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/:3:1","tags":["MongoDB","Robo 3T","install"],"title":"MongoDB的安装以及配置","uri":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/"},{"categories":["环境配置"],"content":"数据库可视化软件 可以使用的数据库可视化软件有很多，这里介绍Robo 3T，用就完事了。 Robo 3T官方网站：https://robomongo.org/ 选择Download Studio 3T Only，然后在跳转出来的下载页面点击Download Robo 3T 安装包还是选择zip，解压出来就能用！ 下载完成后，解压文件，放到想存放的位置，然后双击文件夹中的robo3t.exe文件，程序就启动了，如果觉得每次都要打开文件夹比较麻烦，可以给它创建一个桌面快捷方式。 一般情况下，只要MongoDB服务启动了，那么打开Robo 3T就是连接界面。如果没有本地端口，就点击Create进行创建 填好连接的名称（没有固定要求），以及地址就好了。 ","date":"2020-03-14","objectID":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/:4:0","tags":["MongoDB","Robo 3T","install"],"title":"MongoDB的安装以及配置","uri":"/posts/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E9%85%8D%E7%BD%AE/"},{"categories":["网络爬虫"],"content":"Scrapy的中文文档似乎也和许多文档一样，在中文翻译上、内容上以及版本上存在很多的问题 ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:0:0","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"命令 ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:1:0","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"创建项目 进入到项目存放的目录使用命令提示符创建 scrapy startproject 项目名称 ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:1:1","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"使用命令创建一个爬虫 进入到这个项目的目录中，使用命令提示符创建爬虫 scapy genspider 爬虫名字 域名 爬虫的名字不能和项目的名字相同 ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:1:2","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"查看模板 如果不指定模板就会根据basic模板来生成爬虫 scrapy genspider --list ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:1:3","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"指定模板 scrapy genspider -t crawl 爬虫名字 域名 ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:1:4","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"下载图片 scrapy中提供了自动下载图片的机制，只需要在settings中配置好就可以了，在项目文件的setting下的ITEM_PIPELINES中添加一个键 ITEM_PIPELINES = { 'BingPicture.pipelines.BingpicturePipeline': 300, 'scrapy.pipelines.images.ImagesPipeline': 1, } IMAGES_URLS_FIELD = \"图片url字段\" IMAGES_STORE = \"图片存储路径\" 注意： 需要安装pillow这个库 下载图片传递的url需要是一个列表（item[\"img_url\"] = [url]），这个列表中可以存放很多个urls ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:2:0","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"限定下载图片大小 如果想过滤掉小图片需要在settings中配置 IMAGES_MIN_HEIGHT = 高度 IMAGES_MIN_WIDTH = 宽度 ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:2:1","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"获取图片路径 想要知道图片的路径和名称需要在Pipelines中重写ImagesPipeline中的item_completed方法 自定义的这个Pipeline类需要在settings中添加，这时候就不需要写原生的ImagesPipeline了，因为这个Pipeline只重写了item_completed方法，其他的方法会去下载图片。 class SpiderPipeline(ImagesPipeline): # 重载item_completed方法 def item_completed(self, results, item, info): return item results:是一个list每一个元素都是一个tuple，tuple中第一个值是true，表示成功；第二个值是一个字典，字典中的path是文件保存路径，字典中的url是下载的url。 这时候就可以直接使用item将图片的地址保存下来，然后需要将item返回 ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:2:2","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"拼接URL response对象有urljoin方法，可以将域名与超链接进行拼接 url = response.urljoin(other_url) ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:3:0","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"编写爬虫的注意事项 可以在这个项目中创建一个开始执行的程序 from scrapy import cmdline cmdline.execute('scrapy crawl 爬虫名称'.split()) ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:4:0","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"文件存储 文件存储需要在Pipelines中进行，可以直接操作模板生成的Pipeline，也可以自定义Pipline，如果自定义需要将自定义Pipeline的优先级设置得比模板高，也就settings中Pipeline对应的键值更小 scrapy的exporters中提供了很多种文件存储机制 __all__ = ['BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter', 'CsvItemExporter', 'XmlItemExporter', 'JsonLinesItemExporter', 'JsonItemExporter', 'MarshalItemExporter'] 可以直接调用这些类然后方便的写入文件，下面以JsonLinesItemExporter为例 class JsonExporterPipeline(object): def __init__(self): self.file = open(\"images_message.json\", \"wb\") # 将ensure_ascii设置为false，否则会存储为Unicode编码 self.exporter = JsonLinesItemExporter(self.file, encoding=\"utf8\", ensure_ascii=False) def process_item(self, item, spider): self.exporter.export_item(item) return item def close_spider(self, spider): self.file.close() Pipline类中有两个信号量方法，一个是open_spider方法，open_spider方法爬虫开始会执行；另一个时close_spider方法，close_spider方法爬虫结束时会执行，他们都需要传递一个spider参数，他们的执行是通过scrapy中的信号量来控制的 ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:5:0","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"在Pipeline中使用settings中的配置 在Pipeline中使用settings中的配置可以不需要导入settings中的配置，定义一个from_settings(cls, settings)类方法，这个方法会将settings中的所有配置以字典的形式传递过来，只需要按照settings中的配置取键 @classmethod def from_settings(cls, settings): host = settings[\"MONGO_HOST\"] ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:5:1","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"scrapy异步写入MySQL 在scrapy中异步写入MySQL from twisted.enterprise import adbapi class MySQLtwistedPipeline(object): def __init__(self, dbpool): self.dbpool = dbpool @classmethod def from_settings(cls, settings): dbparms = dict( host = settings[\"MYSQL_HOST\"], port = settings[\"MYSQL_PORT\"], user = settings[\"MYSQL_USER\"], password = settings[\"MYSQL_PASSWORD\"], database = settings[\"MYSQL_DATABASE\"], charset = \"utf8\", cursorclass = pymysql.cursors.DictCursor, use_unicode = True ) dbpool = adbapi.ConnectionPool(\"pymysql\", **dbparms) # 使用这个类来实例化连接池 return cls(dbpool) def process_item(self, item, spider): # 建立连接池，将插入变成异步的 qurey = self.dbpool.runInteraction(self.insert, item) qurey.addErrback(self.handler_error, item, spider) def handler_error(self, failure, item, spider): # 处理异步插入的异常 print(failure) def insert(self, cursor, item): # 连接池会自动帮助提交 insert_sql = \"\"\"insert into bing_picture(img_url, img_id, img_file_path) values (%s, %s, %s)\"\"\" cursor.execute(insert_sql, (item[\"img_url\"][0], item[\"img_id\"], item[\"img_file_path\"])) from_settings方法中定义的是链接信息，在process_item方法中定义的是当item传递过来后运行这个连接池，然后这个连接池调用insert方法来插入数据。 因为整个插入过程是异步的，所以难免会发生异常，运行连接池执行方法后会返回一个对象，用一个变量来保存这个对象，这个对象有一个错误回调方法addErrback，这个方法中的item参数和spider参数可以不用传递（不传递那么回调方法中的item和spider就不用写），整个错误处理逻辑放到回调方法中来处理 注意： 需要单独导入cursors对象，即import pymysql.cursors ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:6:0","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"item loaders scrapy中的item使用起来和简单，他就是一个数据容器，可以存储任意类型的数据，提供了类似字典的API，操作起来很方便，但是在parser方法中解析的数据想要存储到item中还需要赋值，这时候代码写起来就比较混乱 Item Loaders使用起来就很方便，集解析赋值于一身 from scrapy.loader import ItemLoader from myproject.items import ProductItem def parse(self, response): item_loader = ItemLoader(item=ProductItem(), response=response) item_loader.add_xpath('name', '//div[@class=\"product_name\"]') item_loader.add_xpath('title', '//div[@class=\"product_title\"]') item_loader.add_xpath('price', '//p[@id=\"price\"]') item_loader.add_css('stock', 'p#stock') item_loader.add_value('last_updated', 'today') return item_loader.load_item() add_value这个方法需要两个参数，第一个参数就是字段名，第二个参数就是字段的值，使用这个方法能直接设定字段值。add_css和add_xpath就是按照规则解析response。 最后需要调用load_item()方法才会去执行上面的解析规则。 这样得到的每个字段值都是一个列表，而且有一些字段我们可能并没有解析完，还想在上面加一些处理逻辑，这时候我们就需要到items中去设置Field()的input_processor参数，使用MapCompose来传递方法，当数据传递到这里的时候我们就可以通过方法对value进行处理，可以传递任意多的方法，input_processor参数是对value进行操作，但是现在这value还是一个list，那么就需要导入TakeFirst，然后传递给第二个参数output_processor，这时候就会取第一个值。 from scrapy.loader.processors import MapCompose, TakeFirst def add_name(value): return value + \"-product\" class ProductItem(scrapy.Item): name = scrapy.Field( input_processor = MapCompose(add_name, lambad x: x+\"-hello\"), output_processor = TakeFirst() ) title = scrapy.Field() price = scrapy.Field() stock = scrapy.Field() last_updated = scrapy.Field() value参数不需要我们去传递，MapCompose甚至可以传递lambda表达式，方法传递多个程序会按照顺序自动去执行这些方法。 ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:7:0","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"自定义itemloader 如果我们爬取的字段很多，有上百个，那么在原来的item中我们就需要给每个字段都传递output_processor参数，这样程序就会显得很乱。 我们可以自己定义一个itemloader类，让自定义的itemloader继承自scrapy中的ItemLoader，ItemLoader中有一个类属性default_output_processor，重写这个类属性 from scrapy.loader import ItemLoader from scrapy.loader.processors import MapCompose, TakeFirst def add_name(value): return value + \"-product\" class ProductItem(scrapy.Item): name = scrapy.Field( input_processor = MapCompose(add_name, lambad x: x+\"-hello\"), ) title = scrapy.Field() price = scrapy.Field() stock = scrapy.Field() last_updated = scrapy.Field() class ProductItemLoader(ItemLoader): default_output_processor = TakeFirst() 然后在解析方法中实例化item_loader的时候就需要使用这里重写的这个ProductItemLoader。 实际上在ItemLoader有四个类属性，分别是： default_item_class = Item default_input_processor = Identity() default_output_processor = Identity() default_selector_class = Selector 因此我们可以更改其中的参数可以使解析变得更加高效。 如果出现列表中的多个内容都是我们想要的，而且我们想将它们连接成一个字符串，这个逻辑其实不需要通过定义方法来处理value，只需要在地段的定义中重新传递output_processor参数就可以了 from scrapy.loader import ItemLoader from scrapy.loader.processors import MapCompose, TakeFirst, Join class ProductItem(scrapy.Item): title = scrapy.Field() tags = scrapy.Field( output_processor = Join(\",\") ) last_updated = scrapy.Field() class ProductItemLoader(ItemLoader): default_output_processor = TakeFirst() 如果我们已经重写了ItemLoader，但是我们想要的字段任然是列表，那么我们就需要在字段中覆盖output_processor def return_value(value): return value class ProductItem(scrapy.Item): tags = scrapy.Field( output_processor = MapCompose(return_value) ) ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:7:1","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"回调 在scrapy的爬虫中，如果不写回调函数，那么通过请求返回的页面默认是调用parse方法 ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:8:0","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"FormRequest 如果在爬虫最开始的时通过post登录到某一个页面，那么可以重写start_requests方法。如果需要提交表单，那么可以不使用scrapy.Request方法，可以使用scrapy.FormRequest，这个方法，然后可以设置回调函数为一个验证是否登录成功的方法。 def start_requests(self): return [scrapy.FormRequest(\"http://www.example.com/login\", formdata={'user': 'john', 'pass': 'secret'}, headers=self.headers callback=self.logged_in)] def logged_in(self, response): pass 那么登录成功之后呢？如果验证了已经登录成功，想要按照start_urls中的url进行爬取，那么可以将原来的start_requests中的逻辑放置在logged_in中，原来的start_requests逻辑如下 def start_requests(self): for url in self.start_urls: yield self.make_requests_from_url(url) 将这里面的逻辑放入到logged_in，那么就可以实现接下来的爬取任务。 ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:9:0","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"scrapy shell 使用scrapy shell 命令可以进入scrapy进入scrapy交互环境，可以在后面加上url，也可以不加 scrapy shell [url] # 添加User-agent scrapy shell -s USER_AGENT=\"u-a\" url 进入scrapy shell后可以使用交互式命令，可以在里面调试爬虫 使用shelp()命令可以查看scrapy自带的一些命令 [s] crawler \u003cscrapy.crawler.Crawler object at 0x000002392B95D588\u003e [s] item {} [s] settings \u003cscrapy.settings.Settings object at 0x000002392C3F4EC8\u003e [s] Useful shortcuts: [s] fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed) [s] fetch(req) Fetch a scrapy.Request and update local objects [s] shelp() Shell help (print this help) [s] view(response) View response in a browser 注意：如果是在scrapy项目中使用scrapy shell，那么项目配置将会是当前scrapy sehll的配置 函数 功能 fetch() 传入一个url或者scrapy.Request对象，那么就能请求该url，请求成功后会对当前作用域内的request和response对象重新赋值 view 用浏览器打开response对象内的网页 settings 保存所有设置信息的Settings对象 在scrapy shell中可以直接对Response对象使用xpath或者css选择器 ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:10:0","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"CrawlSpider CrawlSpider继承自Spider 要使用CrawlSpider就需要在生成爬虫的时候指定模板。 import scrapy from scrapy.spiders import CrawlSpider, Rule from scrapy.linkextractors import LinkExtractor class MySpider(CrawlSpider): name = 'example.com' allowed_domains = ['example.com'] start_urls = ['http://www.example.com'] rules = ( Rule(LinkExtractor(allow=('category\\.php', ), deny=('subsection\\.php', ))), Rule(LinkExtractor(allow=('item\\.php', )), callback='parse_item'), ) def parse_item(self, response): item = scrapy.Item() item['id'] = response.xpath('//td[@id=\"item_id\"]/text()').re(r'ID: (\\d+)') item['name'] = response.xpath('//td[@id=\"item_name\"]/text()').get() item['description'] = response.xpath('//td[@id=\"item_description\"]/text()').get() item['link_text'] = response.meta['link_text'] return item CrawlSpider也会先爬取start_urls中的链接，然后解析请求到的页面中的url，这时候就需要我们去指定规则，只要满足规则的url，就会进行请求，然后执行回调方法。 rules中的callback的方法必须是字符串，因为这个是一个实例化的过程，Rule自身是没有方法的，使用函数的方式是不能完成传递的，所以只能通过字符串传递这个方法的名称 ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:11:0","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"parse方法 Crawl生成的模板中我们不能去重载parse方法，因为parse方法是继承自Spider中的，在CrawlSpider中已经重载了parse方法，如果改了parse方法整个程序就会报错。 CrawlSpider中的parse方法回调函数是parse_start_url，parse_start_url的值返回之后会传递给process_results，CrawlSpider中这两个方法默认都是返回空列表，因此我们可以对这两个方法进行重载。 ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:11:1","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"Rule class Rule(): def __init__(link_extractor=None, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None, errback=None): link_extractor 就是一个具体的Link Extractor对象，它定义了如何从爬取到的页面中提取url callback，如果有规则满足这个url就会执行回调函数 cb_kwargs为传递给link_extractor的参数 follow的值为True或False，表示是否对满足条件的url进行跟进，也就是向下爬取 process_links ， 从link_extractor中获取到链接列表时将会调用该函数。该方法主要用来过滤url process_request，该规则提取到每个request时都会调用该函数。该函数必须返回一个request或者None。 ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:11:2","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"LinkExtractor LinkExtractor实际上是LxmlLinkExtractor from .lxmlhtml import LxmlLinkExtractor as LinkExtractor class LxmlLinkExtractor(FilteringLinkExtractor): def __init__(allow=(), deny=(), allow_domains=(), deny_domains=(), deny_extensions=None, restrict_xpaths=(), restrict_css=(), tags=('a', 'area'), attrs=('href', ), canonicalize=False, unique=True, process_value=None, strip=True): allow中是一个正则表达式，只要url满足正则表达式的格式就进行提取 deny中也是正则表达式，但是满足条件的url就会被丢弃掉 allow_domains，满足条件的域名就不做处理 restrict_xpaths进一步去限定url，只会从xpath中提取到的内容去提取url ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:11:3","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["网络爬虫"],"content":"Item scrapy中的Item是我们用来保存字段的类，需要保存的字段我们需要提前定义，scrapy.Item继承自scrapy中的DictItem，DictItem继承于MutableMapping, BaseItem，scrapy中的Field继承自dict，scrapy中关于Field的描述是Container of field metadata ","date":"2020-03-14","objectID":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/:11:4","tags":["Scrapy","Python","网络爬虫"],"title":"Scrapy使用笔记","uri":"/posts/scrapy%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"},{"categories":["环境配置"],"content":"若网络不好无法自行下载文件，可以直接使用文末的百度网盘链接下载 ","date":"2020-03-13","objectID":"/posts/%E4%BD%BF%E7%94%A8vscode-mingw%E9%85%8D%E7%BD%AEc%E8%AF%AD%E8%A8%80%E7%9A%84%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/:0:0","tags":["教程","C语言","VScode"],"title":"使用VScode+MinGW配置C语言的开发环境","uri":"/posts/%E4%BD%BF%E7%94%A8vscode-mingw%E9%85%8D%E7%BD%AEc%E8%AF%AD%E8%A8%80%E7%9A%84%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/"},{"categories":["环境配置"],"content":"下载GCC GCC需要到SOURCEFORGE下载，可以直接在SOURCEFORGE官网的搜索框中搜索MinGW MinGW - Minimalist GNU for Windows下载的是mingw-get-setup.exe，这个软件是一个安装器，启动之后还需要从网上拉取GCC的程序文件，如果网络不好很容易失败。 这里介绍的是从MinGW-w64 - for 32 and 64 bit Windows下载MinGW-w64的免安装版（纯净压缩文件），进入项目界面后选择Files 然后往下拉就会看到各个版本的GCC 可以选择适合的版本中的x86_64-win32-seh下载 下载之后直接解压（压缩格式是7z，可以下载7zip来解压，文末放上zip格式）目录结构如下 +--- mingw64 | +--- bin | +--- etc | +--- incloud | +--- lib | +--- libexec | +--- licenses | +--- opt | +--- share | +--- x86_64-w64-mingw32 | +--- build-info.txt 将整个mingw64文件夹放到合适的位置（正常安装软件时软件相关文件存放的位置），将bin目录添加到环境变量中 然后在终端中输入 gcc -v 输出如下信息（因为版本以及文件存储位置可能会存在差别）说明安装成功 Using built-in specs. COLLECT_GCC=gcc COLLECT_LTO_WRAPPER=Z:/soft/mingw64/bin/../libexec/gcc/x86_64-w64-mingw32/8.1.0/lto-wrapper.exe Target: x86_64-w64-mingw32 Configured with: ../../../src/gcc-8.1.0/configure --host=x86_64-w64-mingw32 --build=x86_64-w64-mingw32 --target=x86_64-w64-mingw32 --prefix=/mingw64 --with-sysroot=/c/mingw810/x86_64-810-win32-seh-rt_v6-rev0/mingw64 --enable-shared --enable-static --disable-multilib --enable-languages=c,c++,fortran,lto --enable-libstdcxx-time=yes --enable-threads=win32 --enable-libgomp --enable-libatomic --enable-lto --enable-graphite --enable-checking=release --enable-fully-dynamic-string --enable-version-specific-runtime-libs --disable-libstdcxx-pch --disable-libstdcxx-debug --enable-bootstrap --disable-rpath --disable-win32-registry --disable-nls --disable-werror --disable-symvers --with-gnu-as --with-gnu-ld --with-arch=nocona --with-tune=core2 --with-libiconv --with-system-zlib --with-gmp=/c/mingw810/prerequisites/x86_64-w64-mingw32-static --with-mpfr=/c/mingw810/prerequisites/x86_64-w64-mingw32-static --with-mpc=/c/mingw810/prerequisites/x86_64-w64-mingw32-static --with-isl=/c/mingw810/prerequisites/x86_64-w64-mingw32-static --with-pkgversion='x86_64-win32-seh-rev0, Built by MinGW-W64 project' --with-bugurl=https://sourceforge.net/projects/mingw-w64 CFLAGS='-O2 -pipe -fno-ident -I/c/mingw810/x86_64-810-win32-seh-rt_v6-rev0/mingw64/opt/include -I/c/mingw810/prerequisites/x86_64-zlib-static/include -I/c/mingw810/prerequisites/x86_64-w64-mingw32-static/include' CXXFLAGS='-O2 -pipe -fno-ident -I/c/mingw810/x86_64-810-win32-seh-rt_v6-rev0/mingw64/opt/include -I/c/mingw810/prerequisites/x86_64-zlib-static/include -I/c/mingw810/prerequisites/x86_64-w64-mingw32-static/include' CPPFLAGS=' -I/c/mingw810/x86_64-810-win32-seh-rt_v6-rev0/mingw64/opt/include -I/c/mingw810/prerequisites/x86_64-zlib-static/include -I/c/mingw810/prerequisites/x86_64-w64-mingw32-static/include' LDFLAGS='-pipe -fno-ident -L/c/mingw810/x86_64-810-win32-seh-rt_v6-rev0/mingw64/opt/lib -L/c/mingw810/prerequisites/x86_64-zlib-static/lib -L/c/mingw810/prerequisites/x86_64-w64-mingw32-static/lib ' Thread model: win32 gcc version 8.1.0 (x86_64-win32-seh-rev0, Built by MinGW-W64 project) ","date":"2020-03-13","objectID":"/posts/%E4%BD%BF%E7%94%A8vscode-mingw%E9%85%8D%E7%BD%AEc%E8%AF%AD%E8%A8%80%E7%9A%84%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/:1:0","tags":["教程","C语言","VScode"],"title":"使用VScode+MinGW配置C语言的开发环境","uri":"/posts/%E4%BD%BF%E7%94%A8vscode-mingw%E9%85%8D%E7%BD%AEc%E8%AF%AD%E8%A8%80%E7%9A%84%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/"},{"categories":["环境配置"],"content":"配置VS code VS code官网：https://code.visualstudio.com/ 下载好VS code后直接安装 在VS code中需要安装两个插件（如果不知道插件在哪下载安装可以按快捷键Ctrl+Shift+X） ","date":"2020-03-13","objectID":"/posts/%E4%BD%BF%E7%94%A8vscode-mingw%E9%85%8D%E7%BD%AEc%E8%AF%AD%E8%A8%80%E7%9A%84%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/:2:0","tags":["教程","C语言","VScode"],"title":"使用VScode+MinGW配置C语言的开发环境","uri":"/posts/%E4%BD%BF%E7%94%A8vscode-mingw%E9%85%8D%E7%BD%AEc%E8%AF%AD%E8%A8%80%E7%9A%84%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/"},{"categories":["环境配置"],"content":"C/C++ ","date":"2020-03-13","objectID":"/posts/%E4%BD%BF%E7%94%A8vscode-mingw%E9%85%8D%E7%BD%AEc%E8%AF%AD%E8%A8%80%E7%9A%84%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/:2:1","tags":["教程","C语言","VScode"],"title":"使用VScode+MinGW配置C语言的开发环境","uri":"/posts/%E4%BD%BF%E7%94%A8vscode-mingw%E9%85%8D%E7%BD%AEc%E8%AF%AD%E8%A8%80%E7%9A%84%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/"},{"categories":["环境配置"],"content":"Code Runner 安装好Code Runner之后需要到设置中将Run In Terminal这一项勾选上（Ctrl + ,可以直接跳转到设置界面，如果找不到的Run In Terminal可以在搜索框中搜索） 然后就可以在VS code中运行编写的C语言或者C++程序了，只需要建立对应后缀名的文件就可以了 ","date":"2020-03-13","objectID":"/posts/%E4%BD%BF%E7%94%A8vscode-mingw%E9%85%8D%E7%BD%AEc%E8%AF%AD%E8%A8%80%E7%9A%84%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/:2:2","tags":["教程","C语言","VScode"],"title":"使用VScode+MinGW配置C语言的开发环境","uri":"/posts/%E4%BD%BF%E7%94%A8vscode-mingw%E9%85%8D%E7%BD%AEc%E8%AF%AD%E8%A8%80%E7%9A%84%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/"},{"categories":["环境配置"],"content":"MinGW64百度网盘下载地址 链接：https://pan.baidu.com/s/1tijtgm8BBz2dw1vwRGH8og 提取码：j3g9 ","date":"2020-03-13","objectID":"/posts/%E4%BD%BF%E7%94%A8vscode-mingw%E9%85%8D%E7%BD%AEc%E8%AF%AD%E8%A8%80%E7%9A%84%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/:3:0","tags":["教程","C语言","VScode"],"title":"使用VScode+MinGW配置C语言的开发环境","uri":"/posts/%E4%BD%BF%E7%94%A8vscode-mingw%E9%85%8D%E7%BD%AEc%E8%AF%AD%E8%A8%80%E7%9A%84%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/"},{"categories":["网络爬虫"],"content":"Requests http for humans 使用urllib库的时候可能会被各种编码、Handler、Opener弄得头大，有时候要构造一个请求需要的繁琐的步骤也是让人感觉十分繁琐 requests是Python实现的最简单易用的第三方HTTP请求库，充分体现了Python的简洁与优雅 requests官方文档：https://requests.readthedocs.io/en/latest/ ","date":"2020-03-02","objectID":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/:0:0","tags":["Python","requests","网络爬虫"],"title":"requests快速上手","uri":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/"},{"categories":["网络爬虫"],"content":"安装 pip install requests ","date":"2020-03-02","objectID":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/:0:1","tags":["Python","requests","网络爬虫"],"title":"requests快速上手","uri":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/"},{"categories":["网络爬虫"],"content":"GET请求 使用requests发起get请求可以直接调用requests库中的get方法 import requests def spider(url): response = requests.get(url) html = response.content.decode(\"utf-8\") with open(\"baidu.html\", \"w\", encoding=\"utf-8\") as file: file.write(html) if __name__ == \"__main__\": url = \"http://www.baidu.com/\" spider(url) ","date":"2020-03-02","objectID":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/:1:0","tags":["Python","requests","网络爬虫"],"title":"requests快速上手","uri":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/"},{"categories":["网络爬虫"],"content":"POST请求 post请求需要提交数据，但是并不需要转码 import requests def spider(url): data = { \"表单\": \"数据\", \"键\": \"值\" } response = requests.post(url, data=data) print(response.text) if __name__ == \"__main__\": url = \"http://httpbin.org/post\" spider(url) ","date":"2020-03-02","objectID":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/:2:0","tags":["Python","requests","网络爬虫"],"title":"requests快速上手","uri":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/"},{"categories":["网络爬虫"],"content":"timeout import requests response = requests.get(\"http://www.baidu.com\", timeout=0.01) print(response) ","date":"2020-03-02","objectID":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/:3:0","tags":["Python","requests","网络爬虫"],"title":"requests快速上手","uri":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/"},{"categories":["网络爬虫"],"content":"状态码 import requests response = requests.get(\"http://www.baidu.com\") print(response.status_code) ","date":"2020-03-02","objectID":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/:4:0","tags":["Python","requests","网络爬虫"],"title":"requests快速上手","uri":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/"},{"categories":["网络爬虫"],"content":"传递URL参数 传递url参数（拼接url）同样不需要转码 import requests def spider(url): params = { \"k1\": \"v1\", \"k2\": \"v2\" } response = requests.get(url, params=params) print(response.url) if __name__ == \"__main__\": url = \"http://httpbin.org/get\" spider(url) ","date":"2020-03-02","objectID":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/:5:0","tags":["Python","requests","网络爬虫"],"title":"requests快速上手","uri":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/"},{"categories":["网络爬虫"],"content":"添加请求头 请求头也是直接传递，不需要构造请求对象 import requests def spider(url): headers = { \"User-agent\": \"Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\" } response = requests.get(url, headers=headers) print(response.text) if __name__ == \"__main__\": url = \"https://maoyan.com/\" spider(url) ","date":"2020-03-02","objectID":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/:6:0","tags":["Python","requests","网络爬虫"],"title":"requests快速上手","uri":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/"},{"categories":["网络爬虫"],"content":"添加Cookies import requests def spider(url): headers = { \"User-agent\": \"Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\", \"Cookie\": \"k1=v1;k2=v2\" } response = requests.get(url, headers=headers) print(response.text) if __name__ == \"__main__\": url = \"http://www.httpbin.org/headers\" spider(url) ","date":"2020-03-02","objectID":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/:7:0","tags":["Python","requests","网络爬虫"],"title":"requests快速上手","uri":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/"},{"categories":["网络爬虫"],"content":"代理 import requests proxies = { \"http\": \"http://10.10.1.10:3128\", \"https\": \"http://10.10.1.10:1080\", } headers = { \"User-agent\": \"Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\" } response = requests.get(\"http://python.org\", headers=headers, proxies=proxies) ","date":"2020-03-02","objectID":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/:8:0","tags":["Python","requests","网络爬虫"],"title":"requests快速上手","uri":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/"},{"categories":["网络爬虫"],"content":"Session会话维持 会话对象让你能够跨请求保持某些参数，它也会在同一个 Session 实例发出的所有请求之间保持 cookie 也就是说，如果之前通过Session对象成功访问了某个网站的某个网页，那么再次使用该Session对象请求该网站的其他网页时，就会默认使用该之前使用的cookie参数 import requests session = requests.Session() session.get(\"http://httpbin.org/cookies/set/sessioncookie/123456789\") response = session.get(\"http://httpbin.org/cookies\") print(response.text) 这里获得的cookie就是服务器给我们的cookie ","date":"2020-03-02","objectID":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/:9:0","tags":["Python","requests","网络爬虫"],"title":"requests快速上手","uri":"/posts/requests%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/"},{"categories":["网络爬虫"],"content":"网络爬虫分为通用爬虫和聚焦爬虫 ","date":"2020-03-01","objectID":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/:0:0","tags":["网络爬虫"],"title":"关于网络爬虫","uri":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"},{"categories":["网络爬虫"],"content":"什么是爬虫 爬虫是一种按照一定的规则，自动抓取万维网信息的程序或脚本 ","date":"2020-03-01","objectID":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/:0:1","tags":["网络爬虫"],"title":"关于网络爬虫","uri":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"},{"categories":["网络爬虫"],"content":"通用爬虫 通用爬虫就是我们常见的搜索引擎，搜索引擎背后的技术支撑就是爬虫技术 ","date":"2020-03-01","objectID":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/:0:2","tags":["网络爬虫"],"title":"关于网络爬虫","uri":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"},{"categories":["网络爬虫"],"content":"聚焦爬虫 聚焦爬虫是专门针对某一网站，或者某一网页的爬虫，这种也是我们最常见的网络爬虫 ","date":"2020-03-01","objectID":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/:0:3","tags":["网络爬虫"],"title":"关于网络爬虫","uri":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"},{"categories":["网络爬虫"],"content":"爬虫的基本组成 URL管理模块 HTML下载模块 HTML解析模块 数据存储模块 爬虫调度模块 ","date":"2020-03-01","objectID":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/:1:0","tags":["网络爬虫"],"title":"关于网络爬虫","uri":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"},{"categories":["网络爬虫"],"content":"常用的Python爬虫模块 requests scrapy pysipder 爬虫是一种技术，python只是写爬虫最合适的语言，但是不是说别的语言不能写爬虫 ","date":"2020-03-01","objectID":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/:2:0","tags":["网络爬虫"],"title":"关于网络爬虫","uri":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"},{"categories":["网络爬虫"],"content":"网络请求方法 网络请求的方式有很多种，但是最常见的是GET请求和POST请求 GET：参数、关键信息在URL中 POST：需要提交信息 ","date":"2020-03-01","objectID":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/:3:0","tags":["网络爬虫"],"title":"关于网络爬虫","uri":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"},{"categories":["网络爬虫"],"content":"常见的HTML解析方法 正则表达式 xpath语法 css选择器 ","date":"2020-03-01","objectID":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/:4:0","tags":["网络爬虫"],"title":"关于网络爬虫","uri":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"},{"categories":["网络爬虫"],"content":"数据存储 文本存储csv等 Mongo Redis MySQL ","date":"2020-03-01","objectID":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/:5:0","tags":["网络爬虫"],"title":"关于网络爬虫","uri":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"},{"categories":["网络爬虫"],"content":"爬虫犯法吗 技术是无罪的，关键要看使用者是怎么使用的。 一般来讲，我们在正常浏览器上能够直接看到的数据都是能够采集的，但是要注意访问速度，如果速度过高，很可能就会让服务器的压力过大甚至是瘫痪 当然通过正常手段破解了一个网站，或者一个APP，抓取了一些我们能够直接看到的数据，一般是没问题的，但是千万不要将破解方法在网上传播，不然可能就会收到律师函，这些东西在网站的使用协议和APP的使用协议一般都会有明确说明的 如果爬取了别人网站的一些数据，拿来做做数据分析提取一些特征什么的，这些都是不透明的，但是如果直接放到自己的网站上，别人没有授权给你，那么也是会收到律师函的 如果使用爬虫去进行数据造假，比如在直播的时候刷弹幕，刷评论之类的，这就是数据造假，如果因此严重误导了大众的话，牢房是安排上了 至于用户信息和隐私数据更不要说了，动了不该动的数据，牢房肯定是能分配的 至于更多的可以了解一下**《中华人民共和国网络安全法》** ","date":"2020-03-01","objectID":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/:6:0","tags":["网络爬虫"],"title":"关于网络爬虫","uri":"/posts/%E5%85%B3%E4%BA%8E%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"},{"categories":["tool"],"content":"搭建一个博客用来记录学习过程是非常不错的选择！ ","date":"2020-03-01","objectID":"/posts/%E4%BD%BF%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%B5%81%E7%A8%8B/:0:0","tags":["Hexo","blog"],"title":"使用Hexo框架搭建博客流程","uri":"/posts/%E4%BD%BF%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%B5%81%E7%A8%8B/"},{"categories":["tool"],"content":"准备工作 ","date":"2020-03-01","objectID":"/posts/%E4%BD%BF%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%B5%81%E7%A8%8B/:1:0","tags":["Hexo","blog"],"title":"使用Hexo框架搭建博客流程","uri":"/posts/%E4%BD%BF%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%B5%81%E7%A8%8B/"},{"categories":["tool"],"content":"安装Nodejs 访问Nodejs的官网就可以下载：nodejs.org 左边的是稳定版，右边的是最新版，建议下载稳定版 如果从官网下载的速度较慢，可以使用国内的镜像，如清华大学开源镜像站Nodejs的版本地址：https://mirrors.tuna.tsinghua.edu.cn/nodejs-release/ 下载安装后可以使用终端查看Nodejs和npm的版本 node -v npm -v 在安装完Nodejs之后还有一个问题，因为Nodejs的包管理工具npm默认使用的是国外的镜像源，速度是比较慢的，因此建议使用淘宝的镜像源，先使用npm安装cnpm（淘宝的镜像源），使用全局安装的方式安装（-g） npm install cnpm -g --registry=https://registry.npm.taobao.org 可以通过查看cnpm的版本来验证是否安装成功 cnpm -v ","date":"2020-03-01","objectID":"/posts/%E4%BD%BF%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%B5%81%E7%A8%8B/:1:1","tags":["Hexo","blog"],"title":"使用Hexo框架搭建博客流程","uri":"/posts/%E4%BD%BF%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%B5%81%E7%A8%8B/"},{"categories":["tool"],"content":"安装Git 其他Git客户端也是可以的 访问git的官网就可以下载：https://git-scm.com/download 下载的过程中可能会经常出现下载失败的情况，Mac可以使用homebrew安装，命令如下 brew install git Windows建议使用淘宝的镜像进行下载：https://npm.taobao.org/mirrors/git-for-windows/ Git安装好之后还要配置Git的用户，在终端输入下面的命令： git config --global user.email \"邮箱地址\" git config --global user.name \"用户名\" ","date":"2020-03-01","objectID":"/posts/%E4%BD%BF%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%B5%81%E7%A8%8B/:1:2","tags":["Hexo","blog"],"title":"使用Hexo框架搭建博客流程","uri":"/posts/%E4%BD%BF%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%B5%81%E7%A8%8B/"},{"categories":["tool"],"content":"安装Hexo框架 安装Hexo的命令如下： cnpm install -g hexo-cli 可以通过查看Hexo的版本来验证是否安装成功 hexo -v ","date":"2020-03-01","objectID":"/posts/%E4%BD%BF%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%B5%81%E7%A8%8B/:1:3","tags":["Hexo","blog"],"title":"使用Hexo框架搭建博客流程","uri":"/posts/%E4%BD%BF%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%B5%81%E7%A8%8B/"},{"categories":["tool"],"content":"初始化项目 初始化项目需要注意的一点，这个项目是使用Hexo的命令行工具生成的，所以项目是在当前目录下生成的，想要将项目放在在指定文件夹中，则需要在终端工具里将目录切换到指定目录下，也可以指定到存在的目录里生成，但是Windows下是不能跨磁盘的。（项目名称也就是项目路径，可以加也可以不加的，加上的话会自动生成路径的最后一个文件） hexo init {项目名称} 比如我的终端工具是将目录切换到了D盘下的blog文件夹下面，然后我创建项目就是这样 hexo init demo 然后就会在blog文件夹下面生成一个demo文件夹，这个demo文件夹里面就是项目的配置文件。 项目生成后，Hexo框架还会自动clone一个landscape主题，所以时间可能会比较长。 创建成功之后看一下文件夹，发现多出了很多文件 整个创建项目的过程是十分简单的，如果项目在创建过程中出来什么问题也不要担心，大不了把文件全部删除后重新来。 项目生成后在终端工具中将目录切换到项目文件夹中去，我这里就是D:\\blog\\demo，然后就可以使用server（或者s）命令让项目在本地运行 hexo s 然后就会提示博客已经在本地运行了，端口是： http://localhost:4000 ，使用Ctrl+C就能停止项目运行。 在浏览器输入地址后就能进入原始状态的博客，在这个博客页面上可以看到Hexo的命令，这些命令都可以使用简写，也就是头字母。 ","date":"2020-03-01","objectID":"/posts/%E4%BD%BF%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%B5%81%E7%A8%8B/:2:0","tags":["Hexo","blog"],"title":"使用Hexo框架搭建博客流程","uri":"/posts/%E4%BD%BF%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%B5%81%E7%A8%8B/"},{"categories":["tool"],"content":"部署到Github 要部署博客，要将本地服务断掉。 首先要在自己的Github上创建一个新仓库Create a new repository，然后这个仓库的名称必须是{username}.github.io 创建成功后会生成一个仓库的https地址，然后就可以通过这个https地址部署项目了。 要部署这个项目，首先要修改以下这个项目中的_config.yml这个文件，打开_config.yml后，直接跳转到最底部，然后在# Deployment这一项里面做如下修改，将没有的添加上，记得要在冒号后打一个空格 deploy: type: git repository: {Github仓库地址} branch: master 然后还要在项目所在目录下安装一个支持Git的部署插件，名字叫做hexo-deployer-git，如果没有这个插件，执行部署命令的时候会报错，使用cnpm来安装这个插件 cnpm install --save hexo-deployer-git 安装成功之后就可以执行部署命令了 hexo d 部署成功之后就可以使用Github Repository的同名链接来访问博客了，也就是{username}.github.io，然后就可以看到和本地启动一摸一样的内容。 ","date":"2020-03-01","objectID":"/posts/%E4%BD%BF%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%B5%81%E7%A8%8B/:3:0","tags":["Hexo","blog"],"title":"使用Hexo框架搭建博客流程","uri":"/posts/%E4%BD%BF%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%B5%81%E7%A8%8B/"},{"categories":["tool"],"content":"写博客文章 在终端工具中使用hexo的命令就可以创建md格式的文章（路径要在博客项目中） hexo n \"{文章名字}\" 例如我输入的是 hexo n \"我的第一篇博客\" 然后就会提示：INFO Created: D:\\blog\\demo\\source\\_posts\\我的第一篇博客.md，说明程序已经在这个文件夹下面生成了一篇叫做“我的第一篇博客”的Markdown文件。 创建的md文件是空的，进入到博客项目的\\source\\_posts文件夹下面就能看到刚才使用命令创建的md文件，打开创建的md文件，对文件进行修改 文件保存后，需要在项目路径下使用命令清理一下 hexo clean 再生成这个项目 hexo g 在本地启动看一下效果 hexo s 可以看到刚才写的那篇博客已经在网页上显示了，接下来再将这个项目部署到Github上 hexo d ","date":"2020-03-01","objectID":"/posts/%E4%BD%BF%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%B5%81%E7%A8%8B/:4:0","tags":["Hexo","blog"],"title":"使用Hexo框架搭建博客流程","uri":"/posts/%E4%BD%BF%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%B5%81%E7%A8%8B/"},{"categories":["tool"],"content":"更换主题 文章能写了，也能部署到Github上了，但是这个博客看起来不是太美观，给它换一个主题。 推荐NexT主题，官方文档很清楚，使用人数很广，网上解决问题的方案也很成熟 NexT的说明文档地址：http://theme-next.iissnan.com/ 这里使用的是icarus这个主题。 这是icarus主题项目地址：https://github.com/ppoffice/hexo-theme-icarus 点击Clone or download，然后复制Clone地址。 使用终端工具进入到项目目录下，然后使用git的克隆命令，并且指定到themes文件夹下的icarus文件夹下。 git clone https://github.com/ppoffice/hexo-theme-icarus.git themes/icarus 如果下载成功，那么在D:\\blog\\demo\\themes下面就会多出一个叫icarus的文件夹，landscape是创建博客项目的时候默认下载的。 然后修改博客项目中的_config.yml文件，也就是D:\\blog\\demo路径下的_config.yml文件。 找到theme然后将landscape改为icarus 然后再使用命令重新生成一下博客 hexo clean 然后这里报错了（其他主题可能不会报错），报错信息如下 INFO Checking dependencies ERROR Package cheerio is not installed. ERROR Please install the missing dependencies from the root directory of your Hexo site. 原因是缺少cheerio依赖项，使用cnpm将cheerio安装到当前项目中， cnpm install cheerio -s 再使用hexo clean命令，这时候会提示 INFO Checking dependencies INFO Validating the configuration file WARN themes\\icarus\\_config.yml is not found. We are creating one for you... INFO themes\\icarus\\_config.yml is created. Please restart Hexo to apply changes. 意思是说icarus主题中缺少_config.yml文件，已经自动生成了，接着再使用hexo clean命令，已经正常了，然后再生成一下博客，并且启动它 hexo g hexo s 现在博客的样式就很好看了 然后将项目部署到Github上就可以通过url访问了。 ","date":"2020-03-01","objectID":"/posts/%E4%BD%BF%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%B5%81%E7%A8%8B/:5:0","tags":["Hexo","blog"],"title":"使用Hexo框架搭建博客流程","uri":"/posts/%E4%BD%BF%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%B5%81%E7%A8%8B/"},{"categories":["网络爬虫","Python"],"content":"在Python中，要发起一个请求获取服务器的响应，我们不需要去关心计算机怎么去发起的这个请求，更不需要知道计算机底层是怎样实现的这个通信，我们只需要关注我们的这个请求的URL、参数、等等的信息，而这一切Python标准库中的urllib库已经为我们准备好了 urllib是Python内置的一个HTTP请求库 Python3 urllib库官方文档地址：https://docs.python.org/3/library/urllib.html urllib库包含四个基本的URL模块 模块名称 用途 urllib.request 打开和读取URL urllib.error 包含了urllib.request抛出的异常类型 urllib.parse 用于解析URL urllib.robotparser 用于解析robots.txt文件 ","date":"2020-03-01","objectID":"/posts/urllib%E5%BA%93%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:0:0","tags":["Python","网络爬虫"],"title":"urllib库基本使用","uri":"/posts/urllib%E5%BA%93%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["网络爬虫","Python"],"content":"urllib.request 模拟浏览器像服务器发送请求，需要使用的是urllib.request模块，其中最基础的请求方法就是urlopen()方法 urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None) url可以是字符串，也可以是自己构造的Request对象 data是需要发送给服务器的数据对象，如果没有则不用填写，默认为None timeout超时设置，是一个可选参数，传入超时时间后，如果在指定的时间内服务器没有响应则抛出time out异常 cafile和capath代表 CA 证书和 CA 证书的路径。如果使用HTTPS则可能需要用到 context参数必须是ssl.SSLContext类型，用来指定SSL设置 请求网页 import urllib.request response = urllib.request.urlopen(\"http://www.baidu.com\") html = response.read() print(html.decode(\"utf-8\")) 使用urlopen方法获取到的是一个http.client.HTTPResponse对象 使用read()方法将response中的网页源代码读出来 使用decode()方法将读取出来的网页源代码编码转换成utf-8编码 可以通过下面的方法查看http.client.HTTPResponse对象的一些基本信息： response.getcode()，获取状态码 response.readline()，以字节流 返回所有得数据 以列表格式保存 response.getheaders()，获取响应头 response.geturl()，获取url 请求超时设置 在一些情况下，如果服务器识别到我们的爬虫不是一个正常的客户端，那么它可能就会延长返回结果，甚至返回的结果是迷惑我们的信息，那么即耗时也没拿到想要的数据，还有一种情况就是服务器的负载过高，那么返回响应的时间就会延长。 import urllib.request response = urllib.request.urlopen(\"http://tieba.baidu.com\", timeout=0.1) print(response.read().decode('utf-8')) 从上面的代码看，仅仅只需要两行代码就能从服务器上获取到一个响应体 但是事实上很多网站并没有这么简单，大多数情况服务器会读取请求传输过去的请求体，然后去判断到底是不是一个正常的客户端，如果不是，那么得到的响应体就是不正常的数据。 ","date":"2020-03-01","objectID":"/posts/urllib%E5%BA%93%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:0:1","tags":["Python","网络爬虫"],"title":"urllib库基本使用","uri":"/posts/urllib%E5%BA%93%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["网络爬虫","Python"],"content":"urllib.parse 大多数我们请求的url可能是不固定的，那么我们可能就面临着需要去构造url，因为Python是解释型语言，解释器只支持ASCII码，所以需要对url进行编码 import urllib.request import urllib.parse import string def crawler(): basis_url = \"https://www.baidu.com/s?wd=\" key_word = \"ip\" url = basis_url + key_word encode_url = urllib.parse.quote(key_word, safe=string.printable) response = urllib.request.urlopen(encode_url) html = response.read().decode(\"utf-8\") print(html) if __name__ == \"__main__\": crawler() 当关键参数较多的时候可以构造为dict，然后使用urlencode()来进行编码 import urllib.request import urllib.parse def crawler(): basis_url = \"http://www.baidu.com/s?\" params = { \"wd\": \"男\", \"value\": \"20\" } encode_url = urllib.parse.urlencode(params) print(encode_url) url = basis_url + encode_url print(url) if __name__ == \"__main__\": crawler() 注意：经过urlencode编码后的url已经是ASCII码了，但是如果还是问题，那么就再使用quote编码一次 ","date":"2020-03-01","objectID":"/posts/urllib%E5%BA%93%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:0:2","tags":["Python","网络爬虫"],"title":"urllib库基本使用","uri":"/posts/urllib%E5%BA%93%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["网络爬虫","Python"],"content":"data参数 在请求某些网页时需要携带一些数据，我们就需要使用到 data params 需要被转码成字节流，而 params 是一个字典，我们需要使用 urllib.parse.urlencode() 将字典转化为字符串，再使用 bytes() 转为字节流，最后使用 urlopen() 发起请求，请求是模拟用 POST 方式提交表单数据 import urllib.request import urllib.parse def crawler(): form_data = { \"测试\": \"post请求\", \"编程语言\": \"派森\" } data = bytes(urllib.parse.urlencode(form_data), encoding=\"utf-8\") response = urllib.request.urlopen(\"http://httpbin.org/post\", data=data) print(response.read()) if __name__ == \"__main__\": crawler() 返回的数据中就可以看到上面提交的数据，只是数据是经过编码的 这个http://httpbin.org是一个用来做HTTP请求测试的网站 实战1：利用必应翻译API，开发一个翻译程序 必应翻译url：https://cn.bing.com/translator/ 进入网站，打开浏览器开发者工具，定位到Network 然后在必应翻译的文本框输入翻译的内容，比如说爬虫，然后注意Netword中的变化，会出现很多通信的请求API，然后我们需要找到翻译的API，获取到真实的URL地址，以及传输的信息 打开ttranslatev3?is……这个响应信息，往下拉就可以发现Form Data中的text的值就是我们需要翻译的文字 打开Preview这一项，可以看到这里面的text的值就是翻译的结果 上面的抓包过程可以多尝试几次 然后回到Headers中，在General中可以看到这次请求的基本信息，其中Request URL就是真实的url，Request Method就是请求方式，Status Code是这次请求的状态码，我们需要使用真实的url去发起请求 实例程序如下： import urllib.parse import urllib.request def crawler(): url = \"https://cn.bing.com/ttranslatev3?isVertical=1\u0026\u0026IG=2B73CBCDC8F54EAFABF49389E29DC19A\u0026IID=translator.5028.2\" form_data = { \"fromLang\": \"auto-detect\", \"text\": \"爬虫\", \"to\": \"en\" } data = bytes(urllib.parse.urlencode(form_data), encoding='utf8') response = urllib.request.urlopen(url, data=data) print(response.read().decode('utf-8')) if __name__ == \"__main__\": crawler() import urllib.request import urllib.parse class BingSpider(object): def __init__(self): self.url = \"https://cn.bing.com/ttranslatev3?isVertical=1\u0026\u0026IG=4FEF33F31181435589194F495E9386B0\u0026IID=translator.5028.2\" self.from_data = { \"fromLang\": \"auto-detect\", \"text\": \"爬虫\", \"to\": \"en\" } def crawl(self): \"\"\" 爬虫下载模块 \"\"\" data = bytes(urllib.parse.urlencode(self.from_data), encoding=\"utf-8\") response = urllib.request.urlopen(url=self.url, data=data) r = response.read().decode(\"utf-8\") print(r) def run(self): \"\"\" 爬虫调度模块 \"\"\" self.crawl() if __name__ == \"__main__\": spider = BingSpider() spider.run() ","date":"2020-03-01","objectID":"/posts/urllib%E5%BA%93%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:0:3","tags":["Python","网络爬虫"],"title":"urllib库基本使用","uri":"/posts/urllib%E5%BA%93%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["网络爬虫","Python"],"content":"构造Request对象 urlopen()可以支撑我们的一些简单的请求，但是请记住，一个没有请求头的爬虫是没有灵魂的。虽然不使用请求头也可以访问一些网页，但是这样的行为是直接告诉服务器“我是一个爬虫”，那么服务器可能就会拒绝程序的请求，因此我们需要进行伪装，这时候我们就需要去构造我们的HTTP请求体，一个Request对象。 import urllib.request def crawler(): url = \"https://maoyan.com/\" request = urllib.request.Request(url=url) response = urllib.request.urlopen(request) print(response.read().decode('utf-8')) if __name__ == '__main__': crawler() 执行上面的程序，程序将抛出异常urllib.error.HTTPError: HTTP Error 403: Forbidden，这是服务器禁止访问时程序产生的错误。 先看一下我们不伪装爬虫时的请求头是什么样的 import urllib.request def crawler(): url = \"http://www.httpbin.org/headers\" headers = { \"User-agent\": \"Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\" } response = urllib.request.urlopen(url) print(response.read().decode('utf-8')) if __name__ == '__main__': crawler() 从返回的信息中可以看出，这个爬虫的请求头（User-agent）是\"User-Agent\": \"Python-urllib/3.7\"，如果是这样，服务器很容易就可看穿我们的爬虫 这时候就需要加上请求头来伪装我们的爬虫 urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None) 其中headers就是我们要传递的参数 import urllib.request def crawler(): url = \"https://maoyan.com/\" headers = { \"User-agent\": \"Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\" } request = urllib.request.Request(url=url, headers=headers) response = urllib.request.urlopen(request) print(response.read().decode('utf-8')) if __name__ == '__main__': crawler() ","date":"2020-03-01","objectID":"/posts/urllib%E5%BA%93%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:0:4","tags":["Python","网络爬虫"],"title":"urllib库基本使用","uri":"/posts/urllib%E5%BA%93%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["网络爬虫","Python"],"content":"Handler和OpenerDirector 原始的urlopen()方法已经够强大了，但是这并不能满足我们构建一个更加像浏览器的爬虫，所以这时候需要我们自己去构造我们的处理器（handler），然后通过OpenerDirector（opener）去使用处理器，其实urlopen就是一个Python为我们构造好的opener。 Handler 能处理请求（HTTP、HTTPS、FTP等）中的各种事情，它是通过 urllib.request.BaseHandler这个类来实现的。urllib.request.BaseHandler是所有的 Handler 的基类，其提供了最基本的Handler的方法 常见的Handler类： ProxyHandler：为请求设置代理 HTTPCookieProcessor：处理 HTTP 请求中的 Cookies HTTPPasswordMgr：用于管理密码，它维护了用户名密码的表。 HTTPBasicAuthHandler：用于登录认证，一般和 HTTPPasswordMgr 结合使用。 cookie 一个 Web 站点可能会为每一个访问者产生一个唯一的ID, 然后以 Cookie 文件的形式保存在每个用户的机器上。如果使用浏览器访问 Web, 会看到所有保存在硬盘上的 Cookie。在这个文件夹里每一个文件都是一个由“键/值”对组成的文本文件,另外还有一个文件保存有所有对应的 Web 站点的信息 当客户端再次访问这个 Web 站点时这些信息可供该站点使用。由于“Cookie”具有可以保存在本地客户端上的神奇特性, 因此它可以帮助我们实现记录用户个人信息的功能 我们可以通过使用cookie避免一些不要登录认证或者来完善的伪装我们的爬虫 import http.cookiejar import urllib.request def crawler(): url = \"http://tieba.baidu.com/\" headers = { \"User-agent\": \"Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\", \"Cookie\": \"cookies\" } request = urllib.request.Request(url,headers=headers) cookie = http.cookiejar.CookieJar() handler = urllib.request.HTTPCookieProcessor(cookie) opener = urllib.request.build_opener(handler) response = opener.open(requests) if __name__ == \"__main__\": crawler() 如果在构造完opener后使用urllib.request.install_opener(opener)，那么构造的这个opener将是一个全局的opener，如果之后凡是调用urlopen方法将会使用这个全局的opener方法 实战二：B站评论 找一个我们都喜欢的土味视频，打开浏览器的开发者工具调到Network，然后将页面拉到下面的评论框，发送一条评论 然后我可以很轻松的看到服务器给我们的浏览器返回了一个名字叫做add的包 点进去之后可以看到刚才发送的评论，以及这个视频的oid，可以多发送几条查看一下Request Headers和Form Data哪些参数会变化，会发生变化的参数分别是Content-Length这个是评论的长度、oid是视频的id，以及message import urllib.request import http.cookiejar import urllib.parse import gzip def crawler(): url = \"https://api.bilibili.com/x/v2/reply/add\" form_data = { \"oid\": \"type\": 1, \"message\": \"plat\": 1, \"jsonp\": \"jsonp\", \"csrf\": \"ebfe0e90c2867d672bfcbe62fb1b257c\" } headers = { \"User-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36\", \"Host\": \"api.bilibili.com\", \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\", \"Accept\": \"application/json, text/javascript, */*; q=0.01\", \"Accept-Encoding\": \"gzip, deflate, br\", \"Accept-Language\": \"zh-CN,zh;q=0.9\", \"Connection\": \"keep-alive\", \"Origin\": \"https://www.bilibili.com\", \"Referer\": \"https://www.bilibili.com/video/av88366823\", \"Cookie\": } data = bytes(urllib.parse.urlencode(form_data), encoding=\"utf-8\") request = urllib.request.Request(url,headers=headers, data=data) cookie = http.cookiejar.CookieJar() handler = urllib.request.HTTPCookieProcessor(cookie) opener = urllib.request.build_opener(handler) response = opener.open(request) response = gzip.GzipFile(fileobj=response) print(response.read().decode(\"utf-8\")) if __name__ == '__main__': crawler() ","date":"2020-03-01","objectID":"/posts/urllib%E5%BA%93%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:0:5","tags":["Python","网络爬虫"],"title":"urllib库基本使用","uri":"/posts/urllib%E5%BA%93%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["网络爬虫","Python"],"content":"代理 写爬虫一般情况下都是要使用代理的，如果我们请求该网站频率过高，该网站会被封 IP，禁止我们的访问 透明代理 代理服务器将客户端的信息转发至目标访问对象，并没有完全隐藏客户端真实的身份。即服务器知道客户端使用了代理IP，并且知道客户端的真实IP地址。 普通匿名代理 代理服务器用自己的IP代替了客户端的真实IP，但是告诉了目标访问对象这是代理访问。 高匿代理 代理服务器良好地伪装了客户端，不但用一个随机的IP代替了客户端的IP，也隐藏了代理信息，服务器不会察觉到客户端是通过代理实现访问的，即用户仿佛就是直接使用代理服务器作为自己的客户端。 import urllib.request def spider(): url = \"http://www.httpbin.org/ip\" proxy = { \"http\": \"110.243.2.58:9999\", \"http\": \"117.69.150.100:9999\" } proxy_handler = urllib.request.ProxyHandler(proxy) opener = urllib.request.build_opener(proxy_handler) urllib.request.install_opener(opener) request = urllib.request.Request(url) response = urllib.request.urlopen(request) print(response.read().decode(\"utf-8\")) if __name__ == \"__main__\": spider() ","date":"2020-03-01","objectID":"/posts/urllib%E5%BA%93%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:0:6","tags":["Python","网络爬虫"],"title":"urllib库基本使用","uri":"/posts/urllib%E5%BA%93%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["Python"],"content":"Python中的list对象是Python中最常用的数据类型，而list切片是让list对象变得灵活且方便操作的对象，对！切片也是一种对象！ 切片是我们中文的称呼，官方的叫法是slice！ slice的运用 要想明白slice的实现原理，肯定要先明白slice的运用。 切片模式：[start:end:step] start表示切片开始的位置，默认为0 end表示切片截至（但不包含）位置（默认为列表长度） step表示切取数据的步长（默认为1） 当start为0时可以省略，当end为列表长度时可以省略 当step为1时可以省略，并且省略步长时可以同时省掉一个冒号 当step为负整数时，表示反向切片，这时start应该要比end的值大才行 返回包含原列表所有元素的新列表 \u003e\u003e\u003e int_list = [3, 4, 5, 6, 7, 8, 9, 11, 13, 15, 17] \u003e\u003e\u003e my_list = int_list[:] 上面的返回结果虽然和原来是相同的，不过是一个全新的对象，也可以看成是深拷贝，由于end、step的长度都是默认的，所以他可以不写，而且也可以只写一个冒号（:）。对于IntList和MyList到底是不是两个对象，可以通过地址来确定 \u003e\u003e\u003e id(int_list) \u003e\u003e\u003e id(my_list) 输出的两个地址肯定是不同的 可能会疑惑，为什么地址不同就是两个对象 \u003e\u003e\u003e a = [1, 2, 3] \u003e\u003e\u003e b = a \u003e\u003e\u003e id(a) 1897328218248 \u003e\u003e\u003e id(b) 1897328218248 \u003e\u003e\u003e b[2] = 30 \u003e\u003e\u003e a [1, 2, 30] 可以看到，这种通过浅拷贝而来的对象，本质上是将这个列表的引用赋值给了另一个对象，而不是在内存中新开辟内存空间用来存放一个新的列表，所以是一个对象 返回包含原列表中所有元素的逆序列表 \u003e\u003e\u003e int_list = [3, 4, 5, 6, 7, 8, 9, 11, 13, 15, 17] \u003e\u003e\u003e int_list[::-1] [17, 15, 13, 11, 9, 8, 7, 6, 5, 4, 3] 隔一个取一个，获取偶数位置的元素 \u003e\u003e\u003e int_list = [3, 4, 5, 6, 7, 8, 9, 11, 13, 15, 17] \u003e\u003e\u003e int_list[::2] [3, 5, 7, 9, 13, 17] 隔一个取一个，获取奇数位置的元素 \u003e\u003e\u003e int_list = [3, 4, 5, 6, 7, 8, 9, 11, 13, 15, 17] \u003e\u003e\u003e int_list[1::2] [4, 6, 8, 11, 15] 指定切片开始和结束的位置 \u003e\u003e\u003e int_list = [3, 4, 5, 6, 7, 8, 9, 11, 13, 15, 17] \u003e\u003e\u003e int_list[3:6] [6, 7, 8] 切片的长度大于列表的长度时从列表尾部开始截断 \u003e\u003e\u003e int_list = [3, 4, 5, 6, 7, 8, 9, 11, 13, 15, 17] \u003e\u003e\u003e int_list[3:100] [6, 7, 8, 9, 11, 13, 15, 17] 切片的开始位置大于列表长度时，返回空列表 \u003e\u003e\u003e int_list = [3, 4, 5, 6, 7, 8, 9, 11, 13, 15, 17] \u003e\u003e\u003e int_list[100:] [] 在列表尾部插入元素 \u003e\u003e\u003e int_list = [3, 4, 5, 6, 7, 8, 9, 11, 13, 15, 17] \u003e\u003e\u003e int_list[len(int_list):] = [0, 0] \u003e\u003e\u003e int_list [3, 4, 5, 6, 7, 8, 9, 11, 13, 15, 17, 0, 0] 可以看到这一种切片操作已经改变了原来的IntList对象，相当于IntList += [0, 0]，而且增加的列表长度不是固定的。 在列表头部插入元素 \u003e\u003e\u003e int_list = [3, 4, 5, 6, 7, 8, 9, 11, 13, 15, 17] \u003e\u003e\u003e int_list[:0] = [1, 2] \u003e\u003e\u003e int_list [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 13, 15, 17] 这种方法相当于[1, 2] += IntList 在列表中间位置插入元素 \u003e\u003e\u003e int_list = [3, 4, 5, 6, 7, 8, 9, 11, 13, 15, 17] \u003e\u003e\u003e int_list[3:3] = [0] \u003e\u003e\u003e int_list [3, 4, 5, 0, 6, 7, 8, 9, 11, 13, 15, 17] 这种插入方式也可以将元素更改 \u003e\u003e\u003e int_list = [3, 4, 5, 6, 7, 8, 9, 11, 13, 15, 17] \u003e\u003e\u003e int_list[1:2] = ['a', 'b'] \u003e\u003e\u003e int_list [3, 'a', 'b', 5, 6, 7, 8, 9, 11, 13, 15, 17] 其实上面的三种插入元素的方式叫做设置更合适，总结为以下两点： 当冒号前后位置相同时，在原列表指定位置前插入指定元素 当位置不同时，将指定元素设置到原列表指定位置 将一个列表中的元素以列表为单位复制多份 \u003e\u003e\u003e int_list = [1, 2, 3] \u003e\u003e\u003e int_list * 3 [1, 2, 3, 1, 2, 3, 1, 2, 3] 每一份都是接着上一份的结尾的 隔一个修改一个 \u003e\u003e\u003e int_list = [1, 2, 3, 4, 5, 6] \u003e\u003e\u003e int_list[::2] = [0] * 3 \u003e\u003e\u003e int_list [0, 2, 0, 4, 0, 6] 但是需要注意，左切片不连续的时候，等号两边的切片长度必须相等，像上面的代码中，左边切片IntList[::2]的长度是3，[0] * 3的长度也是3，长度不相同的话是会报错的，例如下面的程序 \u003e\u003e\u003e a = [1, 2, 3, 4, 5, 6, 7, 8, 9] \u003e\u003e\u003e a[::2] = [0] * 3 Traceback (most recent call last): File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e ValueError: attempt to assign sequence of size 3 to extended slice of size 5 这里就报错了，原因是左边的切片长度是5，而右边的切片长度是3。 删除列表中的元素 \u003e\u003e\u003e int_list = [1,2,3,4,5,6,7,8,9] \u003e\u003e\u003e int_list[:3] = [] \u003e\u003e\u003e int_list [4, 5, 6, 7, 8, 9] 上面的程序相当于del int_list[:3] 支持切片的对象 一个对象要想支持切片操作，那么用来实例化它的类就要实现__getitem__这个魔法函数 class College: def __init__(self, college_name, class_name, students): self.college_name = college_name self.class_name = class_name self.students = students # 实现切片操作的关键函数 __getitem__ def __getitem__(self, item): pass students = ['李华', \"张三\", \"王五\", \"王大锤\"] college = College(college_name=\"信工\", class_name=\"Python\", students=students) print(college[:2]) 上面的程序虽然没有输出任何的东西，但是没有报错，说明是可以进行切片操作的，但是将__getitem__注释掉以后就会报错。 对象的切片操作会自动调用__getitem__函数，可以不用管其他的东西，然后将一切都交给list处理就好了 class College: def __init__(self, college_name, class_name, students): self.college_name = college_name self.class_name = class_name self.students = students # 实现切片操作的关键函数 __getitem__ def __getitem__(self, item): return self.students[item] students = ['李华', \"张三\", \"王五\", \"王大锤\"] college = College(college_name=\"信工\", class_name=\"Python\", students=students) print(college[:2]) print(type(college)) sub_list = college[:2] print(type(sub_list)) 上面的程序运行后的结果如下 ['李华', '张三'] \u003cclass '__main__.College'\u003e \u003cclass 'list'\u003e 可以看到，虽然college对象的类型是College，但是college经过切片后返回值对象是list类型，希望切片操作后返回的值的类型还是一个College类，而且当使用索引的时候能","date":"2020-01-24","objectID":"/posts/python-list%E5%88%87%E7%89%87/:0:0","tags":["Pyhton","魔法函数","list"],"title":"List切片","uri":"/posts/python-list%E5%88%87%E7%89%87/"},{"categories":["其他"],"content":"使用学生的身份来薅JetBrains的羊毛！用上正版的IDE 使用学生特权获取jet brains产品的免费特权 ","date":"2020-01-15","objectID":"/posts/jetbrains%E5%AD%A6%E7%94%9F%E4%BC%98%E6%83%A0/:0:0","tags":["JetBrains"],"title":"用学生身份免费使用IDE","uri":"/posts/jetbrains%E5%AD%A6%E7%94%9F%E4%BC%98%E6%83%A0/"},{"categories":["其他"],"content":"注册学生邮箱 想要使用教育优惠，那么就需要一个大学电子邮件地址，也就是学校的学生邮箱。因为学校可能不同，所以这里需要自己去学校的官网或者是学校的信息网络中心网站去申请，当然也可以全凭本事！（坏笑） 申请的结果是一个结尾是.edu.cn的邮箱，不管前面有多少个点，但是邮箱地址肯定是以.edu.cn结尾的。 ","date":"2020-01-15","objectID":"/posts/jetbrains%E5%AD%A6%E7%94%9F%E4%BC%98%E6%83%A0/:1:0","tags":["JetBrains"],"title":"用学生身份免费使用IDE","uri":"/posts/jetbrains%E5%AD%A6%E7%94%9F%E4%BC%98%E6%83%A0/"},{"categories":["其他"],"content":"申请JetBrains的免费产品 如果觉得下面的东西有些过于废话！直接戳这! 注册完邮箱以后，就需要进入JetBrains官方网站，在网站首页的底部有一个Get Student Pack的链接，甭管这是什么意思，有Student还需要多想吗！点它！ 跳转出来的文章标题的Free也是十分的醒目，建议翻译成中文阅读一下，然后点击APPLY NOW 然后就进入了正式申请的界面，建议阅读一下JetBrains Account Agreement（JetBrains帐户协议）。申请页面要注意Name那一栏，如果填的是中文，那么在打开IDE的时候Licensed to后面的名字就不能正常显示。 信息填写完之后，点击APPLY FOR FREE PRODUCTS，然后进入邮箱验证，然后就可以使用邮箱在IDE的许可证激活中激活了 ","date":"2020-01-15","objectID":"/posts/jetbrains%E5%AD%A6%E7%94%9F%E4%BC%98%E6%83%A0/:2:0","tags":["JetBrains"],"title":"用学生身份免费使用IDE","uri":"/posts/jetbrains%E5%AD%A6%E7%94%9F%E4%BC%98%E6%83%A0/"},{"categories":["其他"],"content":"过期后怎么办？ 只要还是在校学生，过期后都可以续期，会发来一封电子邮件，按照提示来就好了。当然也可以在过期后也可以使用别的同学的信息重新申请，毕竟一个学校的又不是都是搞编程的；就算都是搞编程的但是又不是都用JetBrains的IDE。 ","date":"2020-01-15","objectID":"/posts/jetbrains%E5%AD%A6%E7%94%9F%E4%BC%98%E6%83%A0/:3:0","tags":["JetBrains"],"title":"用学生身份免费使用IDE","uri":"/posts/jetbrains%E5%AD%A6%E7%94%9F%E4%BC%98%E6%83%A0/"},{"categories":["Python"],"content":"学习Python的时候我们经常需要安装各种各样的框架，第三方的包或者库。有的时候不同的项目依赖库的版本可能不用，这时候就可以使用虚拟环境来安装需要的包，将各个项目的开发环境独立出来。 ","date":"2020-01-09","objectID":"/posts/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:0:0","tags":["Python","虚拟环境"],"title":"Python虚拟环境","uri":"/posts/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"Python多版本共存 在Windows下要解决Python多版本共存的问题很简单，将Python安装文件夹路径添加到环境变量，然后复制python.exe，到当前目录下，重命名就行 在终端调用Python的时候就使用重命名后的Python名称 在Linux下只需要在创建软链接时标明软链接名称就行 ln -s /usr/local/python38/bin/python3.8 /usr/bin/python38 ","date":"2020-01-09","objectID":"/posts/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:1:0","tags":["Python","虚拟环境"],"title":"Python虚拟环境","uri":"/posts/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"什么是虚拟环境 在使用Python语言的时候我们使用pip来安装第三方包，但是由于pip的特性，系统中只能安装每个包的一个版本。但是在实际项目开发中，不同项目可能需要第三方包的不同版本，比如Jupyter这个WebUI的后台服务是tornado框架支持的，pyspiderWebUI的后台服务也是靠tornado支持的，但是他们需要的tornado版本是不同的，就会产生冲突，但是没有必要为这两个框架去安装不同版本的Python。 Python的解决方案就是虚拟环境。虚拟环境就是虚拟出来的一个隔离的Python环境，每个项目都可以有自己的虚拟环境，用pip安装各自的第三方包，不同项目之间也不会存在冲突，虚拟环境和实体环境共享一套标准库。 ","date":"2020-01-09","objectID":"/posts/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:2:0","tags":["Python","虚拟环境"],"title":"Python虚拟环境","uri":"/posts/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"venv Python3.3以上的版本通过venv模块原生支持虚拟环境，这个模块在Python3.4以后都自带了，并不需要去安装，直接使用就行。 venv模块提供了创建轻量级“虚拟环境”，提供与系统Python的隔离支持。 创建虚拟环境 python -m venv venv_name 创建成功之后就会在当前文件夹多出一个文件夹，这个文件夹的名称就是虚拟环境的名称，里面通常会有几个文件和文件夹 虚拟环境文件夹 激活虚拟环境 在Windows下激活虚拟环境需要到创建的虚拟环境的文件夹下面的Scripts文件夹中，在终端工具中使用命令激活 activate.bat 在Linux也是使用命令激活，但是有些不一样的地方 source venv_name/bin/activate 虚拟环境激活以后就可以在虚拟环境中安装需要的包了。 使用虚拟环境创建的项目，只能进入到虚拟环境以后才能使用命令对项目进行控制。比如，在这个虚拟环境下使用Django框架创建了一个项目，如果想使用Django的命令开启这个项目的服务的话只能进入到这个虚拟环境开启 退出虚拟环境 deactivate ","date":"2020-01-09","objectID":"/posts/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:3:0","tags":["Python","虚拟环境"],"title":"Python虚拟环境","uri":"/posts/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"virtualenv virtualenv是一个管理虚拟环境的第三方包，相对于Python自带的venv它的功能会更丰富一些 virtualenv可以创建requirements.txt文件来存储依赖包的以及python版本信息 基本命令 创建虚拟环境 virtualenv venv_name 指定Python版本创建 virtualenv -p [Python路径] venv_name 是否继承第三方库 virtualenv --system-site-packages venv_name 激活虚拟环境 activate.bat 退出虚拟环境 deactivate ","date":"2020-01-09","objectID":"/posts/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:4:0","tags":["Python","虚拟环境"],"title":"Python虚拟环境","uri":"/posts/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"pipenv 上面的两种虚拟环境的创建方法其实都不太推荐，因为，他们创建的虚拟环境都不在一个位置，不方便管理。pipenv就比较好的解决了这个问题，使用pipenv创建虚拟环境会自动生成一个随机的虚拟环境目录名，生成的虚拟环境在C:\\Users\\user-name\\.virtualenvs文件夹下。pipenv在创建虚拟环境的时候会自动读取当前目录下是否有requirements.txt，如果有则将依赖包的信息写入Pipfile 安装 pip install pipenv 基本命令 创建虚拟环境并指定python版本 pipenv --python 3.6 查看虚拟环境的存放位置 pipenv --venv 安装第三方包（不需要进入虚拟环境） pipenv install package-name 更新包（不加包的名称则全部更新） pipenv update package-name 查看依赖包 pipenv graph 进入虚拟环境 pipenv shell 退出虚拟环境 exit 读取Pipfile来创建虚拟环境，没有Pipfile则创建Pipfile和Pipfile.lock pipenv install 删除虚拟环境 pipenv --rm ","date":"2020-01-09","objectID":"/posts/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:5:0","tags":["Python","虚拟环境"],"title":"Python虚拟环境","uri":"/posts/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"poetry poetry的使用和上面的创建虚拟环境的工具有些不同。 poetry创建的虚拟环境不需要进入，只需要在运行项目的时候使用命令即可 poetry创建的虚拟环境都在一个目录下，方便管理 项目使用的python版本以及项目的依赖包都会写入配置文件中 使用poetry创建虚拟环境时指定的python版本一定要是电脑中存在的 poetry适合用来创建需要发布的项目或者适合在开发环境中使用，poetry并不适合在生产环境中使用 Windows安装 在power shell中输入下面的命令就会自动安装，安装的位置就是power shell当前所处目录，并且会自动将bin目录添加到环境变量中。 (Invoke-WebRequest -Uri https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py -UseBasicParsing).Content | python 基本命令 创建一个新的项目 poetry new project_name 已有项目，但是想在项目中使用poetry，需要进入项目中 poerty init 创建虚拟环境（需要根据toml配置文件来创建） poetry install 安装项目的依赖包 poetry add package-name 移除不需要的包 poetry remove package-name 使用虚拟环境运行程序 poetry run project_name 查看项目的依赖包（待解决） poetry show project_name Pycharm以及其他IDE并不能直接使用poetry创建的虚拟环境，所以在进行生产开发的时候还是显得比较麻烦，因为这样在这样的环境中，对程序的调试就不能直接在IDE中进行调试。 ","date":"2020-01-09","objectID":"/posts/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:6:0","tags":["Python","虚拟环境"],"title":"Python虚拟环境","uri":"/posts/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"virtualenvwrapper 使用和virtualenv类似，但是功能更加强大 使用Workon命令就可以查看电脑中的虚拟环境，Workon后面加上虚拟环境名称就会进入虚拟中 如果是在Windows上安装 pip install virtualenvwrapper-win 创建虚拟环境 mkvirtualenv envname 删除 rmvirtualenv envname 在环境变量中创建变量WORKON_HOME（不是在path中添加），变量值为文件路径，然后创建的虚拟环境就会放到这个文件夹中。 指定python版本 mkvirtualenv -p python36 venv_name 生成requirements.txt问件 pip3 freeze \u003e requirements.txt ","date":"2020-01-09","objectID":"/posts/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:7:0","tags":["Python","虚拟环境"],"title":"Python虚拟环境","uri":"/posts/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"结语 没有介绍使用Anaconda和Pycharm创建虚拟环境的原因很简单。Anaconda创建虚拟环境的方式比较简单，但是conda目前还不支持安装scrapy这样的框架，不符合我的需求，而且没有必要为了虚拟环境而安装一个Anaconda（除非是需要科学计算的环境），并且Anaconda也存在一些问题，使用更简单更方便管理的工具岂不是更好。不适使Pycharm的原因也是因为创建的虚拟环境不方便管理。 目前看来使用virtualenvwrapper会更方便更快捷更简单一些，不需要付出多学习命令的成本，而且也很方便。 ","date":"2020-01-09","objectID":"/posts/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/:8:0","tags":["Python","虚拟环境"],"title":"Python虚拟环境","uri":"/posts/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"categories":["tool"],"content":"将博客的主题设置为icarus后还需要设置一些东西，更改一下这个主题的配置文件！ ","date":"2020-01-06","objectID":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/:0:0","tags":["icarus","Hexo","blog"],"title":"配置icarus主题","uri":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"categories":["tool"],"content":"修改语言 icarus主题的默认语言是英文，将语言修改为中文，进入到主题文件夹的languages文件夹中可以看到，是有一个叫zh-CN.yml的文件的 这个就是icarus主题的中文语言包，然后退到博客项目的目录下，将config.yml中的language改为zh-CN同时也可以给title，author，description加上自己的个性信息 需要注意的是，不同主题的语言包名字可能不同，但是只要进入主题的language文件夹下面查看就可以了。 修改之后，如果觉得像链接、标签这样的名字不喜欢，可以到zh-CN.yml这个语言包里面去修改相关语言配置，只需要将引号里的中文改为自己喜欢的就可以了。 common: archive: one: '归档' other: '归档' category: one: '分类' other: '分类' tag: one: '标签' other: '标签' post: one: '文章' other: '文章' prev: '上一页' next: '下一页' widget: follow: '关注我' recents: '最新文章' links: '友情链接' tag_cloud: '标签云' catalogue: '目录' article: more: '阅读更多' comments: '评论' read: '读完' about: '大约' words: '个字' donate: title: '如果你觉得这篇文章对你有帮助，可以请作者喝杯茶！' alipay: '支付宝' wechat: '微信' plugin: backtotop: '回到顶端' visit: '%s次访问' visitor: '共%s个访客' search: search: '搜索' hint: '想要查找什么...' insight: hint: '想要查找什么...' posts: '文章' pages: '页面' categories: '分类' tags: '标签' untitled: '(无标题)' ","date":"2020-01-06","objectID":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/:1:0","tags":["icarus","Hexo","blog"],"title":"配置icarus主题","uri":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"categories":["tool"],"content":"修改主题配置文件 配置文件全是英文，阅读起来难度的确不小，但是需要修改的地方还是大多能看懂知道是在哪的。 不想要的东西直接注释掉掉，或者将链接删除掉就行了，但是如果前面有-要把-也注释掉。 下面要修改的配置文件中很多都会涉及到图片的地址，如果有云床可以使用云床。如果没有云窗，可以将图片就放到本地文件夹当中，然后使用文件路径即可。 图片放到云床就没有什么好说的了，如果放到本地，建议在主题文件夹的source文件夹下面新建一个assets文件夹，然后在assets中建一个img文件夹（注意：是在主题文件夹下面的source文件夹，路径为\\icarus\\source），然后将需要的图片放到里面，然后 直接引用地址即可。如我的标签页图标地址就是D:\\blog\\themes\\icarus\\source\\assets\\img\\favicon.png，但是在填写地址的时候只能填写相对路径，不然部署后的博客是无法正常显示图片的。 ","date":"2020-01-06","objectID":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/:2:0","tags":["icarus","Hexo","blog"],"title":"配置icarus主题","uri":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"categories":["tool"],"content":"标签页图标 注意是/ favicon: /assets/img/favicon.png ","date":"2020-01-06","objectID":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/:2:1","tags":["icarus","Hexo","blog"],"title":"配置icarus主题","uri":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"categories":["tool"],"content":"logo图标 这个logo是icarus主题左上角的图标 logo: /assets/img/logo.png ","date":"2020-01-06","objectID":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/:2:2","tags":["icarus","Hexo","blog"],"title":"配置icarus主题","uri":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"categories":["tool"],"content":"导航栏 导航栏的菜单可以直接改为中文，链接是主题右上角的Github图标的链接，如果可以改为自己的Github链接，如果不喜欢也可以注释掉 navbar: # Navigation bar menu links menu: 主页: / 归档: /archives 分类: /categories 标签: /tags 关于: /about # Navigation bar links to be shown on the right links: Download on GitHub: icon: fab fa-github url: 'https://github.com/ppoffice/hexo-theme-icarus' ","date":"2020-01-06","objectID":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/:2:3","tags":["icarus","Hexo","blog"],"title":"配置icarus主题","uri":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"categories":["tool"],"content":"页脚链接 这部分是左边页脚的链接，可以换成自己的，也可以注释掉 footer: # Links to be shown on the right of the footer section links: Creative Commons: icon: fab fa-creative-commons url: 'https://creativecommons.org/' Attribution 4.0 International: icon: fab fa-creative-commons-by url: 'https://creativecommons.org/licenses/by/4.0/' Download on GitHub: icon: fab fa-github url: 'https://github.com/ppoffice/hexo-theme-icarus' ","date":"2020-01-06","objectID":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/:2:4","tags":["icarus","Hexo","blog"],"title":"配置icarus主题","uri":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"categories":["tool"],"content":"文章设置 article: # 代码高亮显示设置 highlight: # 代码高亮主题 # https://github.com/highlightjs/highlight.js/tree/master/src/styles这个项目有很多代码 # 高亮主题，下载下来放到博客项目的 node_modules\\_highlight.js@9.17.1@highlight.js\\styles # 这个目录下然后将这里的 atom-one-light 修改既可 theme: atom-one-light # 显示代码复制按钮 clipboard: true # 代码块的默认折叠状态。\"folded\"是折叠, \"unfolded\"是展开 fold: unfolded # 是否显示文章缩略图 thumbnail: true # 是否显示预估文章阅读时间 readtime: true ","date":"2020-01-06","objectID":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/:2:5","tags":["icarus","Hexo","blog"],"title":"配置icarus主题","uri":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"categories":["tool"],"content":"搜索 搜索就是主题右上角的搜索，设为默认即可，如果有需要也可以改为百度或者Google的搜索引擎 search: # Name of the search plugin type: insight ","date":"2020-01-06","objectID":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/:2:6","tags":["icarus","Hexo","blog"],"title":"配置icarus主题","uri":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"categories":["tool"],"content":"评论 关于评论，会用一篇专门的文章讲解 comment: # Name of the comment plugin type: ","date":"2020-01-06","objectID":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/:2:7","tags":["icarus","Hexo","blog"],"title":"配置icarus主题","uri":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"categories":["tool"],"content":"打赏功能 donate: - type: alipay qrcode: 图片地址 - type: wechat qrcode: 地址地址 ","date":"2020-01-06","objectID":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/:2:8","tags":["icarus","Hexo","blog"],"title":"配置icarus主题","uri":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"categories":["tool"],"content":"分享功能 想要使用分享功能需要使用插件 share: # Share plugin name type: ","date":"2020-01-06","objectID":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/:2:9","tags":["icarus","Hexo","blog"],"title":"配置icarus主题","uri":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"categories":["tool"],"content":"侧边栏粘滞 这个功能就是让侧边栏不随页面的滚动而上移，但是要侧边栏的部件至少有一个的时候才能开启 sidebar: # 左边的侧边栏设置 left: # https://ppoffice.github.io/hexo-theme-icarus/Configuration/Theme/make-a-sidebar-sticky-when-page-scrolls/ sticky: true # right sidebar settings right: # https://ppoffice.github.io/hexo-theme-icarus/Configuration/Theme/make-a-sidebar-sticky-when-page-scrolls/ sticky: false ","date":"2020-01-06","objectID":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/:2:10","tags":["icarus","Hexo","blog"],"title":"配置icarus主题","uri":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"categories":["tool"],"content":"修改小部件 小部件就是在博客页面中左右两侧一块一块功能分类的东西，同样，不需要的可以注释掉。 widgets: - # 放头像的那一块部件 type: profile # 放置的位置，左边还是右边 position: left # 作者的名字 author: 黑白 # 在个人资料小部件中显示的作者标题 author_title: 普通的我 · 读着普通的书 # 作者的位置 location: 攀枝花 # 头像地址 avatar: /assets/img/head.png # 这一项目前不知道是干嘛的 gravatar: # 将头像显示为圆形还是正方形 false是正方形 avatar_rounded: false # 关注我这个按钮的URL follow_link: 'https://github.com' # 这个部件底部的图标以及URL social_links: Github: icon: fab fa-github url: 'https://github.com' Facebook: icon: fab fa-facebook url: 'https://facebook.com' Twitter: icon: fab fa-twitter url: 'https://twitter.com' Dribbble: icon: fab fa-dribbble url: 'https://dribbble.com' RSS: icon: fas fa-rss url: / - # 目录 type: toc # 显示在左边还是右边 position: left - # 链接 type: links # 显示在左边还是右边 position: right # 链接地址 links: 廖雪峰博客: 'https://www.liaoxuefeng.com/' 夜幕博客: 'https://blog.nightteam.cn/' 骆昊Python-100-Days: 'https://github.com/jackfrued/Python-100-Days' - # 归档 type: category # Where should the widget be placed, left or right position: left - # 标签云 type: tagcloud # Where should the widget be placed, left or right position: right - # 最近文章 type: recent_posts # Where should the widget be placed, left or right position: right - # 分类 type: archive # Where should the widget be placed, left or right position: right #- # 标签 type: tag Where should the widget be placed, left or right position: right ","date":"2020-01-06","objectID":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/:2:11","tags":["icarus","Hexo","blog"],"title":"配置icarus主题","uri":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"categories":["tool"],"content":"添加雪花效果 白色背景的博客虽然看上去的感觉挺清新的，但是整体效果却差了一点，加个雪花效果 ","date":"2020-01-06","objectID":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/:3:0","tags":["icarus","Hexo","blog"],"title":"配置icarus主题","uri":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"categories":["tool"],"content":"添加snow.js文件 在主题文件夹的\\icarus\\source\\js路径下新建一个src文件夹，然后在scr文件夹下新建一个snow.js 文件，将下面的代码写入snow.js中 function snowFall(snow) { snow = snow || {}; this.maxFlake = snow.maxFlake || 100; this.flakeSize = snow.flakeSize || 10; this.fallSpeed = snow.fallSpeed || 1; } requestAnimationFrame = window.requestAnimationFrame || window.mozRequestAnimationFrame || window.webkitRequestAnimationFrame || window.msRequestAnimationFrame || window.oRequestAnimationFrame || function(callback) { setTimeout(callback, 1000 / 60); }; cancelAnimationFrame = window.cancelAnimationFrame || window.mozCancelAnimationFrame || window.webkitCancelAnimationFrame || window.msCancelAnimationFrame || window.oCancelAnimationFrame; snowFall.prototype.start = function(){ snowCanvas.apply(this); createFlakes.apply(this); drawSnow.apply(this) } function snowCanvas() { var snowcanvas = document.createElement(\"canvas\"); snowcanvas.id = \"snowfall\"; snowcanvas.width = window.innerWidth; snowcanvas.height = document.body.clientHeight; snowcanvas.setAttribute(\"style\", \"position:absolute; top: 0; left: 0; z-index: 1; pointer-events: none;\"); document.getElementsByTagName(\"body\")[0].appendChild(snowcanvas); this.canvas = snowcanvas; this.ctx = snowcanvas.getContext(\"2d\"); window.onresize = function() { snowcanvas.width = window.innerWidth; /* snowcanvas.height = window.innerHeight */ } } function flakeMove(canvasWidth, canvasHeight, flakeSize, fallSpeed) { this.x = Math.floor(Math.random() * canvasWidth); this.y = Math.floor(Math.random() * canvasHeight); this.size = Math.random() * flakeSize + 2; this.maxSize = flakeSize; this.speed = Math.random() * 1 + fallSpeed; this.fallSpeed = fallSpeed; this.velY = this.speed; this.velX = 0; this.stepSize = Math.random() / 30; this.step = 0 } flakeMove.prototype.update = function() { var x = this.x, y = this.y; /* 摆动曲线，这里是余弦*/ this.velX *= 0.98; if (this.velY \u003c= this.speed) { this.velY = this.speed } this.velX += Math.cos(this.step += .05) * this.stepSize; this.y += this.velY; this.x += this.velX; if (this.x \u003e= canvas.width || this.x \u003c= 0 || this.y \u003e= canvas.height || this.y \u003c= 0) { this.reset(canvas.width, canvas.height) } }; flakeMove.prototype.reset = function(width, height) { this.x = Math.floor(Math.random() * width); this.y = 0; this.size = Math.random() * this.maxSize + 2; this.speed = Math.random() * 1 + this.fallSpeed; this.velY = this.speed; this.velX = 0; }; // 渲染雪花-随机形状（此处可修改雪花颜色！！！） flakeMove.prototype.render = function(ctx) { var snowFlake = ctx.createRadialGradient(this.x, this.y, 0, this.x, this.y, this.size); snowFlake.addColorStop(0, \"rgba(255,255,255, 0.9)\"); /* 此处是雪花RGB颜色，默认是白色 */ snowFlake.addColorStop(.5, \"rgba(255,255,255, 0.5)\"); /* 若要改为其他颜色，请自行查 */ snowFlake.addColorStop(1, \"rgba(255,255,255, 0)\"); /* 找16进制的RGB 颜色代码。 */ ctx.save(); ctx.fillStyle = snowFlake; ctx.beginPath(); ctx.arc(this.x, this.y, this.size, 0, Math.PI * 2); ctx.fill(); ctx.restore(); }; function createFlakes() { var maxFlake = this.maxFlake, flakes = this.flakes = [], canvas = this.canvas; for (var i = 0; i \u003c maxFlake; i++) { flakes.push(new flakeMove(canvas.width, canvas.height, this.flakeSize, this.fallSpeed)) } } function drawSnow() { var maxFlake = this.maxFlake, flakes = this.flakes; ctx = this.ctx, canvas = this.canvas, that = this; ctx.clearRect(0, 0, canvas.width, canvas.height); for (var e = 0; e \u003c maxFlake; e++) { flakes[e].update(); flakes[e].render(ctx); } this.loop = requestAnimationFrame(function() { drawSnow.apply(that); }); } /* 修改maxFlake能修改雪花的最大数量 */ var snow = new snowFall({maxFlake:50}); snow.start(); 修改上面代码中的RGB值和maxFlake就可以更改雪花的颜色和最大数量 ","date":"2020-01-06","objectID":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/:3:1","tags":["icarus","Hexo","blog"],"title":"配置icarus主题","uri":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"categories":["tool"],"content":"引用snow.js文件 在/icarus/layout/layout.ejs的bodybodybody标签中添加代码： \u003cscript type=\"text/javascript\"\u003e var windowWidth = $(window).width(); if (windowWidth \u003e 480) { document.write('\u003cscript type=\"text/javascript\" src=\"/js/src/snow.js\"\u003e\u003c\\/script\u003e'); } \u003c/script\u003e 添加代码后的文件 \u003c!DOCTYPE html\u003e \u003chtml \u003c%- has_config('language') ? ' lang=\"' + get_config('language').substring(0, 2) + '\"' : '' %\u003e\u003e \u003chead\u003e \u003c%- _partial('common/head') %\u003e \u003c/head\u003e \u003cbody class=\"is-\u003c%= column_count() %\u003e-column\"\u003e \u003c%- _partial('common/navbar') %\u003e \u003c% function main_column_class() { switch (column_count()) { case 1: return 'is-12'; case 2: return 'is-8-tablet is-8-desktop is-8-widescreen'; case 3: return 'is-8-tablet is-8-desktop is-6-widescreen' } return ''; } %\u003e \u003csection class=\"section\"\u003e \u003cdiv class=\"container\"\u003e \u003cdiv class=\"columns\"\u003e \u003cdiv class=\"column \u003c%= main_column_class() %\u003e has-order-2 column-main\"\u003e\u003c%- body %\u003e\u003c/div\u003e \u003c%- _partial('common/widget', { position: 'left' }) %\u003e \u003c%- _partial('common/widget', { position: 'right' }) %\u003e \u003c/div\u003e \u003c/div\u003e \u003c/section\u003e \u003c%- _partial('common/footer') %\u003e \u003c%- _partial('common/scripts') %\u003e \u003c% if (has_config('search.type')) { %\u003e \u003c%- _partial('search/' + get_config('search.type')) %\u003e \u003c% } %\u003e \u003cscript type=\"text/javascript\"\u003e var windowWidth = $(window).width(); if (windowWidth \u003e 480) { document.write('\u003cscript type=\"text/javascript\" src=\"/js/src/snow.js\"\u003e\u003c\\/script\u003e'); } \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e 保存后刷新博客就可以看到效果了。 ","date":"2020-01-06","objectID":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/:3:2","tags":["icarus","Hexo","blog"],"title":"配置icarus主题","uri":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"categories":["tool"],"content":"文章发表日期 icarus主题的文章左上角文章发表日期默认是几天前这样的信息，看起来其实是不太方便的 需要修改\\themes\\icarus\\source\\js下的main.js文件， ","date":"2020-01-06","objectID":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/:3:3","tags":["icarus","Hexo","blog"],"title":"配置icarus主题","uri":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"categories":["tool"],"content":"增加文章版权 修改themes/icarus/layout/common/article.ejs 在下面的代码中增加 \u003cdiv class=\"content\"\u003e \u003c%- index \u0026\u0026 post.excerpt ? post.excerpt : post.content %\u003e \u003c/div\u003e \u003c!- 在这中间添加 -\u003e \u003c% if (!index \u0026\u0026 post.tags \u0026\u0026 post.tags.length) { %\u003e 在中间添加如下程序 \u003c% if (!index \u0026\u0026 post.layout === 'post' \u0026\u0026 post.copyright !== false) { %\u003e \u003cul class=\"post-copyright\"\u003e \u003cli\u003e\u003cstrong\u003e本文标题：\u003c/strong\u003e\u003ca href=\"\u003c%= post.permalink %\u003e\"\u003e\u003c%= page.title %\u003e\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003cstrong\u003e本文作者：\u003c/strong\u003e\u003ca href=\"\u003c%= theme.url %\u003e\"\u003e\u003c%= theme.author %\u003e\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003cstrong\u003e本文链接：\u003c/strong\u003e\u003ca href=\"\u003c%= post.permalink %\u003e\"\u003e\u003c%= post.permalink %\u003e\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003cstrong\u003e发布时间：\u003c/strong\u003e\u003c%= post.date.format(\"YYYY-MM-DD\") %\u003e\u003c/li\u003e \u003cli\u003e\u003cstrong\u003e版权声明：\u003c/strong\u003e本博客所有文章除特别声明外，均采用 \u003ca href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh\" rel=\"external nofollow\" target=\"_blank\"\u003eCC BY-NC-SA 4.0\u003c/a\u003e 许可协议。转载请注明出处！ \u003c/li\u003e \u003c/ul\u003e \u003c% } %\u003e ","date":"2020-01-06","objectID":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/:3:4","tags":["icarus","Hexo","blog"],"title":"配置icarus主题","uri":"/posts/%E9%85%8D%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"categories":null,"content":"使用Python代码来测试一下Hexo框架对Markdown格式中代码块颜色以及语法高亮的情况！ from collections import namedtuple Card = namedtuple('Card', ['rank', 'suit']) class FrenchDeck: ranks = [str(n) for n in range(2, 11)] + list('JQKA') suits = 'spades diamonds clubs hearts'.split() #初始化结果是一个列表 def __init__(self): self._cards = [Card(rank, suit) for suit in self.suits for rank in self.ranks] def __len__(self): return len(self._cards) def __getitem__(self, item): return self._cards[item] # 定义spades diamonds clubs hearts的大小规则 suit_values = dict(spades=3, hearts=2, diamonds=1, clubs=0) def spades_high(card): rank_value = FrenchDeck.ranks.index(card.rank) return rank_value * len(suit_values) + suit_values[card.suit] deck = FrenchDeck() print(len(deck)) print(type(deck)) card_list = sorted(deck, key=spades_high) print(card_list) ","date":"2020-01-02","objectID":"/posts/%E4%BB%A3%E7%A0%81%E6%B5%8B%E8%AF%95/:0:0","tags":null,"title":"测试代码块颜色","uri":"/posts/%E4%BB%A3%E7%A0%81%E6%B5%8B%E8%AF%95/"},{"categories":["Python"],"content":"Python中的魔法函数是一个很神奇的东西 什么是魔法函数 在python中，有一些内置好的特定的函数，这些方法在进行特定的操作时会自动被调用，称之为魔法函数。 Python中的魔法函数就是以双下划线开头和结尾的函数，这些函数可以让我们自己随意的制定自定义类的特性。 魔法函数可以在任意一个自定义类中重写，因此它其实不是object类的一个方法。 魔法函数并不和类挂钩，它是为了增加类的特性 在使用的时候并不需要调用，只需要使用相关的函数就会自动调用 ","date":"2019-11-22","objectID":"/posts/%E4%BB%80%E4%B9%88%E6%98%AF%E9%AD%94%E6%B3%95%E5%87%BD%E6%95%B0/:0:0","tags":["Python","魔法函数"],"title":"什么是魔法函数","uri":"/posts/%E4%BB%80%E4%B9%88%E6%98%AF%E9%AD%94%E6%B3%95%E5%87%BD%E6%95%B0/"},{"categories":["Python"],"content":"元类相关 __init__: 初始化函数，在创建实例对象为其赋值的时候使用，在__new__之后，__init__必须至少有一个参数self，就是这个__new__返回的实例，__init__是在__new__的基础上可以完成一些其他初始化的动作，__init__不需要返回值。__init__函数会在对象初始化的时候调用 class MySelf(object): def __init__(self, name, age): self.name = name self.age = age def do(self, doing): return \"%s已经%s岁了，正在学习%s\" % (self.name, self.age, doing) def main(): myself = MySelf(\"pujic\", 18) print(myself.do(\"魔法函数\")) if __name__ == '__main__': main() pujic已经18岁了，正在学习魔法函数 class Company(object): def __init__(self, employee_list): self.employee = employee_list company = Company([\"tom\", \"bob\", \"jane\"]) employee = company.employee for em in employee: print(em) tom bob jane ","date":"2019-11-22","objectID":"/posts/%E4%BB%80%E4%B9%88%E6%98%AF%E9%AD%94%E6%B3%95%E5%87%BD%E6%95%B0/:1:0","tags":["Python","魔法函数"],"title":"什么是魔法函数","uri":"/posts/%E4%BB%80%E4%B9%88%E6%98%AF%E9%AD%94%E6%B3%95%E5%87%BD%E6%95%B0/"},{"categories":["Python"],"content":"集合序列相关__getitem__ 使用getitem这个魔法函数的时候就给这个实例化的对象添加了一个可迭代的属性，使用for循环的时候，解释器会尝试着去寻找对象的迭代器，找不到迭代器就会尝试找对象中有没有getitem这个方法，有的话就回去调用这个方法，直到抛出异常，然后结束for循环 class Company(object): def __init__(self, employee_list): self.employee = employee_list def __getitem__(self, item): return self.employee[item] company = Company([\"tom\", \"bob\", \"jane\"]) company1 = company[:2] for em in company: print(em) print(len(company1)) tom bob jane 2 ","date":"2019-11-22","objectID":"/posts/%E4%BB%80%E4%B9%88%E6%98%AF%E9%AD%94%E6%B3%95%E5%87%BD%E6%95%B0/:2:0","tags":["Python","魔法函数"],"title":"什么是魔法函数","uri":"/posts/%E4%BB%80%E4%B9%88%E6%98%AF%E9%AD%94%E6%B3%95%E5%87%BD%E6%95%B0/"},{"categories":["Python"],"content":"集合序列相关__len__ class Company(object): def __init__(self, employee_list): self.employee = employee_list def __getitem__(self, item): return self.employee[item] company = Company([\"tom\", \"bob\", \"jane\"]) for em in company: print(em) print(len(company)) tom bob jane --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u003cipython-input-4-0016aa621a9e\u003e in \u003cmodule\u003e 12 print(em) 13 ---\u003e 14 print(len(company)) TypeError: object of type 'Company' has no len() 一个对象虽然可迭代，但是没有长度属性，也是不能使用len()函数去计算长度的，所以需要使用len这个魔法方法，给对象添加计算长度的属性 class Company(object): def __init__(self, employee_list): self.employee = employee_list def __getitem__(self, item): return self.employee[item] def __len__(self): return len(self.employee) company = Company([\"tom\", \"bob\", \"jane\"]) print(len(company)) 3 ","date":"2019-11-22","objectID":"/posts/%E4%BB%80%E4%B9%88%E6%98%AF%E9%AD%94%E6%B3%95%E5%87%BD%E6%95%B0/:3:0","tags":["Python","魔法函数"],"title":"什么是魔法函数","uri":"/posts/%E4%BB%80%E4%B9%88%E6%98%AF%E9%AD%94%E6%B3%95%E5%87%BD%E6%95%B0/"},{"categories":["Python"],"content":"字符串表示__str__ 我们直接输出一个类实例化的对象，那么得到的将会是这个对象的内存地址 class Company(object): def __init__(self, employee_list): self.employee = employee_list company = Company([\"tom\", \"bob\", \"jane\"]) print(company) \u003c__main__.Company object at 0x05EDEF50\u003e 想打印一个对象的内容，就需要添加一个__str__魔法方法 class Company(object): def __init__(self, employee_list): self.employee = employee_list def __str__(self): return \",\".join(self.employee) company = Company([\"tom\", \"bob\", \"jane\"]) print(company) tom,bob,jane 虽然一个对象是有魔法函数str的但是需要配合print函数才能打印，在交互模式下直接输入此对象是会提示该对象的地址问题，在IDE里面就不会有任何输出，想在交互模式下直接输入此对象并打印出来就需要使用repr这个魔法函数 \u003e\u003e\u003e class Company(object): ... def __init__(self, employee_list): ... self.employee = employee_list ... ... def __str__(self): ... return \",\".join(self.employee) ... ... def __repr__(self): ... return \",\".join(self.employee) ... \u003e\u003e\u003e company = Company([\"tom\", \"bob\", \"jane\"]) \u003e\u003e\u003e company #隐含的去调用repr魔法函数 tom,bob,jane ","date":"2019-11-22","objectID":"/posts/%E4%BB%80%E4%B9%88%E6%98%AF%E9%AD%94%E6%B3%95%E5%87%BD%E6%95%B0/:4:0","tags":["Python","魔法函数"],"title":"什么是魔法函数","uri":"/posts/%E4%BB%80%E4%B9%88%E6%98%AF%E9%AD%94%E6%B3%95%E5%87%BD%E6%95%B0/"},{"categories":["Python"],"content":"Python中的数据类型 更详细的常见内置类型可参照官方文档https://docs.python.org/zh-cn/3.7/library/stdtypes.html None 程序全局中只有一个（Pytho在解释器启动的时候Python会用None类型生成一个None对象） None 是 Python 特殊的数据类型NoneType，它是一个空值，可将 None 赋值给任何数据类型，但是不能创建其他NoneType对象，None不等于0，也不等于True或False，因此在布尔表达式上不能用None来单独表示 \u003e\u003e\u003e type(None) \u003cclass 'NoneType'\u003e \u003e\u003e\u003e None == 0 False \u003e\u003e\u003e None == True False \u003e\u003e\u003e None == False False 数值类型 ","date":"2019-11-22","objectID":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/:0:0","tags":["Python"],"title":"Python内置类型","uri":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/"},{"categories":["Python"],"content":"整数 整数具有无限的精度，整数永远是精确的，结果是一个整数，但是结果不一定是int型 \u003e\u003e\u003e 1/2 0.5 \u003e\u003e\u003e a = -1 \u003e\u003e\u003e type(a) \u003cclass 'int'\u003e \u003e\u003e\u003e 1//2 0 \u003e\u003e\u003e type(1//2) \u003cclass 'int'\u003e \u003e\u003e\u003e type(4.0) \u003cclass 'float'\u003e \u003e\u003e\u003e pow(0, 0) 1 4.0是整数，但是4.0并不是int类型 ","date":"2019-11-22","objectID":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/:1:0","tags":["Python"],"title":"Python内置类型","uri":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/"},{"categories":["Python"],"content":"浮点数 浮点数转换为整数会被舍掉小数，而不是四舍五入 \u003e\u003e\u003e int(4.6) 4 \u003e\u003e\u003e int(4.1) 4 \u003e\u003e\u003e int(4.9) 4 ","date":"2019-11-22","objectID":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/:2:0","tags":["Python"],"title":"Python内置类型","uri":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/"},{"categories":["Python"],"content":"虚数 虚部不填时默认为0，在Python中虚数的虚部不是用i，而是j \u003e\u003e\u003e int(4.1) 4 \u003e\u003e\u003e int(4.9) 4 \u003e\u003e\u003e complex(4, 8) (4+8j) \u003e\u003e\u003e complex(4) (4+0j) ","date":"2019-11-22","objectID":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/:3:0","tags":["Python"],"title":"Python内置类型","uri":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/"},{"categories":["Python"],"content":"布尔型 布尔值属于整数的子类型 not的优先级比非布尔运算符低，因此 not a == b 会被解读为 not (a == b) ，而 a == not b 会引发语法错误 \u003e\u003e\u003e a = 3 \u003e\u003e\u003e b = 0 \u003e\u003e\u003e a == b False \u003e\u003e\u003e not a == b True \u003e\u003e\u003e not b == a True \u003e\u003e\u003e (not b) == a False \u003e\u003e\u003e not b True \u003e\u003e\u003e b == not a File \"\u003cstdin\u003e\", line 1 b == not a ^ SyntaxError: invalid syntax \u003e\u003e\u003e 4.0 == 4 True 迭代类型 Python 支持在容器中进行迭代 容器对象要提供迭代支持，必须定义一个方法 可以通过for …… in ……的语法来实现循环遍历，这个循环我们就叫做迭代 通过collections中的Iterable类型判断该对象是否可以被迭代 \u003e\u003e\u003e from collections.abc import Iterable \u003e\u003e\u003e isinstance([1, 2, 3], Iterable) True \u003e\u003e\u003e isinstance(123, Iterable) False \u003e\u003e\u003e isinstance(\"abc\", Iterable) True \u003e\u003e\u003e dic = {\"name\": \"pujic\", \"age\": \"4\"} \u003e\u003e\u003e isinstance(dic, Iterable) True \u003e\u003e\u003e for key, value in dic.items(): ... print(key, value) ... name pujic age 4 \u003e\u003e\u003e tup = 1, \"a\", \"2\" \u003e\u003e\u003e isinstance(tup, Iterable) True \u003e\u003e\u003e s = set([1, 2, 3]) \u003e\u003e\u003e isinstance(s, Iterable) True \u003e\u003e\u003e for i in s: ... print(i) ... 1 2 3 \u003e\u003e\u003e a = \"abc\" \u003e\u003e\u003e isinstance(a, Iterable) 集合虽然无序，但是集合却是可迭代类型 序列类型 ","date":"2019-11-22","objectID":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/:4:0","tags":["Python"],"title":"Python内置类型","uri":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/"},{"categories":["Python"],"content":"list 列表是Python中最常用到的内置类型 \u003e\u003e\u003e #列表是动态变化的，当A列表赋值给B列表后，A列表的值发生了变化，B列表的值也会发生变化 \u003e\u003e\u003e list1 = [1, 2, 3, 4, 5, 6, 7, 8, 9] \u003e\u003e\u003e list2 = list1 \u003e\u003e\u003e del list1[2] \u003e\u003e\u003e list2 [1, 2, 4, 5, 6, 7, 8, 9] \u003e\u003e\u003e #使用index可以返回列表的索引 \u003e\u003e\u003e list2.index(4) 2 \u003e\u003e\u003e #直接使用列表索引可以改变列表元素的值 \u003e\u003e\u003e list1[0] = \"a\" \u003e\u003e\u003e list1 ['a', 2, 4, 5, 6, 7, 8, 9] \u003e\u003e\u003e #使用append在列表的末尾添加元素 \u003e\u003e\u003e list1.append(\"b\") \u003e\u003e\u003e list1 ['a', 2, 4, 5, 6, 7, 8, 9, 'b'] \u003e\u003e\u003e #使用insert在指定位置插入元素 \u003e\u003e\u003e list1.insert(2, \"c\") \u003e\u003e\u003e list1 ['a', 2, 'c', 4, 5, 6, 7, 8, 9, 'b'] \u003e\u003e\u003e # 删除元素，pop方法会将删除的元素作为返回值返回 \u003e\u003e\u003e del list1[0] \u003e\u003e\u003e list1 [2, 'c', 4, 5, 6, 7, 8, 9, 'b'] \u003e\u003e\u003e list1.pop() 'b' \u003e\u003e\u003e list1 [2, 'c', 4, 5, 6, 7, 8, 9] \u003e\u003e\u003e list1.pop(1) 'c' \u003e\u003e\u003e list1 [2, 4, 5, 6, 7, 8, 9] \u003e\u003e\u003e #使用remove删除指定元素，如果列表中有多个指定值，remove只会删除指定值的第一个 \u003e\u003e\u003e s = ['a', 'b', 'c', 'd'] \u003e\u003e\u003e s.remove('a') \u003e\u003e\u003e s ['b', 'c', 'd'] \u003e\u003e\u003e #使用切片取值 \u003e\u003e\u003e list1 = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \u003e\u003e\u003e list1[0:10:2] [0, 2, 4, 6, 8] \u003e\u003e\u003e max(list1) 9 \u003e\u003e\u003e min(list1) 0 \u003e\u003e\u003e list2 = [\"wdhqu\", \"dqhh\", \"djweiy\", \"as\", \"zzz\"] \u003e\u003e\u003e max(list2) 'zzz' \u003e\u003e\u003e list1 = [989089, -483748928, 7432] \u003e\u003e\u003e max(list1, key=abs) -483748928 \u003e\u003e\u003e list1.extend(list2) \u003e\u003e\u003e list1 [989089, -483748928, 7432, 'wdhqu', 'dqhh', 'djweiy', 'as', 'zzz'] python中max函数默认数值型参数，取值大者；字符型参数，取字母表排序靠后者；序列型参数，则依次按索引位置的值进行比较取最大者。还可以通过传入命名参数key，指定取最大值方法。 \u003e\u003e\u003e list1 = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \u003e\u003e\u003e max(list1) 9 \u003e\u003e\u003e min(list1) 0 \u003e\u003e\u003e list2 = [\"wdhqu\", \"dqhh\", \"djweiy\", \"as\", \"zzz\"] \u003e\u003e\u003e max(list2) 'zzz' \u003e\u003e\u003e list1 = [989089, -483748928, 7432] \u003e\u003e\u003e max(list1, key=abs) -483748928 extend方法，只要是可迭代对象就可以直接传递，extend会调用对象中的迭代器 \u003e\u003e\u003e a = [\"student1\", \"student2\"] \u003e\u003e\u003e b = [\"student3\", \"student4\"] \u003e\u003e\u003e a.extend(b) \u003e\u003e\u003e a ['student1', 'student2', 'student3', 'student4'] ","date":"2019-11-22","objectID":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/:5:0","tags":["Python"],"title":"Python内置类型","uri":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/"},{"categories":["Python"],"content":"tuple 元组这种数据结构同列表类似，都可以描述一组数据的集合，它们都是容器，是一系列组合的对象，不同的地方在于，元组里的元素是不能更改的 元组中只包含一个元素时，需要在元素后面添加逗号，否则括号会被当作运算符使用 元组可以进行连接组合 元组访问元素的方法和列表相同 \u003e\u003e\u003e tup1 = (1, 3, \"p\") \u003e\u003e\u003e tup2 = (88, 32, 5, \"name\", \"age\") \u003e\u003e\u003e tup3 = tup1 + tup2 \u003e\u003e\u003e tup3 (1, 3, 'p', 88, 32, 5, 'name', 'age') \u003e\u003e\u003e #元组元素不可改变，也不可增减 \u003e\u003e\u003e del tup1[0] Traceback (most recent call last): File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e TypeError: 'tuple' object doesn't support item deletion \u003e\u003e\u003e tup1[2] 'p' \u003e\u003e\u003e a, b, c = tup1 \u003e\u003e\u003e a 1 \u003e\u003e\u003e type(a) \u003cclass 'int'\u003e \u003e\u003e\u003e # 创建元组时如果只有一个元素要在元素后添加一个逗号 \u003e\u003e\u003e a = (1) \u003e\u003e\u003e a 1 \u003e\u003e\u003e type(a) \u003cclass 'int'\u003e \u003e\u003e\u003e tup1 = 1, 2, 3, 4 \u003e\u003e\u003e tup1 (1, 2, 3, 4) \u003e\u003e\u003e type(tup1) \u003cclass 'tuple'\u003e 元组拆包 元组拆包可以使用*来处理剩下的元素，这个应用最先出现在函数用*args来获取不确定数量的参数 \u003e\u003e\u003e tup = (1, 2, \"a\") \u003e\u003e\u003e a, b, c = tup \u003e\u003e\u003e print(a, b, c) 1 2 a \u003e\u003e\u003e a, *_ = tup \u003e\u003e\u003e a 1 \u003e\u003e\u003e _ [2, 'a'] \u003e\u003e\u003e ","date":"2019-11-22","objectID":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/:6:0","tags":["Python"],"title":"Python内置类型","uri":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/"},{"categories":["Python"],"content":"range range() 返回的是一个可迭代对象，可以用来生成列表，可以以用来迭代 \u003e\u003e\u003e l = list(range(10)) \u003e\u003e\u003e l [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \u003e\u003e\u003e for i in range(0, 11, 3): ... print(i) ... 0 3 6 9 \u003e\u003e\u003e sum(range(101)) 5050 ","date":"2019-11-22","objectID":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/:7:0","tags":["Python"],"title":"Python内置类型","uri":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/"},{"categories":["Python"],"content":"str 字符串是不可变类型，不能通过索引来修改字符串 \u003e\u003e\u003e a = \"asdfghjkl\" \u003e\u003e\u003e a[3] 'f' \u003e\u003e\u003e a[1:4] 'sdf' \u003e\u003e\u003e a[-1:-6:-2] 'ljg' \u003e\u003e\u003e for i in a: ... print(i) ... a s d f g h j k l \u003e\u003e\u003e a[0] = \"1\" Traceback (most recent call last): File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e TypeError: 'str' object does not support item assignment 映射类型 dict全称dictionary，使用键-值（key-value）存储，具有极快的查找速度。 字典是另一种可变容器模型，且可存储任意类型对象。 字典是Python中唯一内建的映射类型，字典指定值并没有特殊顺序，都存储在一个特殊的键(key)里，键可以是数字、字符串或元组。映射是一种通过名字引用值得数据结构。 字典值可以没有限制地取任何Python对象，既可以是标准对象，也可以是用户定义的对象，但键不可以，字典的键必须是可哈希的 \u003e\u003e\u003e dic = {\"name\": \"p\", \"age\": \"100\"} \u003e\u003e\u003e dic[\"name\"] 'p' \u003e\u003e\u003e len(dic) 2 \u003e\u003e\u003e stu = [('name','pu'),('number','1001')] \u003e\u003e\u003e stu = [('name','p'),('number','1001')] \u003e\u003e\u003e dic_id = dict(stu) \u003e\u003e\u003e dic_id {'name': 'p', 'number': '1001'} \u003e\u003e\u003e dic_id[\"number\"] '1001' \u003e\u003e\u003e dic_id.clear() \u003e\u003e\u003e dic_id {} \u003e\u003e\u003e # 拼接字典，如果有相同的键，那么后面的值会覆盖前面的 \u003e\u003e\u003e x = {\"name\": \"p\", \"age\": 18} \u003e\u003e\u003e y = {\"name\": \"j\", \"age\": 12} \u003e\u003e\u003e z = {**x, **y} \u003e\u003e\u003e z {'name': 'j', 'age': 12} 字典的方法还是很重要的，但是用的比较少 方法和方法 描述 len() len(dict)函数用于计算字典元素的个数，即键的总数。PS：从1开始计算，并不是从0开始计算 type() type(variable)函数返回输入的变量类型 clear() clear()方法用于删除字典内的所有项，该方法不需要参数，该函数是一个原地操作函数，没有任何返回值。字典调用clear方法后整个字典内所有项都被删除 copy() copy()方法返回一个具有相同键/值对的新字典。这个方法是浅复制，因为值本身是相同的，而不是副本.该方法不需要参数，返回结果为一个字典的浅复制。替换副本的值时原始字典不受影响。如果修改了某个值（原地修改，不是替换），原始字典就会改变，因为同样的值也在原字典中，以这种方式进行复制就是浅复制，而使用深复制可以避免该问题 fromkeys() fromkeys()方式用于创建一个新字典；以序列seq中的元素做字典的键，value为字典所有键对应的初始值。 get() get()方法返回指定键的值，如果值不在字典中，就返回默认值。此语法中dict代表指定字典，key代表字典中要查找的键，default代表指定键的值不存在时返回默认值。该方法返回结果为指定键的值，如果值不在字典中，就返回默认值None。由结果看到，其他方法试图访问字典中不存在的项时会报错，而使用get方法就不会。使用get方法访问一个不存在的键时，返回None，这里还可以自定义默认值，用于替换None key in dict() Python字典in操作符用于判断键是否存在与字典中，如果键在字典dict中就返回True，否则返回Flase items() items()方法以列表返回可遍历的（键,值）元组数组，返回结果为一个元组数组。此语法中dict代表指定字典，该方法不需要参数，返回结果为可遍历的（键/值）元组数组 keys() keys()返回以列表返回一个字典所有键。此语法中dict代表指定字典，该方法不需要参数，返回结果为一个字典所有键 setdefault() setdefault()方法和get()方法类似，用于获得与给定键相关联的值，如果键不存在与字典中，就会添加键并将值设为默认值。由输出结果看到，当键不存在时，setdefault方法返回默认值并更新字典；如果键存在，就返回与其对应的值，不改变字典。和get一样，默认值可以选择，如果不设定就使用None，如果设定就使用设定的值 update() update()方法用于把字典dict2的键/值对更新到dict里。此语法中dict代表指定字典，dict2代表添加到指定字典dict里的字典。该方法没有任何返回值。提供的字典中的项被添加到旧字典中，如果有相同的键就会覆盖 values() values()方法以列表形式返回字典中所有值，与返回键的列表不同，返回的列表中可以包含重复的元素。该方法不需要参数，返回结果为字典中的所有值 pop(key[,default]) 删除字典给定键 key 所对应的值，对应的value也会从dict中删除，返回值为被删除的值。key值必须给出，否则返回default值。dict内部存放的顺序和key放入的顺序是没有关系的 popitem() 随机返回并删除字典中的一对键和值(一般删除末尾对) \u003e\u003e\u003e dic = {} \u003e\u003e\u003e dic['name']='pu' \u003e\u003e\u003e dic['age', 'height']=100, '3米' \u003e\u003e\u003e dic {'name': 'pu', ('age', 'height'): (100, '3米')} \u003e\u003e\u003e dic = dict.fromkeys([\"A\", \"B\"], 0) \u003e\u003e\u003e dic {'A': 0, 'B': 0} \u003e\u003e\u003e a = \"123\" \u003e\u003e\u003e dic = dict.fromkeys(a, \"str\") \u003e\u003e\u003e dic {'1': 'str', '2': 'str', '3': 'str'} \u003e\u003e\u003e for k, v in dic.items(): ... print(f\"{k}:{v}\") ... 1:str 2:str 3:str 集合 ","date":"2019-11-22","objectID":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/:8:0","tags":["Python"],"title":"Python内置类型","uri":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/"},{"categories":["Python"],"content":"set 集合是一个无序不重复元素的集。基本功能包括关系测试和消除重复元素。集合对象还支持 union（联合），intersection（交），difference（差）和 sysmmetric difference（对称差集）等数学运算,创建空集合只能用()不能用{},{}是用来创建空列表的 \u003e\u003e\u003e s1 = set() \u003e\u003e\u003e s1 set() \u003e\u003e\u003e s1 = set([1, 2, 3, 4]) \u003e\u003e\u003e s1 {1, 2, 3, 4} \u003e\u003e\u003e #集合通过add方法添加元素 \u003e\u003e\u003e s1.add(\"a\") \u003e\u003e\u003e s1 {1, 2, 3, 4, 'a'} \u003e\u003e\u003e # update方法是将一个可迭代对象拆分去重后添加到集合中 \u003e\u003e\u003e s1.update(\"abc\") \u003e\u003e\u003e s1 {1, 2, 3, 4, 'a', 'b', 'c'} \u003e\u003e\u003e #discard方法会删除找到的元素，如果没有找到则什么也不做 \u003e\u003e\u003e s1.discard(\"b\") \u003e\u003e\u003e s1 {1, 2, 3, 4, 'a', 'c'} 还有pop(), clear()等方法，使用\u0026可以求两个集合中的交集，使用|可以求两个集合的并集，使用-可以求差集.集合可以使用大于（\u003e）、小于（\u003c）、大于等于（\u003e=）、小于等于（\u003c=）、等于（==）、不等于（！=）来判断某个集合是否完全包含于另一个集合，也可以使用子父集判断函数。 ","date":"2019-11-22","objectID":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/:9:0","tags":["Python"],"title":"Python内置类型","uri":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/"},{"categories":["Python"],"content":"不可变集合frozenset Python中还有一种不可改变的集合，那就是frozenset，不像set集合，可以增加删除集合中的元素，该集合中的内容是不可改变的，类似于字符串、元组。 \u003e\u003e\u003e f1 = frozenset() \u003e\u003e\u003e f1 frozenset() \u003e\u003e\u003e f2 = frozenset({\"a\": \"A\", \"b\": \"B\"}) \u003e\u003e\u003e f2 frozenset({'b', 'a'}) ","date":"2019-11-22","objectID":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/:10:0","tags":["Python"],"title":"Python内置类型","uri":"/posts/python%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/"},{"categories":["Python"],"content":"Python完全面向对象 ","date":"2019-11-22","objectID":"/posts/python%E4%B8%87%E7%89%A9%E7%9A%86%E5%AF%B9%E8%B1%A1/:0:0","tags":["Python"],"title":"Python万物皆对象","uri":"/posts/python%E4%B8%87%E7%89%A9%E7%9A%86%E5%AF%B9%E8%B1%A1/"},{"categories":["Python"],"content":"Python中一切皆对象 类也是对象，在Python中类相当于一种模板对象，实例化的过程就是通过模板对象来创建对象 def ask(name=\"pujic\"): print(name) ask() 执行之后输出结果肯定为pujic 将函数赋值给了变量，对变量进行操作其实就是再对函数进行操作 def ask(name=\"pujic\"): print(name) my_func = ask my_func(\"tom\") 类也是对象，是一种模板对象，Python中的类不像静态语言中的类，静态语言中的类就是类，类实例化的对象才是对象，Python中的类是一种模板对像 class Person: def __init__(self): print(\"jack\") my_class = Person my_class() 上面的程序中并没有实例化Person这个类，因为my_class = Person这句并没有加括号，这个过程相当于对象赋值，下面才是执行这类 函数和类可以添加到集合中，调用函数的时候没有return所以会返回一个None obj_list = [] obj_list.append(ask) obj_list.append(Person) for item in obj_list: print(item()) werther None jack \u003c__main__.Person object at 0x05B56E10\u003e 函数也可以作为函数的返回值 def ret_fanc(): print(\"返回函数\") return ask my_ask = ret_fanc() my_ask() def ret_fanc(): print(\"返回函数\") return ask my_ask = ret_fanc() my_ask(\"tom\") my_ask是ret_fanc函数执行返回的ask函数对象，那么完全可以像操作ask函数一样操作它 ","date":"2019-11-22","objectID":"/posts/python%E4%B8%87%E7%89%A9%E7%9A%86%E5%AF%B9%E8%B1%A1/:1:0","tags":["Python"],"title":"Python万物皆对象","uri":"/posts/python%E4%B8%87%E7%89%A9%E7%9A%86%E5%AF%B9%E8%B1%A1/"},{"categories":["Python"],"content":"type_class_object的关系 type有两种用法，一种是生成一个类，第二种就是返回对象的类型 \u003e\u003e\u003e a = 1 \u003e\u003e\u003e type(a) \u003cclass 'int'\u003e \u003e\u003e\u003e type(int) \u003cclass 'type'\u003e str也一样 \u003e\u003e\u003e b = \"abc\" \u003e\u003e\u003e type(b) \u003cclass 'str'\u003e \u003e\u003e\u003e type(str) \u003cclass 'type'\u003e 自己定义的类也是如此 \u003e\u003e\u003e class Student: ... pass ... \u003e\u003e\u003e stu = Student() \u003e\u003e\u003e type(stu) \u003cclass '__main__.Student'\u003e \u003e\u003e\u003e type(Student) \u003cclass 'type'\u003e 从上面的输出结果可以知道type -\u003e class -\u003e obj，type生成了类，然后类实例化成了对象 object是最顶层的基类，如果一个类没有继承类，那么这个类的基类就是object \u003e\u003e\u003e Student.__bases__ (\u003cclass 'object'\u003e,) \u003e\u003e\u003e class MyStudent(Student): ... pass ... \u003e\u003e\u003e MyStudent.__bases__ (\u003cclass '__main__.Student'\u003e,) Python中万物皆对象，那么type也是一个类，type也是一个对象， \u003e\u003e\u003e type.__bases__ (\u003cclass 'object'\u003e,) \u003e\u003e\u003e type(object) \u003cclass 'type'\u003e \u003e\u003e\u003e object.__bases__ () 可以看到object没有基类，但是object是通过type实例化的，所以这里就形成了一个环路 ","date":"2019-11-22","objectID":"/posts/python%E4%B8%87%E7%89%A9%E7%9A%86%E5%AF%B9%E8%B1%A1/:2:0","tags":["Python"],"title":"Python万物皆对象","uri":"/posts/python%E4%B8%87%E7%89%A9%E7%9A%86%E5%AF%B9%E8%B1%A1/"}]